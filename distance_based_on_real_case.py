#!/usr/bin/env python
# coding: utf-8

# # Test Distance Based Localization on real cases


import copy
import json
import math
import time
from datetime import datetime
from pathlib import Path

import numpy as np
import numpy.typing as npt
import polars as pl
import xtgeo
from ert.field_utils import ErtboxParameters
from ert.storage import open_storage
from surfio import IrapHeader, IrapSurface

from iterative_ensemble_smoother.esmda import ESMDA
from iterative_ensemble_smoother.experimental import AdaptiveESMDA, DistanceESMDA

"""
This script will read info to test distance-based localisation from various sources:
1. Prediction of response parameters are fetched from ERT storage
2. Observations are fetched from both storage and also read from file where also
   (x,y) position of well with production data or (x,y) position of
   4D seismic observation point is saved. The input csv formatted file
   with production data are generated by running a script in
   RMS workflow where it is possible to get the well positions or average well path
   position (per zone). Well position is taken from RMS well paths while the well
   production observations are currently taken from ERT input observation file for
   SUMMARY data from flow simulator. Seismic observations are read from the FMU
   project where tables of seismic observations with position is available.
   Both observations of production data (corresponding to SUMMARY vectors from
   flow simulator) and seismic observations can be associated with geological
   zones. If not, the observations are assumed to be relevant to update parameters
   for all geological zones.
   NOTE: Need to add observation input files for RFT and PLT and TRACER data.
   All these data types can be associated with a well position (RFT, PLT) or
   a set of well positions (TRACER).
3. Ensemble of realizations of all parameters (both scalar and field parameters)
   is fetched from ERT storage.
4. The definition of ERTBOX for each field parameter should be possible to get
   from storage. This information is currently defined by a hard-coded dict
   in this script.

Use of observations:
1. First a dict with all observations are built based on combining the information about
   observations from both storage and external files read. This must be created such
   that it is easy to identify which observations belongs to which zones.
   Default will be that observations belongs to all zones.
2. Secondly, for all observations that cannot be associated with a single position,
   the position data is marked as None existing, but still they must be used.
   Note: In Drogon case all summary observations are related to well position, but
   it should be possible to use summary observations of groups of wells or even field
   scale observations (e.g. FOPT or other field scale observations)
3. Check if there are zone dependent observations (not available in all zones)
   and define groups of observations per zone.
4. Ideally, it should be possible to use subset of observations to update scalar
   parameters and not only use all observations to update each individual
   scalar parameter such that e.g. not all seismic observations or all summary
   observations have effect on updating of all scalar parameter.
   This was an option in the now abandoned non-adaptive localization.
   Alternatively, use adaptive localization or EnIF for scalar parameters.

Responses and observations:
1. For each zone, calculate Y matrix using responses, and get observation vector
   and diagonal covariance matrix C_D for observation error.
   If multiple zones use the same set of observations, we can re-use Y,
   observation vector and C_D instead of re-calculating it.
   If multiple zones have different sets of observations, Y matrix
   and observation vector and C_D must be calculated individually per zone.

The algorithm in this script is roughly as follows:
1. Establish the polars dataframe with observations, observation error, position,
   localization ranges and zone name (if observations are zone dependent)
   One obs dataframe per zone and one for 2D fields. They should all be equal
   as long as there are no zone-dependent observations or localization ranges,
   but in general they may differ from zone to zone and even be different for 3D
   and 2D fields.
2. Create lists of scalar parameters, 2D field parameters and 3D field parameter
   from storage.
2. Run update for both distance-based localization and global update.
3. For distance-based for 3D parameters:
    3.1. Define group of zones having the same observation dataframe.
       This means that all the zones in a group of zones uses the same
       set of observations and localization ranges per observation.
    3.2. For each group of zones:
       3.2.1. Extract Y-matrix, observation vector, observation covariance
             (diagonal matrix), observation position, observation localization range.
       3.2.2. Initialize DistanceESMDA and calculate internal matrices in DistanceESMDA
              that does not depend on any field parameters. No need to re-calculate this
              multiple times for zones using the same observation data frame.
       3.2.3  For given zone:
              3.2.3.1  Get ERTBOX definition of the field parameter
                       (All field parameters belonging to the same zone
                       must always have the same ERTBOX definition).
                       If the ERTBOX is different from previous zone,
                       - transform observation to ERTBOX coordinates
                       - Calculate RHO matrix for one layer of field parameter
                       else
                       - No need to transform observations again or calculate RHO
                        for one layer again
              3.2.3.2  For each 3D field parameter belonging to the zone:
                       3.2.3.2.1 Get field parameter prior X matrix from storage
                       3.2.3.2.2 Update the 3D field parameter given X matrix,
                                 the Y matrix, the RHO matrix for one layer.
                                This step will split the update into one or more
                                batches of grid layers to handle restrictions
                                on available memory.
                        3.2.3.2.3 Update storage with posterior 3D field parameters
                       3.2.3.2.4 Write mean and stdev of resulting ensemble of posterior
                                 fields to files.

4. For distance-based for 2D fields:
    4.1 Extract Y-matrix, observation vector, C_D, position and localization ranges
        using the observation dataframe for 2D fields.
    4.2 Initialize DistanceESMDA and calculate field parameter
        independent internal matrices.
    4.3 For all surfaces (2D field parameters):
        4.3.1 Get X-matrix with prior field parameter ensemble from storage
        4.3.2 Update 2D fields:
            4.3.2.1  Transform position of observations to local 2D field
                     (surface) coordinates.
            4.3.2.2  Calculate RHO matrix
            4.3.2.3  Update 2D parameters given X-matrix, Y-matrix, RHO matrix
        4.3.3  Save posterior ensemble to storage
        4.3.4  Write 2D fields to surface file format (rms text format for surfaces)

5. For scalar parameters, loop over all scalar parameters:
    5.1 Initialize ordinary ESMDA
        (Need to initialize it for each scalar parameter to work properly)
        but this slows down the performance (something to improve here)
    5.2 Get scalar parameter from storage
    5.3 Update scalar parameter (no transformation)
    5.4 Save updated scalar parameter to storage


The optimization of the algorithm is here related to:
1. Calculate rho for one layer for a 3D field parameter and copy it into the other
   layers since distance is only calculated laterally (z coordinate is not used),
   but still observations can be limited to update a subset of all geological zones,
   so in that sense it may be depth dependent, but only on a coarse scale
   (zone scale). To take care of this we have the loop over zones.

   The calculation of rho for one layer can be optimized if observations
   (like seismic observation points) and the field parameter is aligned within ERTBOX.
   This means that both field parameter and seismic obs
   are regularly spaced and can be calculated in the same local coordinate system
   defined by ERTBOX.
   A local coordinate system is defined by the ERTBOX. The ERTBOX coordinate system
   is defined by a corner point (lower left) (xorigo, yorigo) in global coordinates
   (e.g. UTM coordinates). A rotation of the ERTBOX define the rotation of the local
   coordinate system relative to the global coordinate system.
   Note that in general ERTBOX coordinate system may vary from field parameter to field
   parameter, but usually the ERTBOX coordinate system is the same for all zones while
   the grid resolution of the ERTBOX grid may vary from zone to zone, in particular
   the number of layers, nz. Some calculations can be saved by knowing if all field
   parameters share the same definition of the ERTBOX coordinate system or not,
   which means that (xorigo, yorigo, rotation) is the same and that the grid resolution
   laterally (xinc, yinc, nx, ny ) is the same.
   A position of a field parameter witin ERTBOX coordinate is
     xpos =  (i+0.5) * xinc
     ypos =  (j+0.5) * yinc
   where local origo is (0,0) and is the same position as the rotation point
   (xorigo, yorigo) for ERTBOX relative to the global coordinate system.

   Observation position is transformed into the local ERTBOX coordinate system.
   The local ERTBOX coordinate system is defined by the ERTBOX parameters:
     a. Translate observation position to lower left corner point of ERTBOX
        (if ERTBOX is not rotated)
     b. Rotate to the local coordinate axes of the ERTBOX

   After position of observations are transformed into ERTBOX coordinates,
   it is possible to calculate distances between observations and field parameter
   grid cells in ERTBOX. If seismic data is aligned with the ERTBOX and the rows
   and columns of regularly spaced observation points are oriented in same way as
   the cell center points in the ERTBOX grid, it is relatively easy to calculate
   intervals of cell indices in i and j direction for the field parameter
   grid cells within range of a seismic observation. This can speed up the
   calculation a bit instead of having to calculate the distance between each
   individual seismic point and each individual field parameter position.

   Note: There are currently no information telling ERT which zone a field
   parameter belongs to. Field parameters generated by the APS facies method
   follow a naming standard where zone name is part of the field parameter name.
   Example: aps_<zone_name>_<gaussian_field_name> like aps_Valysar_GRF1.
   For field parameters related to petrophysical properties (permeability, porosity)
   which is updated simultaneously with gaussian fields used by the APS facies model to
   create facies, a proposal for a naming convention is to use
   <zone_name>_<facies_name>_<petrophysical_name> for field parameters for petrophysical
   fields related to one particular zone and facies.
   For cases where petrophysical properties is used as (transformed) gaussian fields
   and not dependent on facies, the naming convention could be
   <zone_name>_<petrophysical_name>.
   If we use a naming convention like this, ERT can without any additional information
   identify which 3D field parameters belongs to which geological zone. If no naming
   standard is required it is need for an additional information where zone name is
   specified per FIELD keyword in ERT config file.
   In the current script, which zone a field parameter group belongs to is
   hard-coded in a dict.


2. Calculation of Y, C_D and many steps in the distance-based update algorithm
   within DistanceESMDA can be done once for each field parameter group and not
   necessary to re-calculate for each batch of field parameters belonging to the
   same field parameter group. If multiple field parameters are influenced by the
   same set of observations, they can use the same Y, C_D and observation vector
   and thereby save run time.

3. Define batches of field parameters to be update simultaneously.
   Default can be to update all parameters in a parameter group together,
   but if the 3D fields are large, it may be necessary to split the field parameters
   into a set of layers of parameters. The calculation of batch size should then
   not only depend on current max available memory but rounded down to define a batch
   of complete set of layers of parameters and avoid including e.g 10 complete
   layers and a fraction of layer number 11.
   Then choose the batch size to be 10 layers instead.
   It will then be easier to copy rho for one complete layer into the
   10 complete layers of parameters for the batch.

4. Optimalization of multiplication of kalmain gain matrix K with innovation.
   Note: Need to evaluate the benefit of the various suggestions for
   optimization listed below:
   a. Since K with localization will contain a lot of 0 values due to the
      Schur product between rho and K before rho is multiplied,
      time for computation can be saved by avoiding multiplications and additions
      with 0. Particularly for cases with seismic 4D observations with many
      observations and short localization range around seismic points,
      rho will contain very large portion of elements with 0 as value.

   b. For all field parameter values (and other parameters) having 0 in
      DeltaM = M - mean(M) due to 0 prior uncertainty it is also not necessary
      to do the calculations. These two potential optimizations should be taken
      care of in implementation since the last two steps in the assimilation
      algorithm take most time due to large matrix multiplication when using seismic
      observations.

   c. The last optimalization potential is for cases where there are large portions
      of inactive field parameters within ERTBOX. Today, we have to calculate an
      update for all field parameters in ERTBOX regardless of whether they are active
      or not since which cells have active field parameter value vary from realization
      to realization in general case.
      If we modify the calculation of DeltaM = M - mean(M) and only use active values
      in mean(M), it will be possible to update the field parameters only for the
      active field parameter values within each realization.
      This will also reduce number of multiplications and additions.
      On the other hand, the filtering operation to select which field parameter
      to update also require some CPU time, so a test for various cases should be done.
      This optimalization is most relevant for geological zones modelled as top or
      base conform where number of active parameters in the geological zone vary
      from realization to realization due to structural model uncertainty.
      For zones with proportional gridding, also the number of active parameters
      may vary due to structural model uncertainty that varies from realization
      to realization, but in this case it is typically along staircase faults
      the number of active parameters may vary from realization to realization.

Drogon case:
1. Assume all observations except RFT and PLT data can update parameters for all zones.
   The lateral influence of an observation will limit how far laterally an observation
   will have effect on field parameters, but vertically the field parameters with
   (x,y) distance within the localization range is updated.
   For zone dependent data like RFT and PLT, they are only included in observation
   vector for the relevant zones. It is possible to experiment with zone dependence
   such that some observations are only able to update field parameters in one zone
   and not all 3 zones.
2. The ERTBOX coordinate system is the same for all zones
   (same lateral definition of ERTBOX). This means that coordinate transformation of
   position of observations is only necessary to be calculated once and
   calculation of rho for one layer can be done once as long as all zones use the
   same observations.
3. The size of the field parameters (nx * ny * nz_zone_dependent) is small enough
   that one usually don't have to split the update of a parameter group into smaller
   batches in this case.
4. The naming convention used by APS allow us to identify the zone name for each
   field parameter group but this is in general not the case and therefore a
   hard-coded dict in this script define the zone per field even though this
   is not necessary when only using field parameters coming from APS where zone name
   is part of the field parameter name in ERT.
5. Number of active parameters vary from realization to realization in Therys,
   but not in the other two zones. There are no large staircase faults that
   creates active/inactive field parameters from realization to realization.

What Drogon case cannot test:
1. A more efficient specification of seismic 4D data defined by a regular 2D grid
   aligned with the ERTBOX.
2. No observation related to regions or the reservoir as a whole.
3. No observation of fluid contacts
  (they are used as parameters with a prior distribution)
4. Drogon size of the field parameters are not very large compared
   with real reservoir cases.
5. Drogon does not limit updates of 2D SURFACE parameters to observations directly
   related to volume, but is updated by using all observations.
6. Cannot test the effect of having active/inactive values varying from realization
   to realization along staircase faults.
"""


def run_number():
    current_time = datetime.now()
    ref_date = datetime(2025, 9, 1, 0, 0, 0)
    time_difference = current_time - ref_date
    number = int(time_difference.total_seconds() // 60)
    return str(number)


# Turn on/off to store results in ERT storage
SAVE_UPDATE_TO_STORAGE = True
# SAVE_UPDATE_TO_STORAGE = False

# Turn on/off the two types of update
SKIP_GLOBAL_UPDATE = True
SKIP_DL_UPDATE = False
SKIP_MERGE_SUMMARY_OBS_FROM_RMS_AND_ERT = False
ALPHA = np.array([7.0])
ALPHA_LABEL = "_7_0"

ENSEMBLE_TAG = "DL_" + ALPHA_LABEL

# Create unique label for each ensemble created by running this script
ENSEMBLE_LABEL = run_number()

SEED_7_0 = 292749736
SEED_3_5 = 876292827
SEED_1_75 = 61952988
# SEED_7_0 =  8716671
# SEED_3_5 = 1874551
# SEED_1_75 = 78876655
# SEED_7_0 =  78771671
# SEED_3_5 =   565498551
# SEED_1_75 = 99121232

# SEED = 123456 # Test with same as in ERT config file for Drogon
SEED = SEED_7_0

# Only for temporary test of some pieces of the code
TESTS = False

# Turn on/off more extensive output to screed
DEBUG_PRINT = True

# Turn on/of use of seismic obs
USE_SEIS_OBS = False


if USE_SEIS_OBS:
    # Turn on/off to use correlated seismic observation error
    USE_SEISMIC_OBS_ERR_CORRELATION = False
else:
    # Always off when not using seismic obs
    USE_SEISMIC_OBS_ERR_CORRELATION = False

# Write seismic obs with perturations
WRITE_D_MATRIX = True
# Result directory for ROFF and irap files created for the mean and stdev of fields
OUTPUT_PATH = "TMP"
SCALAR_UPDATE_METHOD = "ADAPTIVE"  # ESMDA or ADAPTIVE

ERT_CONFIG_PATH = (
#    "/project/fmu/users/olia/drogon_20250623_11-50/resmod/ff/25.0.0/ert/model"
    "/project/fmu/users/olia/drogon_current/drogon_20250822_11-23/resmod/ff/25.0.0/ert/model"
)
# ERT_CONFIG_PATH = (
#    "/project/fmu/tutorial/drogon/resmod/ff/users/olia/"
#    + "drogon_20240709_15-30/resmod/ff/24.3.0/ert/model"
# )

# SCRATCH_PATH = "/scratch/fmu/olia/drogon_ahm_ertbox_base"
# SCRATCH_PATH = "/scratch/fmu/olia/drogon_ahm_500"
# SCRATCH_PATH = "/scratch/fmu/olia/drogon_ahm_facies_petro_ertbox_test"
SCRATCH_PATH = "/scratch/fmu/olia/drogon_ahm_current_base"
# Storage
# STORAGE_PATH = (
#    "/project/fmu/users/olia/drogon_20250623_11-50/"
#    + "resmod/ff/25.0.0/ert/output/drogon_ahm/storage"
# )
STORAGE_PATH = (
    "/project/fmu/users/olia/drogon_current/drogon_20250822_11-23/"
    + "resmod/ff/25.0.0/ert/output/drogon_ahm/storage"
)
#STORAGE_PATH = (
#    "/project/fmu/users/olia/drogon_20250623_11-50/"
#    + "resmod/ff/25.0.0/ert/output/drogon_ahm_ertbox/storage"
#)
# STORAGE_PATH = (
#    "/project/fmu/tutorial/drogon/resmod/ff/users/"
#    + "olia/drogon_20240709_15-30/resmod/ff/24.3.0/ert/"
#    + "output/drogon_ahm_facies_petro_ertbox_test/storage"
# )
EXPERIMENT_NAME = "ensemble_experiment"
# EXPERIMENT_NAME = "ensemble_experiment_ertbox"
ENSEMBLE_NAME = "ensemble"
# ENSEMBLE_NAME = "ensemble-post-global-102018corr_seis_7_0"
# ENSEMBLE_NAME = "ensemble-post-global-102070corr_seis_3_5"
# ENSEMBLE_NAME = "ensemble-post-102904corr_seis_DL_3_5"
# ENSEMBLE_NAME = "ensemble-post-104214no_seis_DL_7_0"
# ENSEMBLE_NAME = "ensemble-post-106519seis_DL2_7_0"
# ENSEMBLE_NAME = "ensemble-post-107056seis_DL2_3_5"
# ENSEMBLE_NAME = "ensemble-post-107279seis_DL3_7_0"
# ENSEMBLE_NAME = "ensemble-post-107354seis_DL3_3_5"
# ENSEMBLE_NAME = "ensemble-post-107476nocorrseis_DL2_7_0"
# ENSEMBLE_NAME = "ensemble-post-107595nocorrseis_DL2_3_5"
# ENSEMBLE_NAME = "ensemble-post-107952nocorrseis_DL3_7_0"
# ENSEMBLE_NAME = "ensemble-post-108001nocorrseis_DL3_3_5"
# ENSEMBLE_NAME = "ensemble-post-108871nocorrseis_DL4_7_0"
# ENSEMBLE_NAME = "ensemble-post-108912nocorrseis_DL4_3_5"
# ENSEMBLE_NAME = "ensemble-post-110003seis_DL4_3_5"
# ENSEMBLE_NAME = "ensemble-post-113747seis_nocorr_DL1_7_0"
# ENSEMBLE_NAME = "ensemble-post-114272seis_nocorr_DL1_3_5"
# ENSEMBLE_NAME = "ensemble-post-115814seis_nocorr_DL_R2000_7_0"
# ENSEMBLE_NAME = "ensemble-post-115958seis_nocorr_DL_R2000_3_5"
# ENSEMBLE_NAME = "ensemble-post-121949DL_R2000_7_0"
# ENSEMBLE_NAME = "ensemble-post-122244DL_R2000_3_5"
# ENSEMBLE_NAME = "ensemble-post-126770DL_R2000_3_5"
# For distance-based update
ENSEMBLE_NAME_UPDATE = "ensemble-post-" + ENSEMBLE_LABEL + ENSEMBLE_TAG

# For ordinary update
ENSEMBLE_NAME_UPDATE_GLOBAL = "ensemble-post-global-" + ENSEMBLE_LABEL + ENSEMBLE_TAG


TRUNCATION = 0.99
NLAYER_PER_BATCH = (
    33  # 66 layers in ERTBOX for Drogon which is split into 2 batches here
)
# Turn on/off calculation of rho based on localization or only use rho = 1
USE_LOCALIZATION = True

# Turn on/off export of field parameters into 3D parameter
# files in ROFF format to be read by RMS.
WRITE_STATS_PARAMS = True


# Common definition of ERTBOX for all zones in Drogon
XORIGO = 461505
YORIGO = 5926523
XINC = 75.0
YINC = 74.71
ROTATION = 29.88159640627248
NX = 92
NY = 146
NZ = 66
HANDEDNESS = (
    "right"  # Alternatives: "right" or "left" Drogon uses right handed for ERTBOX
)
ERTBOX_GRID_FILE_NAME = "ertbox.roff"
ERTBOX_GRID = xtgeo.grid_from_file(
    OUTPUT_PATH + "/" + ERTBOX_GRID_FILE_NAME, fformat="roff"
)
# Need to distinguish between observation groups.
# NOTE: This should be available from ERT, but is not yet?
# WellObs - Observations from wells with position (prod data etc)"
# GenObs  - Any observation having a location like seismic obs points,
#           gravimetry obs points
# RegionObs - Any observation without any specific coordinate for
#             its location (e.g. region or field prod data,
#             data for average over regions, FWL, GOC which are
#             defined per region)
# TODO: In this version of the test script all observations are assumed to have position
#       This may not always be the case?

# Get observations with position and localization ranges from external files
# created by running a script in RMS

OBS_FILE_PATH = "./"
OBS_FILE = "summary_obs_with_localization_attributes2.csv"
# OBS_FILE = "summary_obs_with_local2.csv"
# OBS_FILE = "summary_obs_with_localization_attributes.csv"
OBS_SUMMARY_WITH_LOCAL_ATTRIBUTES_FILE = OBS_FILE_PATH + OBS_FILE


# If we could have naming convention we could get zone name from parameter name.
# We have that for APS parameters:  aps_<zone_name>_<grf_name>),
# In general we don't. A proposal for naming convention for petrophysical
# field parameters (e.g for permeability or porosity) could be
# <zone_name>_<facies>_<petro_name> for petrophysical fields to be updated
# simultaneously with facies update from APS.
# For reservoir zones without facies modelling, the convention could
# be  <zone_name>_<petro_name>
# NOTE: An additional parameter in FIELD keyword with zone name can solve the problem
# if naming convention of 3D field parameters is not required.
# Storage should have zone name as an attribute to each field parameter.
# ZONES = ["Valysar"]
ZONES = ["Valysar", "Therys", "Volon"]
# Is used to assosiate fields with zones for cases where we need to distinguish
# between zones (due to different obs per zone). The first version of this script
# should use obs data frames that are equal for all zones.
# ZONE_PER_3D_FIELD_PARAM_GROUP = {
#     "aps_Valysar_GRF1": "Valysar",
#     "aps_Valysar_GRF2": "Valysar",
#     "aps_Valysar_GRF3": "Valysar",
#     "aps_Therys_GRF1": "Therys",
#     "aps_Therys_GRF2": "Therys",
#     "aps_Therys_GRF3": "Therys",
#     "aps_Volon_GRF1": "Volon",
#     "aps_Volon_GRF2": "Volon",
#     "aps_Volon_GRF3": "Volon",
# }

ZONE_PER_3D_FIELD_PARAM_GROUP = {
    "aps_Valysar_GRF1": "Valysar",
    "aps_Valysar_GRF2": "Valysar",
    "aps_Valysar_GRF3": "Valysar",
    "Valysar_Floodplain_PHIT": "Valysar",
    "Valysar_Channel_PHIT": "Valysar",
    "Valysar_Crevasse_PHIT": "Valysar",
    "Valysar_Floodplain_KLOGH": "Valysar",
    "Valysar_Channel_KLOGH": "Valysar",
    "Valysar_Crevasse_KLOGH": "Valysar",
    "aps_Therys_GRF1": "Therys",
    "aps_Therys_GRF2": "Therys",
    "aps_Therys_GRF3": "Therys",
    "Therys_Offshore_PHIT": "Therys",
    "Therys_Lowershoreface_PHIT": "Therys",
    "Therys_Uppershoreface_PHIT": "Therys",
    "Therys_Offshore_KLOGH": "Therys",
    "Therys_Lowershoreface_KLOGH": "Therys",
    "Therys_Uppershoreface_KLOGH": "Therys",
    "aps_Volon_GRF1": "Volon",
    "aps_Volon_GRF2": "Volon",
    "aps_Volon_GRF3": "Volon",
    "Volon_Channel_PHIT": "Volon",
    "Volon_Floodplain_PHIT": "Volon",
    "Volon_Channel_KLOGH": "Volon",
}
# NOTE: ERTBOX should be fetched from the storage, but currently I have specified
# this explicitly here.
#
# Usually ERTBOX definition is the same for all zones (even for NZ), but with the new
# possibility to have individual ERTBOX per geological zone, NZ will usually vary from
# 3D field parameter to 3D field parameter. In more rare cases also
# other ERTBOX definition parameters may vary.
#
# The 3D field parameters belonging to the same zone will also belong to same ERTBOX.
# Depending on observation and their dependency on geological zones, we can simplify
# the calculation of rho:
# 1. If no observations (from well production or seismic) depends on zone,
#    a common version of Y matrix can be used for all zones.
#    It means that localization which only here care about lateral distance (x,y)
#    but not depth will need a calculation of rho for only one grid layer of ERTBOX
#    and this can then be copied to all other layers for the rho for the
#    3D field parameters for the zone.
# 2. If some observations are not defined for all zones, it can be both for
#    seismic 4D and well production observation, then a new version of Y
#    is necessary for each zone.

# For some of the field parameters, the ERTBOX will be the same
# for all ERTBOX parameters. This is the case for field parameters
# coming from the same zone. By identifying which parameters belongs to the same zone
# we can avoid having to recalculate rho matrix for each of these 3D parameters.
# If we have to use batches of layers per field parameter, all the batches for all
# field parameters coming from the same zone can fill rho matrix from the same rho
# calculated for one layer of field parameter values.
# If all observations can update all zones, this can be done even more efficient
# since all 3D field parameters can use the same calculated rho for one single layer.
#
# When ERTBOX is different from field
# parameter to field parameter it is usually because NZ vary
# since the ERTBOX grids for the various zones represents approximately
# the lateral position of a geological grid zone and usually the different
# zones in the geological grid are represented by a multizone grid
# and multi-zone grids usually have the same lateral number of grid cells (NX, NY)
# in each zone. In the most general case, each geological zone has its own geogrid
# with its own orientation, position and resolution and this is possible to represent
# with individual specification of all the ERTBOX parameters per 3D field parameter.
# In Drogon case
# (running a version before 3D field parameters can have individual ERTBOX)
# all ERTBOX parameters are equal as in the case below.
# In the new extended implementation with individual ERTBOX per 3D field parameter,
# I assume each zone of Drogon will have its own value of NZ but the rest is the same.

SCALING_FUNCTION_NAME = (
    "gaspari-cohn"  # Alternatives: "gaussian", "exponential", "gaspari-cohn"
)

ERT_SEIS_OBS_PATH = ERT_CONFIG_PATH + "/" + "../input/observations/seismic"


# exponent in general exponential variogram model for simulation of
# correlated seismic observations
SEISMIC_GEN_EXP_POWER = 1.8
SEISMIC_OBS_AND_RESPONSE_DICT_PER_ZONE = {
    # Valysar related seismic observations and localization ranges
    # Range: 1000
    "Valysar": [
        {
            "name": "topvolantis",
            "obs_file": "meta--topvolantis_amplitude_mean_20200701_20180101_1.txt",
            "response_file": "share/results/points/topvolantis_amplitude_mean_20200701_20180101_1.txt",  # noqa
            "main_range": 2000.0,
            "perp_range": 2000.0,
            "anisotropy_angle": 0.0,
            "obs_err_variogram": "gen_exponential",
            "obs_err_power": SEISMIC_GEN_EXP_POWER,
            "obs_err_main_range": 2000.0,
            "obs_err_perp_range": 2000.0,
            "obs_err_anisotropy_angle": 0.0,
        },
        {
            "name": "basevolantis",
            "obs_file": "meta--basevolantis_amplitude_mean_20200701_20180101_1.txt",
            "response_file": "share/results/points/basevolantis_amplitude_mean_20200701_20180101_1.txt",  # noqa
            "main_range": 2000.0,
            "perp_range": 2000.0,
            "anisotropy_angle": 0.0,
            "obs_err_variogram": "gen_exponential",
            "obs_err_power": SEISMIC_GEN_EXP_POWER,
            "obs_err_main_range": 2000.0,
            "obs_err_perp_range": 2000.0,
            "obs_err_anisotropy_angle": 0.0,
        },
    ],
    # Therys related seismic observations and localization ranges
    "Therys": [
        {
            "name": "topvolantis",
            "obs_file": "meta--topvolantis_amplitude_mean_20200701_20180101_1.txt",
            "response_file": "share/results/points/topvolantis_amplitude_mean_20200701_20180101_1.txt",  # noqa
            "main_range": 2000.0,
            "perp_range": 2000.0,
            "anisotropy_angle": 0.0,
            "obs_err_variogram": "gen_exponential",
            "obs_err_power": SEISMIC_GEN_EXP_POWER,
            "obs_err_main_range": 2000.0,
            "obs_err_perp_range": 2000.0,
            "obs_err_anisotropy_angle": 0.0,
        },
        {
            "name": "basevolantis",
            "obs_file": "meta--basevolantis_amplitude_mean_20200701_20180101_1.txt",
            "response_file": "share/results/points/basevolantis_amplitude_mean_20200701_20180101_1.txt",  # noqa
            "main_range": 2000.0,
            "perp_range": 2000.0,
            "anisotropy_angle": 0.0,
            "obs_err_variogram": "gen_exponential",
            "obs_err_power": SEISMIC_GEN_EXP_POWER,
            "obs_err_main_range": 2000.0,
            "obs_err_perp_range": 2000.0,
            "obs_err_anisotropy_angle": 0.0,
        },
    ],
    # Volon related seismic observations and localization ranges
    "Volon": [
        {
            "name": "topvolantis",
            "obs_file": "meta--topvolantis_amplitude_mean_20200701_20180101_1.txt",
            "response_file": "share/results/points/topvolantis_amplitude_mean_20200701_20180101_1.txt",  # noqa
            "main_range": 2000.0,
            "perp_range": 2000.0,
            "anisotropy_angle": 0.0,
            "obs_err_variogram": "gen_exponential",
            "obs_err_power": SEISMIC_GEN_EXP_POWER,
            "obs_err_main_range": 2000.0,
            "obs_err_perp_range": 2000.0,
            "obs_err_anisotropy_angle": 0.0,
        },
        {
            "name": "basevolantis",
            "obs_file": "meta--basevolantis_amplitude_mean_20200701_20180101_1.txt",
            "response_file": "share/results/points/basevolantis_amplitude_mean_20200701_20180101_1.txt",  # noqa
            "main_range": 2000.0,
            "perp_range": 2000.0,
            "anisotropy_angle": 0.0,
            "obs_err_variogram": "gen_exponential",
            "obs_err_power": SEISMIC_GEN_EXP_POWER,
            "obs_err_main_range": 2000.0,
            "obs_err_perp_range": 2000.0,
            "obs_err_anisotropy_angle": 0.0,
        },
    ],
}
SEISMIC_OBS_AND_RESPONSE_DICT_FOR_2D_FIELDS = [
    {
        "name": "topvolantis",
        "obs_file": "meta--topvolantis_amplitude_mean_20200701_20180101_1.txt",
        "response_file": "share/results/points/topvolantis_amplitude_mean_20200701_20180101_1.txt",  # noqa
        "main_range": 2000.0,
        "perp_range": 2000.0,
        "anisotropy_angle": 0.0,
        "obs_err_variogram": "gen_exponential",
        "obs_err_power": SEISMIC_GEN_EXP_POWER,
        "obs_err_main_range": 2000.0,
        "obs_err_perp_range": 2000.0,
        "obs_err_anisotropy_angle": 0.0,
    },
    {
        "name": "basevolantis",
        "obs_file": "meta--basevolantis_amplitude_mean_20200701_20180101_1.txt",
        "response_file": "share/results/points/basevolantis_amplitude_mean_20200701_20180101_1.txt",  # noqa
        "main_range": 2000.0,
        "perp_range": 2000.0,
        "anisotropy_angle": 0.0,
        "obs_err_variogram": "gen_exponential",
        "obs_err_power": SEISMIC_GEN_EXP_POWER,
        "obs_err_main_range": 2000.0,
        "obs_err_perp_range": 2000.0,
        "obs_err_anisotropy_angle": 0.0,
    },
]


# SEIS_MAIN_RANGE = 100.0 # Case s100
# SEIS_PERP_RANGE = 100.0
# SEIS_MAIN_RANGE = 500.0 # Case s500
# SEIS_PERP_RANGE = 500.0
# SEIS_MAIN_RANGE = 2500.0 # Case s2500
# SEIS_PERP_RANGE = 2500.0
# SEIS_MAIN_RANGE = 1000.0  # Case s1000
# SEIS_PERP_RANGE = 1000.0
# SEIS_MAIN_RANGE = 1000.0  # Case srt300
# SEIS_PERP_RANGE = 1000.0
# SEIS_MAIN_RANGE = 300.0  # Case srt300
# SEIS_PERP_RANGE = 300.0
# SEIS_ANISOTROPY_ROTATION = 0.0

RFT_OBS_AND_RESPONSE_DICT = {
    # Both observations and response is fetched from runpath realizations
    "Valysar": {
        "obs_file": "share/results/tables/rft_ert.csv",
        "response_file": "share/results/tables/rft_ert.csv",
        "main_range": 1500.0,
        "perp_range": 1500.0,
        "anisotropy_angle": 0.0,
    },
    "Therys": {
        "obs_file": "share/results/tables/rft_ert.csv",
        "response_file": "share/results/tables/rft_ert.csv",
        "main_range": 1500.0,
        "perp_range": 1500.0,
        "anisotropy_angle": 0.0,
    },
    "Volon": {
        "obs_file": "share/results/tables/rft_ert.csv",
        "response_file": "share/results/tables/rft_ert.csv",
        "main_range": 1500.0,
        "perp_range": 1500.0,
        "anisotropy_angle": 0.0,
    },
}
RFT_FILENAME = "rft_ert.csv"
USE_RFT_OBS = False

USE_TRACER_OBS = False
TRACER_OBS_FILENAME = "tracer_obs.txt"
TRACER_RESPONSE_FILENAME = "drogon_tracer_sim_1.txt"


def monitor_performance(func):
    """Decorator to monitor time only"""

    def wrapper(*args, **kwargs):
        start_time = time.perf_counter()

        result = func(*args, **kwargs)

        end_time = time.perf_counter()

        print(f"Function {func.__name__}:")
        print(f"  Total time: {end_time - start_time:.2f}s")

        return result

    return wrapper


def read_observations_with_localization_attributes(
    filename: str,
) -> pl.DataFrame:
    """
    Read a csv file with space as separator and generated by RMS containing all
    observations of type SUMMARY with (x,y) position in global (UTM) coordinates
    and localization ranges.
    Return a dataframe.
    """
    df = pl.read_csv(filename, separator=" ")

    # Automatically cast all Float64 columns to Float32
    df = df.with_columns(
        [
            pl.col(col).cast(pl.Float32)
            for col, dtype in df.schema.items()
            if dtype == pl.Float64
        ]
    )
    return df


def read_seis_obs_and_response_for_3D_fields_per_zone(
    iens_active_index: npt.NDArray[np.int32],
    seismic_obs_and_response_per_zone: dict[str, dict],
) -> dict[pl.DataFrame]:
    """
    Read seismic obs from files with position of obs and read response from RUNPATH
    """
    # Check if all zones use the same observations or not
    zone_dependent_seis_obs = False
    keys = list(seismic_obs_and_response_per_zone.keys())
    first_key = keys[0]
    prev_seis_data_list = seismic_obs_and_response_per_zone[first_key]
    for zone_name, seis_data_list in seismic_obs_and_response_per_zone.items():
        if seis_data_list != prev_seis_data_list:
            zone_dependent_seis_obs = True
            break
    if DEBUG_PRINT:
        if zone_dependent_seis_obs:
            print("Seismic observations are zone dependent")
        else:
            print("Seismic observations are not zone dependent")

    # Read seismic obs
    seis_obs_dict = {}
    for zone_name, data_list in seismic_obs_and_response_per_zone.items():
        print(f"Read seismic observations for zone {zone_name}")
        for i, data_dict in enumerate(data_list):
            print(f" Seismic observation {data_dict['name']}")
            filename = data_dict["obs_file"]
            main_seis_range = data_dict["main_range"]
            perp_seis_range = data_dict["perp_range"]
            anisotropy_angle_seis = data_dict["anisotropy_angle"]
            seis_obs_name = data_dict["name"]
            full_filename = ERT_SEIS_OBS_PATH + "/" + filename
            df_zone = pl.read_csv(full_filename, separator=",")

            # Remove columns not used
            df_zone = df_zone.drop(["REGION"])
            # Rename columns
            rename_dict = {
                "X_UTME": "xpos",
                "Y_UTMN": "ypos",
                "OBS": "observations",
                "OBS_ERROR": "std",
            }
            df_zone = df_zone.rename(rename_dict)

            # Add columns with localization ranges
            # Note: The values in response_key and observation_key is here
            # arbitrarily set to 'SEISMIC'. It is not used for anything but makes
            # it easier to look at the output of the dataframe.
            df_zone = df_zone.with_columns(
                [
                    pl.lit("SEISMIC").alias("response_key"),
                    pl.lit("Undefined").alias("index"),
                    pl.lit(seis_obs_name).alias("observation_key"),
                    pl.lit(0.0).alias("min_error"),
                    pl.lit(0.0).alias("max_error"),
                    pl.lit(main_seis_range).alias("main_range"),
                    pl.lit(perp_seis_range).alias("perp_range"),
                    pl.lit(anisotropy_angle_seis).alias("anisotropy_angle"),
                    pl.lit(zone_name).alias("zone_name"),
                    pl.lit(0.0).alias("influence_area"),
                ]
            )
            # Automatically cast all Float64 columns to Float32
            df_zone = df_zone.with_columns(
                [
                    pl.col(col).cast(pl.Float32)
                    for col, dtype in df_zone.schema.items()
                    if dtype == pl.Float64
                ]
            )
            if i == 0:
                seis_obs_dict[zone_name] = df_zone
            else:
                # Add dataframe of observations
                df = seis_obs_dict[zone_name]
                seis_obs_dict[zone_name] = pl.concat([df, df_zone], how="vertical")

    # Read seismic response per obs
    seis_response_dict = {}
    for zone_name, data_list in seismic_obs_and_response_per_zone.items():
        print(f"Read seismic response for zone {zone_name}")
        for i, data_dict in enumerate(data_list):
            print(f" Seismic response for observations {data_dict['name']}")
            response_file_name = data_dict["response_file"]

            first = True
            for real_number in iens_active_index:
                path_real = (
                    SCRATCH_PATH + "/realization-" + str(real_number) + "/iter-0/"
                )
                full_filename = path_real + response_file_name
                file = Path(full_filename)
                assert file.exists()  # storage and scratch disk must be consistent

                col_name = str(real_number)
                df_seis_real = pl.read_csv(
                    full_filename, has_header=False, new_columns=[col_name]
                )
                df_seis_real = df_seis_real.with_columns(
                    [
                        pl.col(col).cast(pl.Float32)
                        for col, dtype in df_seis_real.schema.items()
                        if dtype == pl.Float64
                    ]
                )
                if first:
                    df_seis_response = df_seis_real
                    first = False
                else:
                    df_seis_response = df_seis_response.hstack(df_seis_real)

            if i == 0:
                seis_response_dict[zone_name] = df_seis_response
            else:
                # Add dataframe of responses for current seismic observation set
                df = seis_response_dict[zone_name]
                seis_response_dict[zone_name] = pl.concat(
                    [df, df_seis_response], how="vertical"
                )

    # Merge observation dataframe with response dataframe
    df_obs_and_response_per_zone = {}
    for zone_name, df_obs in seis_obs_dict.items():
        df_response = seis_response_dict[zone_name]
        df_obs_and_response_per_zone[zone_name] = df_obs.hstack(df_response)
    return df_obs_and_response_per_zone


def read_seis_obs_and_response_for_2D_fields(
    iens_active_index: npt.NDArray[np.int32],
    seismic_obs_and_response_for_2D_fields: list[dict],
) -> dict[pl.DataFrame]:
    """
    Read seismic obs from files with position of obs and read response from RUNPATH
    """
    print("Read seismic observations for 2D fields")
    for i, data_dict in enumerate(seismic_obs_and_response_for_2D_fields):
        seis_obs_name = data_dict["name"]
        print(f" Read seismic observation data set {seis_obs_name}")
        obs_filename = data_dict["obs_file"]
        main_seis_range = data_dict["main_range"]
        perp_seis_range = data_dict["perp_range"]
        anisotropy_angle_seis = data_dict["anisotropy_angle"]
        full_filename = ERT_SEIS_OBS_PATH + "/" + obs_filename
        df_current = pl.read_csv(full_filename, separator=",")

        # Remove columns not used
        df_current = df_current.drop(["REGION"])
        # Rename columns
        rename_dict = {
            "X_UTME": "xpos",
            "Y_UTMN": "ypos",
            "OBS": "observations",
            "OBS_ERROR": "std",
        }
        df_current = df_current.rename(rename_dict)

        # Add columns with localization ranges
        # Note: The values in response_key and observation_key is here
        # arbitrarily set to 'SEISMIC'. It is not used for anything but makes
        # it easier to look at the output of the dataframe.
        df_current = df_current.with_columns(
            [
                pl.lit("SEISMIC").alias("response_key"),
                pl.lit("Undefined").alias("index"),
                pl.lit(seis_obs_name).alias("observation_key"),
                pl.lit(0.0).alias("min_error"),
                pl.lit(0.0).alias("max_error"),
                pl.lit(main_seis_range).alias("main_range"),
                pl.lit(perp_seis_range).alias("perp_range"),
                pl.lit(anisotropy_angle_seis).alias("anisotropy_angle"),
                pl.lit("Undefined").alias("zone_name"),
                pl.lit(0.0).alias("influence_area"),
            ]
        )
        # Automatically cast all Float64 columns to Float32
        df_current = df_current.with_columns(
            [
                pl.col(col).cast(pl.Float32)
                for col, dtype in df_current.schema.items()
                if dtype == pl.Float64
            ]
        )
        if i == 0:
            df_seis_obs = df_current
        else:
            # Add dataframe of observations
            df_seis_obs = pl.concat([df_seis_obs, df_current], how="vertical")

    # Read seismic response per obs
    for i, data_dict in enumerate(seismic_obs_and_response_for_2D_fields):
        seis_obs_name = data_dict["name"]
        print(f" Read seismic response for {seis_obs_name}")
        response_file_name = data_dict["response_file"]

        first = True
        for real_number in iens_active_index:
            path_real = SCRATCH_PATH + "/realization-" + str(real_number) + "/iter-0/"
            full_filename = path_real + response_file_name
            file = Path(full_filename)
            assert file.exists()  # storage and scratch disk must be consistent

            col_name = str(real_number)
            df_seis_real = pl.read_csv(
                full_filename, has_header=False, new_columns=[col_name]
            )
            df_seis_real = df_seis_real.with_columns(
                [
                    pl.col(col).cast(pl.Float32)
                    for col, dtype in df_seis_real.schema.items()
                    if dtype == pl.Float64
                ]
            )
            if first:
                df_current = df_seis_real
                first = False
            else:
                df_current = df_current.hstack(df_seis_real)

        if i == 0:
            df_seis_response = df_current
        else:
            # Add dataframe of responses for current seismic observation set
            df_seis_response = pl.concat([df_seis_response, df_current], how="vertical")

    # Merge observation dataframe with response dataframe
    df_obs_and_response = df_seis_obs.hstack(df_seis_response)
    return df_obs_and_response


def read_rft_obs_and_response(
    iens_active_index: npt.NDArray[np.int32],
    rft_input_spec: dict[str, dict],
) -> dict[pl.DataFrame]:
    # Observations are saved in runpath in same file as response
    # Here get the observations for each zone from the first
    # available realization
    for real_number in iens_active_index:
        path_real = SCRATCH_PATH + "/realization-" + str(real_number) + "/iter-0/"
        if Path(path_real).exists():
            first_existing_real_number = real_number
            print(f"First realization number: {first_existing_real_number}")
            break

    df_rft_obs_and_response_dict = {}
    for zone_name, rft_data_dict in rft_input_spec.items():
        path_real = (
            SCRATCH_PATH
            + "/realization-"
            + str(first_existing_real_number)
            + "/iter-0/"
        )

        obs_file_name = rft_data_dict["obs_file"]
        main_range = rft_data_dict["main_range"]
        perp_range = rft_data_dict["perp_range"]
        anisotropy_angle = rft_data_dict["anisotropy_angle"]
        full_filename = path_real + obs_file_name
        file = Path(full_filename)
        assert file.exists()

        df_rft_input = pl.read_csv(full_filename, separator=",")

        # Get only lines with current zone
        df_rft_current_zone = df_rft_input.filter(pl.col("zone") == zone_name)

        # Remove columns not used
        remove_columns = [
            "order",
            "measured_depth",
            "true_vertical_depth",
            "swat",
            "sgas",
            "soil",
            "valid_zone",
            "i",
            "j",
            "k",
            "time",
            "report_step",
            "well",
            "is_active",
        ]
        df_rft_current_zone = df_rft_current_zone.drop(remove_columns)

        print(f"Initialize dataframe for rft for zone {zone_name}")
        # Rename columns
        rename_dict = {
            "utm_x": "xpos",
            "utm_y": "ypos",
            "observed": "observations",
            "error": "std",
            "zone": "zone_name",
        }
        df_rft_current_zone = df_rft_current_zone.rename(rename_dict)

        # Add columns with localization ranges and other columns that need to be present
        df_rft_current_zone = df_rft_current_zone.with_columns(
            [
                pl.lit("RFT").alias("response_key"),
                pl.lit("Undefined").alias("index"),
                pl.lit("RFT").alias("observation_key"),
                pl.lit(0.0).alias("min_error"),
                pl.lit(0.0).alias("max_error"),
                pl.lit(main_range).alias("main_range"),
                pl.lit(perp_range).alias("perp_range"),
                pl.lit(anisotropy_angle).alias("anisotropy_angle"),
            ]
        )

        # Re-arrange columns to match dataframes from well production
        # data and seismic data
        new_order_of_columns = [
            "response_key",
            "index",
            "observation_key",
            "observations",
            "std",
            "min_error",
            "max_error",
            "xpos",
            "ypos",
            "main_range",
            "perp_range",
            "anisotropy_angle",
            "zone_name",
        ]
        df_rft_current_zone = df_rft_current_zone.select(new_order_of_columns)
        df_rft_obs_and_response_dict[zone_name] = df_rft_current_zone

    # Read ensemble of responses for RFT pressure
    for zone_name, rft_data_dict in rft_input_spec.items():
        response_file_name = rft_data_dict["response_file"]
        df_obs = df_rft_obs_and_response_dict[zone_name]

        for real_number in iens_active_index:
            path_real = SCRATCH_PATH + "/realization-" + str(real_number) + "/iter-0/"
            full_filename = path_real + response_file_name
            file = Path(full_filename)
            assert file.exists()

            df_rft_response = pl.read_csv(full_filename, separator=",")

            # Get only lines with current zone
            df_rft_response = df_rft_response.filter(pl.col("zone") == zone_name)

            # Set `pressure` to None where `is_active` is False
            # NOTE: Observations set to None will be removed before the final
            #       observation vector and the Y-matrix is defined.
            df_rft_response = df_rft_response.with_columns(
                pl.when(pl.col("is_active") == False)  # noqa
                .then(None)  # Set to None
                .otherwise(pl.col("pressure"))  # Keep existing value
                .alias("pressure")  # Update the 'pressure' column
            )
            df_rft_response = df_rft_response.select(["pressure", "utm_x", "utm_y"])
            rename_dict = {
                "utm_x": "xpos",
                "utm_y": "ypos",
            }
            df_rft_response = df_rft_response.rename(rename_dict)

            df_tmp1 = df_obs.select(["xpos", "ypos"])
            df_tmp2 = df_rft_response.select(["xpos", "ypos"])
            if df_tmp1.equals(df_tmp2):
                # Observations and response have same same position
                # Add column with response values for the observations for this zone
                df_rft_response = df_rft_response.select(["pressure"])
                df_rft_response = df_rft_response.rename({"pressure": str(real_number)})
                df_obs = df_obs.hstack(df_rft_response)
                df_rft_obs_and_response_dict[zone_name] = df_obs
        # Add a column at end after all responses to make it having same layout as
        # dataframe for obs from production wells
        df_obs = df_obs.with_columns([pl.lit(0.0).alias("influence_area")])

        # Convert to 32 bits float
        df_obs = df_obs.with_columns(
            [
                pl.col(col).cast(pl.Float32)
                for col, dtype in df_obs.schema.items()
                if dtype == pl.Float64
            ]
        )

        df_rft_obs_and_response_dict[zone_name] = df_obs
    return df_rft_obs_and_response_dict


def read_tracer_obs(
    obs_filename: str,
    response_filename: str,
    iens_active_index: npt.NDArray[np.int32],
) -> dict[pl.DataFrame]:
    file = Path(obs_filename)
    assert file.exists()
    print(f"  Read file: {obs_filename}")
    df_tr = pl.read_csv(obs_filename, separator=" ")
    # Add columns to be compatible with other observation dataframes
    # Note: The values in response_key and observation_key is here
    # arbitrarily set to 'TRACER'. It is not used for anything but makes
    # it easier to look at the output of the dataframe.

    df_tr = df_tr.with_columns(
        [
            pl.lit("TRACER").alias("response_key"),
            pl.lit("Undefined").alias("index"),
            pl.lit("TRACER").alias("observation_key"),
            pl.lit(0.0).alias("min_error"),
            pl.lit(0.0).alias("max_error"),
            pl.lit(0.0).alias("influence_area"),
        ]
    )
    # Re-arrange columns to match dataframes from well production
    # data and seismic data
    new_order_of_columns = [
        "response_key",
        "index",
        "observation_key",
        "observations",
        "std",
        "min_error",
        "max_error",
        "xpos",
        "ypos",
        "main_range",
        "perp_range",
        "anisotropy_angle",
    ]
    df_tr = df_tr.select(new_order_of_columns)

    # Read ensemble of realizations of response from runpath on scratch disk
    for real_number in iens_active_index:
        path_real = (
            SCRATCH_PATH + "/realization-" + str(real_number) + "/iter-0/tracer/"
        )
        full_filename = path_real + response_filename
        file = Path(full_filename)
        assert file.exists()

        col_name = str(real_number)
        df_tr_response_one_real = pl.read_csv(
            full_filename, has_header=False, new_columns=[col_name]
        )
        # Add response data to dataframe for tracer
        df_tr = df_tr.hstack(df_tr_response_one_real)

    # Add a column at end after all responses to make it having same layout as
    # dataframe for obs from production wells
    df_tr = df_tr.with_columns([pl.lit(0.0).alias("influence_area")])

    # Convert to 32 bit float
    df_tr = df_tr.with_columns(
        [
            pl.col(col).cast(pl.Float32)
            for col, dtype in df_tr.schema.items()
            if dtype == pl.Int64
        ]
    )
    df_tr = df_tr.with_columns(
        [
            pl.col(col).cast(pl.Float32)
            for col, dtype in df_tr.schema.items()
            if dtype == pl.Float64
        ]
    )

    return df_tr


def transform_to_local_coordinates_3D(
    ertbox_params: ErtboxParameters,
    xpos: npt.NDArray[np.double],
    ypos: npt.NDArray[np.double],
    ellipse_anisotropy_angle: npt.NDArray[np.double],
) -> tuple[npt.NDArray[np.double], npt.NDArray[np.double], npt.NDArray[np.double]]:
    # Translate
    x1 = xpos - ertbox_params.origin[0]
    y1 = ypos - ertbox_params.origin[1]
    rotation_of_ertbox = ertbox_params.rotation_angle
    rotation_angle = rotation_of_ertbox * np.pi / 180.0
    cos_theta = np.cos(rotation_angle)
    sin_theta = np.sin(rotation_angle)
    x2 = x1 * cos_theta + y1 * sin_theta
    y2 = -x1 * sin_theta + y1 * cos_theta
    # Both angles measured anti-clock from global coordinate systems x-axis in degrees
    ellipse_anisotropy_transformed = ellipse_anisotropy_angle - rotation_of_ertbox
    return x2, y2, ellipse_anisotropy_transformed


def transform_to_local_coordinates_2D(
    surface_coord_dict: dict,
    xpos: npt.NDArray[np.double],
    ypos: npt.NDArray[np.double],
    ellipse_anisotropy_angle: npt.NDArray[np.double],
) -> tuple[npt.NDArray[np.double], npt.NDArray[np.double], npt.NDArray[np.double]]:
    # Translate
    x1 = xpos - surface_coord_dict["xorigo"]
    y1 = ypos - surface_coord_dict["yorigo"]
    rotation_of_ertbox = surface_coord_dict["rotation"]
    rotation_angle = rotation_of_ertbox * np.pi / 180.0
    cos_theta = np.cos(rotation_angle)
    sin_theta = np.sin(rotation_angle)
    x2 = x1 * cos_theta + y1 * sin_theta
    y2 = -x1 * sin_theta + y1 * cos_theta
    # Both angles measured anti-clock from global coordinate systems x-axis in degrees
    ellipse_anisotropy_transformed = ellipse_anisotropy_angle - rotation_of_ertbox
    return x2, y2, ellipse_anisotropy_transformed


def perturb_seismic_2D_observations_with_correlated_errors(
    # Errors are simulated and added to all observations.
    # For seismic observations (2D point set), the error is simulated with
    # spatial correlations.
    ensemble_size: int,
    obs_xpos: npt.NDArray[np.double],
    obs_ypos: npt.NDArray[np.double],
    observations: npt.NDArray[np.double],
    obs_std: npt.NDArray[np.double],
    ertbox_params: ErtboxParameters,
    start_seed,
    variogram_name: str = "gen_exponential",
    obs_err_main_corr_range: float = 1000.0,
    obs_err_perp_corr_range: float = 1000.0,
    obs_err_anisotropy_angle: float = 0.0,
    obs_err_power_genexponential_variogram: float = SEISMIC_GEN_EXP_POWER,
    alpha: float = 1.0,
) -> npt.NDArray[np.double]:
    """Create a matrix D for seismic 2D observations with spatially correlated error.

    The observations come with a standard deviation. The observation error
    is assumed to be multinormally distributed and defined by a specified
    spatial correlation function with a correlation ellipse.
    The output is:
    observation value + std_obs * sqrt(alpha) * normalized observation error

    The scaling, alpha is used as part of multiple-data assimilation with
    inflation factor for covariance, alpha
    The normalized observation error has mean = 0 and standard deviation = 1
    but is spatially correlated.

    The method is as follows:
    - Transform observations into a coordinate system defined by the ertbox.
    - Simulate ensemble_size number of 2D unconditional Gaussian random field with
        0 expectation and variance 1 with specified spatial correlation in the
        coordinate system defined by ertbox with a grid resolution (xinc, yinc)
        and size (nx, ny) equal to the specified ertbox grid definition.
    - Extract the simulated values in all observation positions for all realizations
    - Return the D matrix. Each column of D matrix consists of the perturbed
      observations scaled by sqrt(alpha).



    In the Emerick (2013) paper, the matrix D is defined in section 6.
    See section 2(b) of the ES-MDA algorithm in the paper.

    Parameters
    ----------
    ensemble_size : int
        The ensemble size, i.e., the number of columns in the returned array,
        which is of shape (num_observations, ensemble_size).
    alpha : float
        The covariance inflation factor. The sequence of alphas should
        obey the equation sum_i (1/alpha_i) = 1. However, this is NOT enforced
        in this method call. The user/caller is responsible for this.

    Returns
    -------
    D : np.ndarray
        Each column consists of perturbed observations with observation error
        scaled by sqrt(alpha).

    """
    import gaussianfft as sim

    # Transform observations to ertbox coordinates.
    xpos_transf, ypos_transf, _ = transform_to_local_coordinates_3D(
        ertbox_params,
        obs_xpos,
        obs_ypos,
        0.0,
    )
    # Setup simulation og 2D field
    if variogram_name not in ["gen_exponential", "exponential", "gaussian"]:
        raise ValueError(f"Variogram {variogram_name} is not implemented.")
    if variogram_name == "gen_exponential":
        variogram = sim.variogram(
            variogram_name,
            main_range=obs_err_main_corr_range,
            perp_range=obs_err_perp_corr_range,
            azimuth=obs_err_anisotropy_angle,
            power=obs_err_power_genexponential_variogram,
        )
    elif variogram_name in ["exponential", "gaussian"]:
        variogram = sim.variogram(
            variogram_name,
            main_range=obs_err_main_corr_range,
            perp_range=obs_err_perp_corr_range,
            azimuth=obs_err_anisotropy_angle,
        )

    # Simulate realization of gaussian 2D fiel and define D matrix
    nobs = len(observations)
    assert len(obs_xpos) == nobs
    assert len(obs_ypos) == nobs
    assert len(obs_std) == nobs
    sim.seed(start_seed)
    nx = ertbox_params.nx
    ny = ertbox_params.ny
    xinc = ertbox_params.xinc
    yinc = ertbox_params.yinc
    assert nobs <= (nx * ny)
    print("  Simulate correlated seismic observation errors and define D-matrix")
    D = np.zeros((nobs, ensemble_size), dtype=np.float64)
    gaussfields = np.zeros((nx * ny, ensemble_size), dtype=np.float32)
    for i in range(ensemble_size):
        # This result has F-indexing
        gaussfields[:, i] = sim.simulate(
            variogram, nx, xinc, ny, yinc
        )  # (nx*ny ,nreal)
    i_indx = np.round(xpos_transf / xinc).astype(int)  # (nobs)
    j_indx = np.round(ypos_transf / yinc).astype(int)
    indx = i_indx + j_indx * nx
    values = gaussfields[indx, :]
    for i in range(ensemble_size):
        D[:, i] = observations[:] + obs_std[:] * values[:, i] * np.sqrt(alpha)
    return D


def update_with_esmda(
    iens_active_index: npt.NDArray[np.int32],
    realizations: list[int],
    obs_and_response_df_per_zone_dict: dict,
    obs_and_response_df_for_2D_fields,
    seed: int,
    alpha: float,
    field_params_3D_list_per_zone: dict[str, list[str]],
    field_params_2D_list: list[str],
    scalar_param_list: list[str],
    non_updatable_list: list[str],
    ertbox_per_field_param_dict: dict[ErtboxParameters],
) -> None:
    ensemble_size = iens_active_index.shape[0]
    # Define a dict with group of zones having exactly the same observations
    group_of_zones_with_same_obs_dict = define_group_of_zones_with_same_observations(
        obs_and_response_df_per_zone_dict
    )
    # Use the ertbox definition of one of the fields for simulation of
    # seismic observation error correlations. Not important which ertbox to use.
    zone_group_number = 1
    zone_list = group_of_zones_with_same_obs_dict[zone_group_number]
    first_zone_name_in_group = zone_list[0]
    field_param_name = field_params_3D_list_per_zone[first_zone_name_in_group][0]
    ertbox_params_for_seismic_error_simulation = ertbox_per_field_param_dict[
        field_param_name
    ]

    print("")
    print(" Start update of all 3D field parameters for all zones.")
    previous_ertbox_params = None
    for zone_group_number, zone_list in group_of_zones_with_same_obs_dict.items():
        print("")
        print(f" Zone group {zone_group_number} with:")
        for zone_name in zone_list:
            print(f"   {zone_name}")

        first_zone_name_in_group = zone_list[0]
        df_obs_this_zone = obs_and_response_df_per_zone_dict[first_zone_name_in_group]

        # Calculate Y matrix etc
        print(
            " Define response matrix Y, obs vectors for zone group "
            f"{zone_group_number}."
        )
        (
            Y,
            observations,
            C_D,
            _,
            _,
            _,
            _,
            _,
            D,
        ) = define_response_and_observations(
            iens_active_index,
            df_obs_this_zone,
            start_seed=seed,
            zone_group_number=zone_group_number,
            zone_list=zone_list,
            use_obs_err_correlations=USE_SEISMIC_OBS_ERR_CORRELATION,
            seis_obs_and_response_dict_per_zone=SEISMIC_OBS_AND_RESPONSE_DICT_PER_ZONE,
            alpha=alpha,
            ertbox_params=ertbox_params_for_seismic_error_simulation,
        )
        nobs = Y.shape[0]
        print(
            " Number of observations used for zone group "
            f"{zone_group_number} is {nobs}."
        )

        if DEBUG_PRINT:
            print(f"  Y.shape:  {Y.shape}")
            print(f"  observations.shape:  {observations.shape}")
            print(f"  covariance_obs_error.shape:  {C_D.shape}")

        for zone_name in zone_list:
            # For current zone all field parameters should have same size
            # and belong to same ERTBOX coordinate system
            print(f"  Zone name: {zone_name}")
            field_param_list_current_zone = field_params_3D_list_per_zone[zone_name]
            print("   Field params for this zone:")
            for fname in field_param_list_current_zone:
                print(f"    {fname}")
            ertbox_params = ertbox_per_field_param_dict[
                field_param_list_current_zone[0]
            ]

            nx = ertbox_params.nx
            ny = ertbox_params.ny
            nz = ertbox_params.nz
            print(f"   ERTBOX grid size: nx={nx}, ny={ny}, nz={nz}")
            # Check if ertbox definition is the same. Have used same test as
            # for distance-based for simplicity but it is is too restrictive here
            # since only nx, ny,nz is relevant to compare here...
            if previous_ertbox_params is None or (
                not ertbox_has_same_xy_layout(ertbox_params, previous_ertbox_params)
            ):
                previous_ertbox_params = copy.copy(ertbox_params)
                nx = ertbox_params.nx
                ny = ertbox_params.ny
                nz = ertbox_params.nz
                print(f"   Dimension of 3D field parameter: ({nx},{ny},{nz})")

            for field_param_name in field_param_list_current_zone:
                print(f"    Field name: {field_param_name} for zone {zone_name}")

                # All zones in this list use the same set of observations.
                # NOTE:
                # However, ESMDA implementation will require that ESMDA instance
                # is initialized each time it is used to update a parameter.
                # The result will be wrong if this is not done?
                print(
                    "    Initialize ESMDA for 3D fields for zone group number "
                    f"{zone_group_number} zone {zone_name} field "
                    f"{field_param_name}."
                )
                esmda_smoother = ESMDA(
                    covariance=C_D,
                    observations=observations,
                    alpha=alpha,
                    seed=seed,
                )
                if SAVE_UPDATE_TO_STORAGE:
                    with open_storage(STORAGE_PATH, mode="w") as storage:
                        experiment = storage.get_experiment_by_name(EXPERIMENT_NAME)
                        prior_ensemble = experiment.get_ensemble_by_name(ENSEMBLE_NAME)
                        X_prior = prior_ensemble.load_parameters_numpy(
                            field_param_name, realizations
                        )

                        X_prior, X_post = update_3D_field_with_esmda(
                            esmda_smoother,
                            field_param_name,
                            X_prior,
                            Y,
                            nx,
                            ny,
                            nz,
                            D=D,
                        )

                        # Store the field parameter to storage
                        post_ensemble = experiment.get_ensemble_by_name(
                            ENSEMBLE_NAME_UPDATE_GLOBAL
                        )
                        print(
                            f"    Store {field_param_name} to "
                            f"{ENSEMBLE_NAME_UPDATE_GLOBAL}"
                        )
                        nparam = X_post.shape[0]
                        print(
                            f"    Number of parameters in {field_param_name}: {nparam}"
                        )
                        print("")
                        post_ensemble.save_parameters_numpy(
                            parameters=X_post,
                            param_group=field_param_name,
                            iens_active_index=iens_active_index,
                        )
                else:
                    with open_storage(STORAGE_PATH, mode="r") as storage:
                        experiment = storage.get_experiment_by_name(EXPERIMENT_NAME)
                        prior_ensemble = experiment.get_ensemble_by_name(ENSEMBLE_NAME)
                        X_prior = prior_ensemble.load_parameters_numpy(
                            field_param_name, realizations
                        )

                        X_prior, X_post = update_3D_field_with_esmda(
                            esmda_smoother,
                            field_param_name,
                            X_prior,
                            Y,
                            nx,
                            ny,
                            nz,
                            D=D,
                        )

                # Write updated 3D field parameter to file
                if WRITE_STATS_PARAMS:
                    X_prior_3D = X_prior.reshape((nx, ny, nz, ensemble_size))
                    X_post_3D = X_post.reshape((nx, ny, nz, ensemble_size))

                    # Optionally, calculate mean stdev and write to file
                    mean_prior_param = np.mean(X_prior_3D, axis=3)
                    stdev_prior_param = np.std(X_prior_3D, axis=3)
                    mean_post_param = np.mean(X_post_3D, axis=3)
                    stdev_post_param = np.std(X_post_3D, axis=3)
                    param_dict = {}
                    key = "global_mean_prior_" + field_param_name
                    param_dict[key] = mean_prior_param

                    key = "global_std_prior_" + field_param_name
                    param_dict[key] = stdev_prior_param

                    key = "global_mean_post_" + field_param_name
                    param_dict[key] = mean_post_param

                    key = "global_std_post_" + field_param_name
                    param_dict[key] = stdev_post_param

                    key = "global_mean_diff_" + field_param_name
                    param_dict[key] = mean_post_param - mean_prior_param

                    key = "global_std_diff_" + field_param_name
                    param_dict[key] = stdev_post_param - stdev_prior_param

                    write_3D_field_param_to_file(
                        param_dict,
                        nx,
                        ny,
                        nz,
                    )
                    print("")

    # Update 2D field parameters:
    print("")
    print("Start Update of all 2D field parameters.")
    print("")
    print(" Calculate response matrix Y and observation vector")
    print(" Use the same Y, C_D, D and observations for all surfaces (2D fields)")
    (
        Y,
        observations,
        C_D,
        _,
        _,
        _,
        _,
        _,
        D,
    ) = define_response_and_observations(
        iens_active_index,
        obs_and_response_df_for_2D_fields,
        start_seed=seed,
        use_obs_err_correlations=USE_SEISMIC_OBS_ERR_CORRELATION,
        seis_obs_and_response_dict_for_2D_fields=SEISMIC_OBS_AND_RESPONSE_DICT_FOR_2D_FIELDS,
        alpha=alpha,
        ertbox_params=ertbox_params_for_seismic_error_simulation,
    )
    nobs = Y.shape[0]
    print(f" Surfaces are conditioned to {nobs} observations.")
    if DEBUG_PRINT:
        print(f" Y matrix shape for use when updating surfaces:  {Y.shape}")
        print(
            f" Observation shape or use when updating surfaces:  {observations.shape}"
        )

    for surface_name in field_params_2D_list:
        print(f"  Surface field: {surface_name}")
        if SAVE_UPDATE_TO_STORAGE:
            with open_storage(STORAGE_PATH, mode="w") as storage:
                experiment = storage.get_experiment_by_name(EXPERIMENT_NAME)
                param_config = experiment.parameter_configuration
                prior_ensemble = experiment.get_ensemble_by_name(ENSEMBLE_NAME)
                X_prior_group = prior_ensemble.load_parameters_numpy(
                    surface_name, realizations
                )

                # NOTE: Need to initialize ESMDA instance each time it is used.
                print(f" Initialize ESMDA for {surface_name}")
                esmda_smoother = ESMDA(
                    covariance=C_D, observations=observations, alpha=alpha, seed=seed
                )

                X_post_group, surface_coord_dict = update_2D_field_with_esmda(
                    esmda_smoother,
                    surface_name,
                    param_config,
                    X_prior_group,
                    Y,
                    D=D,
                )

                # Store the field parameter to storage
                post_ensemble = experiment.get_ensemble_by_name(
                    ENSEMBLE_NAME_UPDATE_GLOBAL
                )
                print(f"  Store {surface_name} to {ENSEMBLE_NAME_UPDATE_GLOBAL}")
                nparam = X_post_group.shape[0]
                print(f"  Number of parameters in {surface_name}: {nparam}")
                print("")
                post_ensemble.save_parameters_numpy(
                    parameters=X_post_group,
                    param_group=surface_name,
                    iens_active_index=iens_active_index,
                )
        else:
            with open_storage(STORAGE_PATH, mode="r") as storage:
                experiment = storage.get_experiment_by_name(EXPERIMENT_NAME)
                param_config = experiment.parameter_configuration
                prior_ensemble = experiment.get_ensemble_by_name(ENSEMBLE_NAME)
                X_prior_group = prior_ensemble.load_parameters_numpy(
                    surface_name, realizations
                )

                # NOTE: Need to initialize ESMDA instance each time it is used.
                print(f" Initialize ESMDA for {surface_name}")
                esmda_smoother = ESMDA(
                    covariance=C_D, observations=observations, alpha=alpha, seed=seed
                )

                X_post_group, surface_coord_dict = update_2D_field_with_esmda(
                    esmda_smoother,
                    surface_name,
                    param_config,
                    X_prior_group,
                    Y,
                    D=D,
                )

        if WRITE_STATS_PARAMS:
            print("  Calculate statistics and write 2D field params to file")
            prefix = "global"
            write_2D_field_statistics(
                surface_name,
                X_prior_group,
                X_post_group,
                surface_coord_dict,
                prefix,
                output_path=OUTPUT_PATH,
            )

    # Scalar parameter groups are updated
    if SAVE_UPDATE_TO_STORAGE:
        for group_name in scalar_param_list:
            print("")
            print(f"  Scalar parameter group: {group_name}")

            esmda_smoother = ESMDA(
                covariance=C_D,
                observations=observations,
                alpha=alpha,
                seed=seed,
            )

            with open_storage(STORAGE_PATH, mode="w") as storage:
                experiment = storage.get_experiment_by_name(EXPERIMENT_NAME)
                prior_ensemble = experiment.get_ensemble_by_name(ENSEMBLE_NAME)
                X_prior_group = prior_ensemble.load_parameters_numpy(
                    group_name, realizations
                )
                nparam, nreal = X_prior_group.shape
                # Parameters in X_prior_group are standard normal N(0,1)
                # and not transformed

                # Update with ESMDA
                X_post = esmda_smoother.assimilate(X=X_prior_group, Y=Y, D=D)

                print(f"    Store {group_name} to {ENSEMBLE_NAME_UPDATE_GLOBAL}")
                print(f"    Number of parameters in {group_name}: {nparam}")
                print("")
                # Get posterior ensemble and update it
                post_ensemble = experiment.get_ensemble_by_name(
                    ENSEMBLE_NAME_UPDATE_GLOBAL
                )
                post_ensemble.save_parameters_numpy(
                    parameters=X_post,
                    param_group=group_name,
                    iens_active_index=iens_active_index,
                )
        for group_name in non_updatable_list:
            print(f"  Scalar parameter group: {group_name}")
            with open_storage(STORAGE_PATH, mode="w") as storage:
                experiment = storage.get_experiment_by_name(EXPERIMENT_NAME)
                prior_ensemble = experiment.get_ensemble_by_name(ENSEMBLE_NAME)
                post_ensemble = experiment.get_ensemble_by_name(
                    ENSEMBLE_NAME_UPDATE_GLOBAL
                )
                prior_ensemble.experiment.parameter_configuration[
                    group_name
                ].copy_parameters(prior_ensemble, post_ensemble, iens_active_index)
                print(f"    Store {group_name} to {ENSEMBLE_NAME_UPDATE_GLOBAL}")
                print("")


def corr_func_gen_exponential(
    # Ranges must be positive,
    # azimuth must be angle in radians,
    # power must be a value between 1 and 2
    main_range: float,
    perp_range: float,
    azimuth: float,
    dx: float,
    dy: float,
    power: float = SEISMIC_GEN_EXP_POWER,
) -> float:
    if azimuth != 0.0:
        dx_rotated = dx * math.cos(azimuth) + dy * math.sin(azimuth)
        dy_rotated = -dx * math.sin(azimuth) + dy * math.cos(azimuth)
        d = math.sqrt((dx_rotated / main_range) ** 2 + (dy_rotated / perp_range) ** 2)
    else:
        d = math.sqrt((dx / main_range) ** 2 + (dy / perp_range) ** 2)
    return math.exp(-3.0 * math.pow(d, power))


def corr_func_exponential(
    # Ranges must be positive,
    # azimuth must be angle in radians,
    # power must be a value between 1 and 2
    main_range: float,
    perp_range: float,
    azimuth: float,
    dx: float,
    dy: float,
) -> float:
    if azimuth != 0.0:
        dx_rotated = dx * math.cos(azimuth) + dy * math.sin(azimuth)
        dy_rotated = -dx * math.sin(azimuth) + dy * math.cos(azimuth)
        d = math.sqrt((dx_rotated / main_range) ** 2 + (dy_rotated / perp_range) ** 2)
    else:
        d = math.sqrt((dx / main_range) ** 2 + (dy / perp_range) ** 2)
    return math.exp(-3.0 * d)


def corr_func_gaussian(
    # Ranges must be positive,
    # azimuth must be angle in radians,
    # power must be a value between 1 and 2
    main_range: float,
    perp_range: float,
    azimuth: float,
    dx: float,
    dy: float,
) -> float:
    if azimuth != 0.0:
        dx_rotated = dx * math.cos(azimuth) + dy * math.sin(azimuth)
        dy_rotated = -dx * math.sin(azimuth) + dy * math.cos(azimuth)
        d2 = (dx_rotated / main_range) ** 2 + (dy_rotated / perp_range) ** 2
    else:
        d2 = (dx / main_range) ** 2 + (dy / perp_range) ** 2
    return math.exp(-3.0 * d2)


def define_response_and_observations(
    iens_active_index: npt.NDArray[np.int32],
    observations_and_responses: pl.DataFrame,
    start_seed: int,
    zone_group_number: int = None,
    zone_list=None,
    use_obs_err_correlations: bool = False,
    seis_obs_and_response_dict_per_zone: dict = None,
    seis_obs_and_response_dict_for_2D_fields: list[dict] = None,
    alpha: float = 1.0,
    ertbox_params: ErtboxParameters = None,
) -> tuple[
    npt.NDArray[np.double],
    npt.NDArray[np.double],
    npt.NDArray[np.double],
    npt.NDArray[np.double],
    npt.NDArray[np.double],
    npt.NDArray[np.double],
    npt.NDArray[np.double],
    npt.NDArray[np.double],
    npt.NDArray[np.double],
]:
    """
    Calculate Y matrix, C_D matrix and return numpy vectors with:
    - observations
    - xpos for obs in global coordinates (UTM)
    - ypos for obs in global coordinates (UTM)
    - main_range   influence range (first half-axis in influence ellipse
    - perp_range   influence range (second half-axis in influence ellipse
    - anisotropy_angle  rotation of first half-axis of influence ellipse
      relative to global coordinate system
    """
    ensemble_size = len(iens_active_index)
    response_cols = []
    for i, active_number in enumerate(iens_active_index):
        response_cols.append(str(active_number))

    # Here responses with 0 std is not included?
    df_filtered = observations_and_responses.filter(
        pl.concat_list([pl.col(col) for col in response_cols])
        .list.eval(pl.element().std())
        .list.first()
        > 0
    )

    df_removed = observations_and_responses.filter(
        pl.concat_list([pl.col(col) for col in response_cols])
        .list.eval(pl.element().std())
        .list.first()
        <= 0
    )

    # Remove observations where either the observation value
    # or any of the responses for the observation
    # is None, NaN or Infinity.

    # Filter out rows where any of the specified columns contain None, NaN, or Inf
    df_removed_obs_with_undefined_responses = df_filtered.filter(
        pl.concat_list(
            [
                pl.col(col).is_null() | pl.col(col).is_nan() | pl.col(col).is_infinite()
                for col in response_cols
            ]
        ).list.any()
    )

    df_filtered = df_filtered.filter(
        ~pl.concat_list(
            [
                pl.col(col).is_null() | pl.col(col).is_nan() | pl.col(col).is_infinite()
                for col in response_cols
            ]
        ).list.any()
    )
    if df_removed.height > 0:
        print(
            " NOTE: Number of observations removed due to 0 std "
            f"of response: {df_removed.height}"
        )

    if df_removed_obs_with_undefined_responses.height > 0:
        print(
            " NOTE: Number of observations removed due to undefined response in "
            "at least one active realization: "
            f"{df_removed_obs_with_undefined_responses.height}"
        )

    if DEBUG_PRINT:
        print(" Observations removed due to 0 standard deviation of responses:")
        columns_to_print = ["observation_key", "response_key", "observations"]
        print(df_removed.select(columns_to_print))
        if zone_group_number:
            filename = "tmp_obs_for_zone_group_" + str(zone_group_number) + ".csv"
            print(f"Write file:  {filename}")
            df_filtered.write_csv(filename, separator=" ")

        print(
            " Observations removed due to None, Nan or Infinity in "
            "at least one realization of the response for this observation."
        )
        print(df_removed_obs_with_undefined_responses.select(columns_to_print))

    # Select only realizations of responses
    Y = df_filtered.select(response_cols).to_numpy()

    observations = df_filtered["observations"].to_numpy()
    std = df_filtered["std"].to_numpy()
    C_D = std**2

    assert ensemble_size == Y.shape[1]

    if DEBUG_PRINT:
        print(f"{C_D.shape}")

    xpos = df_filtered["east"].to_numpy()
    ypos = df_filtered["north"].to_numpy()
    main_ranges = df_filtered["main_range"].to_numpy()
    perp_ranges = df_filtered["perp_range"].to_numpy()
    anisotropy_angle = df_filtered["anisotropy_angle"].to_numpy()

    # Draw uncorrelated observation errors for all observations and initialize D matrix.
    # The standard deviation is increased by factor sqrt(alpha).
    nobs = observations.shape[0]
    rng = np.random.default_rng(start_seed)
    D = np.zeros((nobs, ensemble_size), dtype=np.float64)
    for i in range(ensemble_size):
        D[:, i] = observations + rng.standard_normal(nobs) * std * np.sqrt(alpha)

    if use_obs_err_correlations:
        # First fill the diagonal of the observation error covariance matrix C_D_matrix
        C_D_matrix = np.zeros((nobs, nobs), dtype=np.float64)
        np.fill_diagonal(C_D_matrix, C_D)

        # For each zone find the seismic obs
        # Calculate observation covariance matrix for seismic obs and update C_D_matrix
        # Simulate observation error for seismic obs and update D matrix

        # Add column with index of observations
        df_new = df_filtered.with_columns(
            pl.arange(0, df_filtered.height).alias("line_index")
        )
        if zone_group_number:
            # Now calculating the D matrix for use when updating 3D field parameters
            for zone_name in zone_list:
                zone_param_list = seis_obs_and_response_dict_per_zone[zone_name]
                for zone_param_dict in zone_param_list:
                    # For each seismic observation dataset for this zone
                    # extract the observations, observation errors and
                    # correlation function and modify the observation
                    # error covariance matrix
                    seis_name = zone_param_dict["name"]
                    obs_err_variogram = zone_param_dict["obs_err_variogram"]
                    obs_err_main_range = zone_param_dict["obs_err_main_range"]
                    obs_err_perp_range = zone_param_dict["obs_err_perp_range"]
                    obs_err_azimuth = (
                        zone_param_dict["obs_err_anisotropy_angle"] * math.pi / 180.0
                    )

                    df_new_seis_obs = df_new.filter(
                        df_new["observation_key"] == seis_name
                    )
                    number_of_seis_obs = df_new_seis_obs.height
                    if number_of_seis_obs > 0:
                        print(
                            f"  Zone: {zone_name}\n"
                            f"  Draw observation error for seismic observation set: {seis_name}.\n"  # noqa
                            f"  Spatial correlations with variogram type: {obs_err_variogram}\n"  # noqa
                            f"  Ranges: {obs_err_main_range} and {obs_err_perp_range}"
                        )
                        # Get observation error and position of seismic obs
                        # for this seismic observation data set
                        columns_to_extract = ["line_index"]
                        seis_obs_indices = np.zeros(number_of_seis_obs, dtype=np.int32)
                        df = df_new_seis_obs.select(columns_to_extract)
                        seis_obs_indices = df["line_index"].to_numpy()
                        columns_to_extract = ["xpos", "ypos"]
                        seis_obs_values = observations[seis_obs_indices]
                        seis_obs_std = std[seis_obs_indices]
                        seis_obs_xpos = xpos[seis_obs_indices]
                        seis_obs_ypos = ypos[seis_obs_indices]

                        # Calculate covariance matrix
                        if obs_err_variogram == "gen_exponential":
                            corr_function = corr_func_gen_exponential
                        elif obs_err_variogram == "exponential":
                            corr_function = corr_func_exponential
                        elif obs_err_variogram == "gaussian":
                            corr_function = corr_func_gaussian
                        else:
                            raise ValueError(
                                "Observation error covariance function "
                                f"{obs_err_variogram} is not implemented."
                            )

                        print("  Calculate observation error covariance matrix C_D")
                        for i in range(number_of_seis_obs):
                            indx1 = seis_obs_indices[i]
                            x1 = xpos[indx1]
                            y1 = ypos[indx1]
                            for j in range(i + 1, number_of_seis_obs):
                                indx2 = seis_obs_indices[j]
                                x2 = xpos[indx2]
                                y2 = ypos[indx2]
                                dx = x1 - x2
                                dy = y1 - y2
                                C_D_matrix[indx1, indx2] = (
                                    corr_function(
                                        obs_err_main_range,
                                        obs_err_perp_range,
                                        obs_err_azimuth,
                                        dx,
                                        dy,
                                    )
                                    * std[indx1]
                                    * std[indx2]
                                )
                                C_D_matrix[indx2, indx1] = C_D_matrix[indx1, indx2]

                        # Draw correlated observation error for the seismic observations
                        # for this seismic observation data set.
                        # Overwrite the uncorrelated observation errors for the seismic
                        # observations for this observation data set.
                        seis_obs_perturbed = perturb_seismic_2D_observations_with_correlated_errors(  # noqa
                            ensemble_size,
                            seis_obs_xpos,
                            seis_obs_ypos,
                            seis_obs_values,
                            seis_obs_std,
                            ertbox_params,
                            start_seed,
                            variogram_name=obs_err_variogram,
                            obs_err_main_corr_range=obs_err_main_range,
                            obs_err_perp_corr_range=obs_err_perp_range,
                            obs_err_anisotropy_angle=obs_err_azimuth,
                            obs_err_power_genexponential_variogram=SEISMIC_GEN_EXP_POWER,
                            alpha=alpha,
                        )
                        # Update the D matrix
                        assert seis_obs_perturbed.shape[0] == seis_obs_values.shape[0]
                        assert seis_obs_perturbed.shape[1] == ensemble_size
                        assert seis_obs_indices.shape[0] == seis_obs_perturbed.shape[0]
                        D[seis_obs_indices, :] = seis_obs_perturbed[:, :]

        else:
            # Now calculating the D matrix for use when updating 2D field parameters
            for surf_param_dict in seis_obs_and_response_dict_for_2D_fields:
                # For each seismic observation dataset for this 2D field
                # extract the observations, observation errors and correlation function
                # and modify the observation error covariance matrix
                seis_name = surf_param_dict["name"]
                obs_err_variogram = surf_param_dict["obs_err_variogram"]
                obs_err_main_range = surf_param_dict["obs_err_main_range"]
                obs_err_perp_range = surf_param_dict["obs_err_perp_range"]
                obs_err_azimuth = (
                    surf_param_dict["obs_err_anisotropy_angle"] * math.pi / 180.0
                )

                df_new_seis_obs = df_new.filter(df_new["observation_key"] == seis_name)
                number_of_seis_obs = df_new_seis_obs.height
                if number_of_seis_obs > 0:
                    print(
                        f"  Draw observation error for seismic observation set: {seis_name}.\n"  # noqa
                        f"  Spatial correlations with variogram type: {obs_err_variogram}\n"  # noqa
                        f"  Ranges: {obs_err_main_range} and {obs_err_perp_range}"
                    )
                    # Get observation error and position of seismic obs for this
                    # seismic observation data set
                    columns_to_extract = ["line_index"]
                    seis_obs_indices = np.zeros(number_of_seis_obs, dtype=np.int32)
                    df = df_new_seis_obs.select(columns_to_extract)
                    seis_obs_indices = df["line_index"].to_numpy()
                    seis_obs_values = observations[seis_obs_indices]
                    seis_obs_std = std[seis_obs_indices]
                    seis_obs_xpos = xpos[seis_obs_indices]
                    seis_obs_ypos = ypos[seis_obs_indices]

                    # Calculate covariance matrix
                    if obs_err_variogram == "gen_exponential":
                        corr_function = corr_func_gen_exponential
                    elif obs_err_variogram == "exponential":
                        corr_function = corr_func_exponential
                    elif obs_err_variogram == "gaussian":
                        corr_function = corr_func_gaussian
                    else:
                        raise ValueError(
                            "Observation error covariance function "
                            f"{obs_err_variogram} is not implemented."
                        )

                    print("  Calculate observation error covariance matrix C_D")
                    for i in range(number_of_seis_obs):
                        indx1 = seis_obs_indices[i]
                        x1 = xpos[indx1]
                        y1 = ypos[indx1]
                        for j in range(i + 1, number_of_seis_obs):
                            indx2 = seis_obs_indices[j]
                            x2 = xpos[indx2]
                            y2 = ypos[indx2]
                            dx = x1 - x2
                            dy = y1 - y2
                            C_D_matrix[indx1, indx2] = (
                                corr_function(
                                    obs_err_main_range,
                                    obs_err_perp_range,
                                    obs_err_azimuth,
                                    dx,
                                    dy,
                                )
                                * std[indx1]
                                * std[indx2]
                            )
                            C_D_matrix[indx2, indx1] = C_D_matrix[indx1, indx2]

                    # Draw correlated observation error for the seismic observations
                    # for this seismic observation data set.
                    # Overwrite the uncorrelated observation errors for the
                    # seismic observations for this observation data set.
                    seis_obs_perturbed = perturb_seismic_2D_observations_with_correlated_errors(  # noqa
                        ensemble_size,
                        seis_obs_xpos,
                        seis_obs_ypos,
                        seis_obs_values,
                        seis_obs_std,
                        ertbox_params,
                        start_seed,
                        variogram_name=obs_err_variogram,
                        obs_err_main_corr_range=obs_err_main_range,
                        obs_err_perp_corr_range=obs_err_perp_range,
                        obs_err_anisotropy_angle=obs_err_azimuth,
                        obs_err_power_genexponential_variogram=SEISMIC_GEN_EXP_POWER,
                        alpha=alpha,
                    )
                    # Update the D matrix
                    assert seis_obs_perturbed.shape[0] == seis_obs_values.shape[0]
                    assert seis_obs_perturbed.shape[1] == ensemble_size
                    assert seis_obs_indices.shape[0] == seis_obs_perturbed.shape[0]
                    D[seis_obs_indices, :] = seis_obs_perturbed[:, :]

        # Seismic obs are spatially correlated here and then use the full covariance
        # matrix instead of the diagonal covariance matrix.
        C_D = C_D_matrix
        if WRITE_D_MATRIX:
            if zone_group_number is not None:
                filename = "tmp_seis_obs_pointset" + str(zone_group_number) + ".csv"
                write_seis_obs_errors(
                    filename,
                    df_new_seis_obs,
                    observations,
                    seis_obs_indices,
                    D,
                )

    return (
        Y,
        observations,
        C_D,
        xpos,
        ypos,
        main_ranges,
        perp_ranges,
        anisotropy_angle,
        D,
    )


def write_seis_obs_errors(
    filename,
    df_seis_obs: pl.DataFrame,
    observations: npt.NDArray[np.double],
    seis_obs_indices: npt.NDArray[np.double],
    D: npt.NDArray[np.double],
):
    selected_columns = [
        "xpos",
        "ypos",
        "observations",
    ]
    df = df_seis_obs.select(selected_columns)
    # This D-matrix contains observation value + simulated error
    D_seis_err1 = D[seis_obs_indices, :]
    obs_seis_array = observations[seis_obs_indices]
    nreal = D_seis_err1.shape[1]
    # Subtract observation value from the values in D-matrix for the seismic obs
    # This D-matrix contains only the observation error
    D_seis_err2 = D_seis_err1 - np.tile(obs_seis_array, (nreal, 1)).T

    # Set column name equal to realization number
    column_names = [str(i) for i in range(D_seis_err2.shape[1])]
    df2 = pl.DataFrame(D_seis_err2, schema=column_names)
    df_obs_error = df.hstack(df2)
    print(f"Write file: {filename}")
    df_obs_error.write_csv(filename)


def write_3D_field_param_to_file(
    param_dict: dict[str, npt.NDArray[np.double]],
    nx: int,
    ny: int,
    nz: int,
    output_path: str = "TMP",
) -> None:
    """
    Write 3D parameter to ROFF format file to be readable by RMS
    """

    for param_name, values in param_dict.items():
        xtgeo_param = xtgeo.GridProperty(
            ncol=nx,
            nrow=ny,
            nlay=nz,
            name=param_name,
            values=values,
        )
        file_name = output_path + "/" + param_name + ".roff"
        print(f"     Write file: {file_name}")
        xtgeo_param.to_file(file_name, fformat="roff")


def write_2D_field_param_to_file(
    filename: str,
    surface_coord_dict: dict,
    values: npt.NDArray[np.double],
):
    print(f"  Write file: {filename}")
    surf = IrapSurface(
        header=IrapHeader(
            ncol=surface_coord_dict["nx"],
            nrow=surface_coord_dict["ny"],
            xori=surface_coord_dict["xorigo"],
            yori=surface_coord_dict["yorigo"],
            xinc=surface_coord_dict["xinc"],
            yinc=surface_coord_dict["yinc"],
            rot=surface_coord_dict["rotation"],
            xrot=surface_coord_dict["xrotationpoint"],
            yrot=surface_coord_dict["yrotationpoint"],
        ),
        values=values,
    )
    surf.to_ascii_file(filename)


def scaling_function(
    distances: npt.NDArray[np.double], name: str = "gaussian"
) -> npt.NDArray[np.double]:
    if name.lower() == "gaussian":
        scaling_factor = np.exp(-3.0 * distances**2)
    elif name.lower() == "exponential":
        scaling_factor = np.exp(-3.0 * distances)
    elif name.lower() == "gaspari-cohn":
        scaling_factor = distances
        d2 = distances**2
        d3 = d2 * distances
        d4 = d3 * distances
        d5 = d4 * distances
        s = -1 / 4 * d5 + 1 / 2 * d4 + 5 / 8 * d3 - 5 / 3 * d2 + 1
        scaling_factor[distances <= 1] = s[distances <= 1]
        s = (
            1 / 12 * d5
            - 1 / 2 * d4
            + 5 / 8 * d3
            + 5 / 3 * d2
            - 5 * distances
            + 4
            - 2 / 3 * 1 / distances
        )
        scaling_factor[(1 < distances) & (distances <= 2)] = s[
            (1 < distances) & (distances <= 2)
        ]
        scaling_factor[distances > 2] = 0.0

    return scaling_factor


def calculate_rho_for_obs_for_one_layer_of_3D_field_old(
    ertbox_params: ErtboxParameters,
    obs_xpos: npt.NDArray[np.double],
    obs_ypos: npt.NDArray[np.double],
    obs_main_range: npt.NDArray[np.double],
    obs_perp_range: npt.NDArray[np.double],
    obs_anisotropy_angle: npt.NDArray[np.double],
    scaling_function_name: str = "gaussian",
) -> npt.NDArray[np.double]:
    """
    Input:
    Specification of ERTBOX for the field
    (xorigo, yorigo,rotation,xinc,yinc,nx,ny,nz)
    Position (obs_xpos, obs_ypos), localization ranges
    (obs_main_range, obs_perp_range, obs_anisotropy_angle)
    for well observations only.
    The input position must be transformed into the local coordinate system
    defined by the ERTBOX, and the influence range and anisotropy_angle
    must also be relative to the local coordinate system defined by the ERTBOX.
    The position: xpos[n], ypos[n],
    localization ellipse defined by: main_range[n],perp_range[n], anisotropy_angle[n])
    refers to observation[n]
    Output: rho for one layer with nx * ny field parameter values for all observations
    having position.
    RHO[[n,m] =
    taperingfunc(distance((xpos_obs[n],ypos_obs[n]),(xpos_field[i,j,k],ypos_field[i,j,k]))
    where m = i + j* nx + k * nx * ny
    Note that:
    distance(   (xpos_obs[n],ypos_obs[n]), (xpos_field[i,j,k1], ypos_field[i,j,k1]) ) =
    distance(   (xpos_obs[n],ypos_obs[n]), (xpos_field[i,j,k2], ypos_field[i,j,k2]) )
    for all k1, k2 in [0,nz-1]  since distance calculation is only lateral.
    """
    # Center points of each grid cell in field parameter grid
    # in local coordinate system (ertbox grid)
    nx = ertbox_params.nx
    ny = ertbox_params.ny
    handedness = HANDEDNESS
    x_local = np.arange(nx, dtype=np.float64)
    xinc = ertbox_params.xinc
    yinc = ertbox_params.yinc
    x_local = (x_local + 0.5) * xinc
    if handedness == "right":
        # Order y from max to min y
        y_local = np.arange(ny - 1, -1, -1, dtype=np.float64)
    else:
        # Order y from min to max y
        y_local = np.arange(ny, dtype=np.float64)

    y_local = (y_local + 0.5) * yinc

    mesh_x_coord, mesh_y_coord = np.meshgrid(x_local, y_local, indexing="ij")
    nobs = len(obs_xpos)
    assert nobs == len(obs_ypos)
    assert nobs == len(obs_anisotropy_angle)
    assert nobs == len(obs_main_range)
    assert nobs == len(obs_perp_range)
    nparam_per_layer = nx * ny
    rho_one_layer = np.ones((nparam_per_layer, nobs), dtype=np.float64)
    for obs_number in range(len(obs_xpos)):
        xpos = obs_xpos[obs_number]
        ypos = obs_ypos[obs_number]
        anisotropy_angle = obs_anisotropy_angle[obs_number]
        main_range = obs_main_range[obs_number]
        perp_range = obs_perp_range[obs_number]
        dX = mesh_x_coord - xpos
        dY = mesh_y_coord - ypos
        rotation = anisotropy_angle * np.pi / 180.0
        cosangle = np.cos(rotation)
        sinangle = np.sin(rotation)
        dX_ellipse = (dX * cosangle + dY * sinangle) / main_range
        dY_ellipse = (-dX * sinangle + dY * cosangle) / perp_range
        distances_2d = np.sqrt(dX_ellipse * dX_ellipse + dY_ellipse * dY_ellipse)
        distances = distances_2d.flatten()
        rho_one_layer[:, obs_number] = scaling_function(
            distances, name=scaling_function_name
        )
    rho_2D = rho_one_layer.reshape((nx, ny, nobs))
    return rho_2D


def calculate_rho_for_summary_obs_for_one_layer_of_3D_field(
    ertbox_params: ErtboxParameters,
    obs_xpos: npt.NDArray[np.double],
    obs_ypos: npt.NDArray[np.double],
    obs_main_range: npt.NDArray[np.double],
    obs_perp_range: npt.NDArray[np.double],
    obs_anisotropy_angle: npt.NDArray[np.double],
    scaling_function_name: str = "gaussian",
) -> npt.NDArray[np.double]:
    """
    Input:
    Specification of ERTBOX for the field
    (xorigo, yorigo,rotation,xinc,yinc,nx,ny,nz)
    Position (obs_xpos, obs_ypos), localization ranges
    (obs_main_range, obs_perp_range, obs_anisotropy_angle)
    for well observations only.
    The input position must be transformed into the local coordinate system
    defined by the ERTBOX, and the influence range and anisotropy_angle must
    also be relative to the local coordinate system defined by the ERTBOX.
    The position: xpos[n], ypos[n] and
    localization ellipse defined by obs_main_range[n],obs_perp_range[n],
    obs_anisotropy_angle[n]) refers to observation[n]
    Output: rho for one layer with nx * ny field parameter values for
    all observations having position.
    RHO[[n,m] =
    taperingfunc(distance((xpos_obs[n],ypos_obs[n]),(xpos_field[i,j,k],ypos_field[i,j,k]))
    where m = k + j* nz + i * nz * ny for left-handed grid index origo and
    m = k + (ny - j - 1) * nz + i * nz * ny for right-handed grid index origo
    Note that:
    distance(   (xpos_obs[n],ypos_obs[n]), (xpos_field[i,j,k1], ypos_field[i,j,k1]) ) =
    distance(   (xpos_obs[n],ypos_obs[n]), (xpos_field[i,j,k2], ypos_field[i,j,k2]) )
    for all k1, k2 in [0,nz-1]  since distance calculation is only lateral.

    """
    # Center points of each grid cell in field parameter grid
    nx = ertbox_params.nx
    ny = ertbox_params.ny
    handedness = HANDEDNESS
    xinc = ertbox_params.xinc
    yinc = ertbox_params.yinc
    x_local = (np.arange(nx, dtype=np.float64) + 0.5) * xinc
    if handedness == "right":
        # y coordinate descreases from max to min
        y_local = (np.arange(ny - 1, -1, -1, dtype=np.float64) + 0.5) * yinc
    else:
        # y coordinate increases from min to max
        y_local = (np.arange(ny, dtype=np.float64) + 0.5) * yinc
    mesh_x_coord, mesh_y_coord = np.meshgrid(x_local, y_local, indexing="ij")

    # Number of observations
    nobs = len(obs_xpos)
    assert nobs == len(obs_ypos)
    assert nobs == len(obs_anisotropy_angle)
    assert nobs == len(obs_main_range)
    assert nobs == len(obs_perp_range)

    # Expand grid coordinates to match observations
    mesh_x_coord_flat = mesh_x_coord.flatten()[:, np.newaxis]  # (nx * ny, 1)
    mesh_y_coord_flat = mesh_y_coord.flatten()[:, np.newaxis]  # (nx * ny, 1)

    # Observation coordinates and parameters
    obs_xpos = obs_xpos[np.newaxis, :]  # (1, nobs)
    obs_ypos = obs_ypos[np.newaxis, :]  # (1, nobs)
    obs_main_range = obs_main_range[np.newaxis, :]  # (1, nobs)
    obs_perp_range = obs_perp_range[np.newaxis, :]  # (1, nobs)
    obs_anisotropy_angle = obs_anisotropy_angle[np.newaxis, :]  # (1, nobs)

    # Compute displacement between grid points and observations
    dX = mesh_x_coord_flat - obs_xpos  # (nx * ny, nobs)
    dY = mesh_y_coord_flat - obs_ypos  # (nx * ny, nobs)

    # Compute rotation parameters
    rotation = obs_anisotropy_angle * np.pi / 180.0  # (1, nobs)
    cos_angle = np.cos(rotation)  # (1, nobs)
    sin_angle = np.sin(rotation)  # (1, nobs)

    # Rotate and scale displacements to local coordinate system defined
    # by the two half axes of the influence ellipse. First coordinate (local x) is in
    # direction defined by anisotropy angle and local y is perpendicular to that.
    # Scale the distance by the ranges to get a normalized distance
    # (with value 1 at the edge of the ellipse)
    dX_ellipse = (dX * cos_angle + dY * sin_angle) / obs_main_range  # (nx * ny, nobs)
    dY_ellipse = (-dX * sin_angle + dY * cos_angle) / obs_perp_range  # (nx * ny, nobs)

    # Compute distances in the elliptical coordinate system
    distances = np.sqrt(dX_ellipse**2 + dY_ellipse**2)  # (nx * ny, nobs)

    # Apply the scaling function
    rho_one_layer = scaling_function(
        distances, name=scaling_function_name
    )  # (nx * ny, nobs)

    if DEBUG_PRINT:
        print(f"  {rho_one_layer.shape=}")
    rho_2D = rho_one_layer.reshape((nx, ny, nobs))
    return rho_2D


def calculate_rho_for_summary_obs_for_2D_field(
    surface_coord_dict: dict,
    obs_xpos: npt.NDArray[np.double],
    obs_ypos: npt.NDArray[np.double],
    obs_main_range: npt.NDArray[np.double],
    obs_perp_range: npt.NDArray[np.double],
    obs_anisotropy_angle: npt.NDArray[np.double],
    scaling_function_name: str = "gaussian",
) -> npt.NDArray[np.double]:
    """
    Calculates rho for all 2D field parameters in a SURFACE parameter.
    Assumption 1: The surface defines a local coordinate defined by
    the surface_dict containing (xorigo, yorigo, xinc, yinc, rotation, nx, ny)
    for the surface.
    Assumption 2: The localization ranges and orientation of the ellipse for
    influence is the same for 2D surface parameters as for  3D field parameters.
    Assumption 3: Input position of well observations is already transformed into
    the local coordinate system defined by the surface.

    Input: Definition of local surface coordinates, position and localization ranges
    for observations for well observations only.
    The input position must be transformed into the local coordinate system
    defined by the local surface coordinates ,and the influence range and
    obs_anisotropy_angle must also be relative to the local coordinate system
    defined by the surface.
    The position: xpos[n], ypos[n] and
    localization ellipse defined by main_range[n],perp_range[n], anisotropy_angle[n])
    refers to observation[n]
    Output: rho for surface parameter with nx * ny field parameter values for all
    observations having position.
    RHO[[n,m] =
    taperingfunc(distance((xpos_obs[n],ypos_obs[n]), (xpos_field[i,j], ypos_field[i,j]))
    where m = j + i* ny
    """
    # Center points of each grid cell in field parameter grid
    nx = surface_coord_dict["nx"]
    ny = surface_coord_dict["ny"]
    x_local = (np.arange(nx) + 0.5) * surface_coord_dict["xinc"]
    y_local = (np.arange(ny) + 0.5) * surface_coord_dict["yinc"]
    mesh_x_coord, mesh_y_coord = np.meshgrid(x_local, y_local, indexing="ij")

    # Number of observations
    nobs = len(obs_xpos)
    assert nobs == len(obs_ypos)
    assert nobs == len(obs_anisotropy_angle)
    assert nobs == len(obs_main_range)
    assert nobs == len(obs_perp_range)

    # Prepare rho array
    nparam_per_layer = nx * ny
    rho_one_layer = np.ones((nparam_per_layer, nobs), dtype=np.float64)

    # Expand grid coordinates to match observations
    mesh_x_coord_flat = mesh_x_coord.flatten()[:, np.newaxis]  # (nx * ny, 1)
    mesh_y_coord_flat = mesh_y_coord.flatten()[:, np.newaxis]  # (nx * ny, 1)

    # Observation coordinates and parameters
    obs_xpos = obs_xpos[np.newaxis, :]  # (1, nobs)
    obs_ypos = obs_ypos[np.newaxis, :]  # (1, nobs)
    obs_main_range = obs_main_range[np.newaxis, :]  # (1, nobs)
    obs_perp_range = obs_perp_range[np.newaxis, :]  # (1, nobs)
    obs_anisotropy_angle = obs_anisotropy_angle[np.newaxis, :]  # (1, nobs)

    # Compute displacement between grid points and observations
    dX = mesh_x_coord_flat - obs_xpos  # (nx * ny, nobs)
    dY = mesh_y_coord_flat - obs_ypos  # (nx * ny, nobs)

    # Compute rotation parameters
    rotation = obs_anisotropy_angle * np.pi / 180.0  # (1, nobs)
    cos_angle = np.cos(rotation)  # (1, nobs)
    sin_angle = np.sin(rotation)  # (1, nobs)

    # Rotate and scale displacements to anisotropic coordinate system
    dX_ellipse = (dX * cos_angle + dY * sin_angle) / obs_main_range  # (nx * ny, nobs)
    dY_ellipse = (-dX * sin_angle + dY * cos_angle) / obs_perp_range  # (nx * ny, nobs)

    # Compute distances in the elliptical coordinate system
    distances = np.sqrt(dX_ellipse**2 + dY_ellipse**2)  # (nx * ny, nobs)

    # Apply the scaling function
    rho_one_layer = scaling_function(
        distances, name=scaling_function_name
    )  # (nx * ny, nobs)
    return rho_one_layer


def get_3D_field_parameter_names_per_zone(
    groups, param_config_all, zone_per_field_param_group: dict
) -> dict[str, list[str]]:
    """
    Scan through all parameter groups. Get the groups of type 3D field parameters.
    Store the field parameter names per zone in a dict. All field parameter belonging
    to the same zone has the same ERTBOX grid and can use the same local coordinate
    system.
    Output: dict with zone as key and value is a list of field parameter names.
    """
    # Define a list of field parameters per zone.
    field_param_3d_list_per_zone_dict = {}
    for group_name in groups:
        if param_config_all[group_name].type == "field":
            zone_name = zone_per_field_param_group[group_name]
            if zone_name in field_param_3d_list_per_zone_dict:
                field_param_3d_list_per_zone_dict[zone_name].append(group_name)
            else:
                field_param_3d_list_per_zone_dict[zone_name] = [group_name]
    return field_param_3d_list_per_zone_dict


def get_2D_field_parameter_names(groups, param_config_all) -> list[str]:
    """
    Get a list of all 2D field parameters (from keyword SURFACE in ERT config file)
    """
    param_names = []
    for group_name in groups:
        if param_config_all[group_name].type == "surface":
            param_names.append(group_name)
    return param_names


def get_scalar_parameter_names(groups, param_config_all) -> list[str]:
    """
    Get a list of all scalar parameter group names
    from keyword GEN_KW in ERT config file.
    """
    param_names = []
    non_updatable = []
    for group_name in groups:
        group_type = param_config_all[group_name].type
        if group_type == "gen_kw":
            if param_config_all[group_name].update:
                param_names.append(group_name)
            else:
                non_updatable.append(group_name)
    return param_names, non_updatable


def get_ertbox_per_field_param_group(groups: list[str], param_config_all):
    ertbox_definition_per_field_dict = {}
    for group_name in groups:
        param_config = param_config_all[group_name]
        if param_config.type == "field":
            ertbox_definition_per_field_dict[group_name] = param_config.ertbox_params

    return ertbox_definition_per_field_dict


def update_3D_field_with_distance_esmda(
    distance_based_esmda_smoother,
    field_param_name: str,
    X_prior: npt.NDArray[np.double],
    Y: npt.NDArray[np.double],
    rho_2D: npt.NDArray[np.double],
    nlayer_per_batch: int,
    nx: int,
    ny: int,
    nz: int,
) -> list[npt.NDArray[np.double], npt.NDArray[np.double]]:
    """
    Calculate posterior update with distance-based ESMDA for one 3D parameter
    The RHO for one layer of the 3D field parameter is input.
    This is copied to all other layers of RHO in each batch of grid parameter
    layers since only lateral distance is used when calculating distances.
    Result is prior and posterior parameter matrices of field parameters for one field.
    """
    nparam = nx * ny * nz
    nparam_per_layer = nx * ny
    nobs = Y.shape[0]

    print(f"    Calculate update for {field_param_name} with {nparam} parameters")
    assert nparam == X_prior.shape[0]
    nreal = X_prior.shape[1]

    X_prior_3D = X_prior.reshape(nx, ny, nz, nreal)

    X_post_3D = np.zeros((nx, ny, nz, nreal), dtype=np.float64)
    if nlayer_per_batch > nz:
        nlayer_per_batch = nz
    nbatch = int(nz / nlayer_per_batch)

    nparam_in_batch = (
        nparam_per_layer * nlayer_per_batch
    )  # For full sized batch of layers

    nlayer_last_batch = nz - nbatch * nlayer_per_batch
    for batch_number in range(nbatch):
        start_layer_number = batch_number * nlayer_per_batch
        end_layer_number = start_layer_number + nlayer_per_batch

        print(
            f"     Batch number: {batch_number} "
            f"start: {start_layer_number} "
            f"end: {end_layer_number - 1}"
        )

        X_batch = X_prior_3D[:, :, start_layer_number:end_layer_number, :].reshape(
            (nparam_in_batch, nreal)
        )

        # Copy rho calculated from one layer of 3D parameter into all layers for
        # current batch of layers
        print(f"     Define rho for 3D field {field_param_name}")
        rho_3D_batch = np.zeros((nx, ny, nlayer_per_batch, nobs), dtype=np.float64)
        for layer in range(nlayer_per_batch):
            rho_3D_batch[:, :, layer, :] = rho_2D[:, :, :]  # (nx,ny,nobs)
        rho_batch = rho_3D_batch.reshape((nparam_in_batch, nobs))
        if DEBUG_PRINT:
            print(f"     {rho_batch.shape=}")

        assert Y.shape[1] == X_batch.shape[1]

        print(f"     Assimilate batch number {batch_number}")
        X_post_batch = distance_based_esmda_smoother.assimilate_batch(
            X_batch=X_batch, Y=Y, rho_batch=rho_batch
        )
        for layer in range(nlayer_per_batch):
            X_post_3D[:, :, start_layer_number:end_layer_number, :] = (
                X_post_batch.reshape((nx, ny, nlayer_per_batch, nreal))
            )

    if nlayer_last_batch > 0:
        batch_number = nbatch
        start_layer_number = batch_number * nlayer_per_batch
        end_layer_number = start_layer_number + nlayer_last_batch
        nparam_in_last_batch = nparam_per_layer * nlayer_last_batch
        print(
            f"     Batch number: {batch_number} "
            f"start: {start_layer_number} "
            f"end: {end_layer_number - 1}"
        )

        X_batch = X_prior_3D[:, :, start_layer_number:end_layer_number, :].reshape(
            (nparam_in_last_batch, nreal)
        )
        if DEBUG_PRINT:
            print(f"     {X_batch.shape=}")

        print(f"     Define rho for 3D field {field_param_name}")

        rho_3D_batch = np.zeros((nx, ny, nlayer_last_batch, nobs), dtype=np.float64)
        # Copy rho calculated from one layer of 3D parameter into all layers for
        # current batch of layers
        for layer in range(nlayer_last_batch):
            rho_3D_batch[:, :, layer, :] = rho_2D[:, :, :]  # (nx,ny,nobs)
        rho_batch = rho_3D_batch.reshape((nparam_in_last_batch, nobs))
        if DEBUG_PRINT:
            print(f"     {rho_batch.shape=}")
        assert Y.shape[1] == X_batch.shape[1]

        print(f"     Assimilate batch number {batch_number}")
        X_post_batch = distance_based_esmda_smoother.assimilate_batch(
            X_batch=X_batch, Y=Y, rho_batch=rho_batch
        )
        for layer in range(nlayer_last_batch):
            X_post_3D[:, :, start_layer_number:end_layer_number, :] = (
                X_post_batch.reshape((nx, ny, nlayer_last_batch, nreal))
            )

    return X_prior_3D, X_post_3D


def equal_surface_coord_system(surface_config1, surface_config2):
    assert surface_config1.type == "surface"
    assert surface_config1.type == "surface"
    if (
        (surface_config1.ncol == surface_config2.ncol)
        and (surface_config1.nrow == surface_config2.nrow)
        and (surface_config1.xori == surface_config2.xori)
        and (surface_config1.yori == surface_config2.yori)
        and (surface_config1.xinc == surface_config2.xinc)
        and (surface_config1.yinc == surface_config2.yinc)
        and (surface_config1.rotation == surface_config2.rotation)
        and (surface_config1.yflip == surface_config2.yflip)
    ):
        return True
    return False


def check_and_group_2D_fields(param_config, field_params_2D_list: list[str]) -> dict:
    if len(field_params_2D_list) == 0:
        return {}

    surface_name = field_params_2D_list[0]
    surface_config = param_config[surface_name]
    group_dict = {}
    group_number = 1
    group_dict[group_number] = [surface_name]

    for surface_name in field_params_2D_list:
        surface_config = param_config[surface_name]
        found = False
        for _, group_list in group_dict.items():
            if surface_name in group_list:
                found = True
                break
            else:
                # Check if surface has same surface_config
                s_config = param_config[group_list[0]]
                if equal_surface_coord_system(surface_config, s_config):
                    group_list.append(surface_name)
                    found = True
                    break
        if not found:
            group_number += 1
            group_dict[group_number] = [surface_name]
    return group_dict


def update_2D_field_with_esmda(
    esmda_smoother,
    field_param_name: str,
    param_config,
    X_prior: npt.NDArray[np.double],
    Y: npt.NDArray[np.double],
    D: npt.NDArray[np.double] = None,
) -> list[npt.NDArray[np.double], dict]:
    """
    Calculate posterior update with ESMDA for one 2D parameter
    Result is prior and posterior parameter matrices of field parameters for one field.
    """

    surface_coord_dict = {}
    surface_coord_dict["nx"] = param_config[field_param_name].ncol
    surface_coord_dict["ny"] = param_config[field_param_name].nrow
    yflip = param_config[field_param_name].yflip
    surface_coord_dict["yflip"] = yflip
    surface_coord_dict["xinc"] = param_config[field_param_name].xinc
    surface_coord_dict["yinc"] = param_config[field_param_name].yinc
    surface_coord_dict["xorigo"] = param_config[field_param_name].xori
    surface_coord_dict["yorigo"] = param_config[field_param_name].yori
    surface_coord_dict["rotation"] = param_config[field_param_name].rotation
    # TODO  Workaround due to missing info for param_config for surface
    # Assume origo and rotation point for the 2D map is the same.
    #    surface_coord_dict["xrotationpoint"] = param_config_all[field_param_name].xrot
    #    surface_coord_dict["yrotationpoint"] = param_config_all[field_param_name].yrot
    surface_coord_dict["xrotationpoint"] = surface_coord_dict["xorigo"]
    surface_coord_dict["yrotationpoint"] = surface_coord_dict["yorigo"]

    assert yflip == 1  # TODO Maybe this can be -1  and we need to set yinc to -yinc
    assert surface_coord_dict["yinc"] > 0
    assert surface_coord_dict["xinc"] > 0

    nx = surface_coord_dict["nx"]
    ny = surface_coord_dict["ny"]
    nparam = nx * ny
    assert nparam == X_prior.shape[0]

    print(f"  Assimilate 2D field parameter {field_param_name}")
    X_post = esmda_smoother.assimilate(X=X_prior, Y=Y, D=D)
    return X_post, surface_coord_dict


def update_3D_field_with_esmda(
    esmda_smoother,
    field_param_name: str,
    X_prior: npt.NDArray[np.double],
    Y: npt.NDArray[np.double],
    nx: int,
    ny: int,
    nz: int,
    D: npt.NDArray[np.double] = None,
) -> list[npt.NDArray[np.double], npt.NDArray[np.double]]:
    """
    Calculate posterior update with ESMDA for one 3D parameter
    Result is prior and posterior parameter matrices of field parameters for one field.
    """
    nparam = nx * ny * nz

    print(f"    Calculate update for {field_param_name} with {nparam} parameters")
    assert nparam == X_prior.shape[0]
    print("     Assimilate with ESMDA")
    X_post = esmda_smoother.assimilate(X=X_prior, Y=Y, D=D)

    return X_prior, X_post


def write_2D_field_statistics(
    field_param_name: str,
    X_prior: npt.NDArray[np.double],
    X_post: npt.NDArray[np.double],
    surface_coord_dict: dict,
    prefix: str,
    output_path: str = "TMP",
) -> None:
    # Optionally, calculate mean stdev and write to file
    nx = surface_coord_dict["nx"]
    ny = surface_coord_dict["ny"]
    mean_prior_param = np.mean(X_prior, axis=1).reshape((nx, ny))
    stdev_prior_param = np.std(X_prior, axis=1).reshape((nx, ny))
    mean_post_param = np.mean(X_post, axis=1).reshape((nx, ny))
    stdev_post_param = np.std(X_post, axis=1).reshape((nx, ny))
    diff_mean_param = mean_post_param - mean_prior_param
    diff_std_param = stdev_post_param - stdev_prior_param
    relative_change_param = diff_std_param / mean_prior_param

    filename = output_path + "/" + prefix + "_mean_prior_" + field_param_name + ".irap"
    write_2D_field_param_to_file(filename, surface_coord_dict, mean_prior_param)

    filename = output_path + "/" + prefix + "_std_prior_" + field_param_name + ".irap"
    write_2D_field_param_to_file(filename, surface_coord_dict, stdev_prior_param)

    filename = output_path + "/" + prefix + "_mean_post_" + field_param_name + ".irap"
    write_2D_field_param_to_file(filename, surface_coord_dict, mean_post_param)

    filename = output_path + "/" + prefix + "_std_post_" + field_param_name + ".irap"
    write_2D_field_param_to_file(filename, surface_coord_dict, stdev_post_param)

    filename = output_path + "/" + prefix + "_diff_mean" + field_param_name + ".irap"
    write_2D_field_param_to_file(filename, surface_coord_dict, diff_mean_param)

    filename = output_path + "/" + prefix + "_diff_std" + field_param_name + ".irap"
    write_2D_field_param_to_file(filename, surface_coord_dict, diff_std_param)

    filename = (
        output_path + "/" + prefix + "_relative_change" + field_param_name + ".irap"
    )
    write_2D_field_param_to_file(filename, surface_coord_dict, relative_change_param)

    print("")


def update_with_distance_esmda_new(
    iens_active_index: npt.NDArray[np.int32],
    realizations: list[int],
    obs_and_response_df_per_zone_dict: dict,
    obs_and_response_df_for_2D_fields,
    alpha: float,
    seed: int,
    field_params_3D_list_per_zone: dict[str, list[str]],
    field_params_2D_list: list[str],
    scalar_param_list: list[str],
    non_updatable_list: list[str],
    ertbox_per_field_param_dict: dict[ErtboxParameters],
    nlayer_per_batch: int,
    scaling_function_name: str,
    truncation: float,
) -> None:
    # Define a dict with group of zones having exactly the same observations
    # and localization settings
    ensemble_size = iens_active_index.shape[0]

    group_of_zones_with_same_obs_dict = define_group_of_zones_with_same_observations(
        obs_and_response_df_per_zone_dict
    )
    if DEBUG_PRINT:
        print(
            "  Group of zones using the same observations and localization parameters:"
        )
        print(f" {json.dumps(group_of_zones_with_same_obs_dict, indent=4)}")

    print("")
    print(" Start update of all 3D field parameters for all zones.")

    for zone_group_number, zone_list in group_of_zones_with_same_obs_dict.items():
        print("")
        print(f" Zone group {zone_group_number} with:")
        for zone_name in zone_list:
            print(f"   {zone_name}")

        first_zone_name_in_group = zone_list[0]
        df_obs_this_zone = obs_and_response_df_per_zone_dict[first_zone_name_in_group]

        # Use the ertbox definition of one of the fields for simulation of
        # seismic observation error correlations.
        field_param_name = field_params_3D_list_per_zone[first_zone_name_in_group][0]
        ertbox_params_for_seismic_error_simulation = ertbox_per_field_param_dict[
            field_param_name
        ]

        # Calculate Y matrix etc
        print(
            " Define response matrix Y, obs vectors and "
            f"localization parameter vectors for zone group {zone_group_number}."
        )
        (
            Y,
            observations,
            C_D,
            obs_xpos,
            obs_ypos,
            obs_local_main_ranges,
            obs_local_perp_ranges,
            obs_local_anisotropy_angle,
            D,
        ) = define_response_and_observations(
            iens_active_index,
            df_obs_this_zone,
            start_seed=seed,
            zone_group_number=zone_group_number,
            zone_list=zone_list,
            use_obs_err_correlations=USE_SEISMIC_OBS_ERR_CORRELATION,
            seis_obs_and_response_dict_per_zone=SEISMIC_OBS_AND_RESPONSE_DICT_PER_ZONE,
            alpha=alpha,
            ertbox_params=ertbox_params_for_seismic_error_simulation,
        )
        nobs = Y.shape[0]
        print(
            " Number of observations used for zone group "
            f"{zone_group_number} is {nobs}."
        )

        if DEBUG_PRINT:
            print(f"  Y.shape:  {Y.shape}")
            print(f"  observations.shape:  {observations.shape}")
            print(f"  covariance_obs_error.shape:  {C_D.shape}")

        # All zones in this list use the same set of observations and localization
        # parameters and can use same instance of DistanceESMDA
        print(
            " Initialize DistanceESMDA for 3D fields for zone group number "
            f"{zone_group_number}."
        )
        distance_based_esmda_smoother = DistanceESMDA(
            covariance=C_D, observations=observations, alpha=alpha, seed=seed
        )

        distance_based_esmda_smoother.prepare_assimilation(
            Y, truncation=truncation, D=D
        )
        previous_ertbox_params = None
        for zone_name in zone_list:
            # For current zone all field parameters should have same size
            # and belong to same ERTBOX coordinate system, but to be sure
            # we check the ertbox.
            print(f"  Zone name: {zone_name}")
            field_param_list_current_zone = field_params_3D_list_per_zone[zone_name]
            print("   Field params for this zone:")
            for fname in field_param_list_current_zone:
                print(f"    {fname}")
            ertbox_params = ertbox_per_field_param_dict[
                field_param_list_current_zone[0]
            ]
            nx = ertbox_params.nx
            ny = ertbox_params.ny
            nz = ertbox_params.nz

            print(f"   ERTBOX grid size: nx={nx}, ny={ny}, nz={nz}")
            # If multiple zones following each other use the same xy layout
            # of the ertbox, which means that all ertbox parameters are
            # equal except for nz, there is no need to transform observations
            # again or re-calculate RHO for one layer
            if previous_ertbox_params is None or (
                not ertbox_has_same_xy_layout(ertbox_params, previous_ertbox_params)
            ):
                previous_ertbox_params = copy.copy(ertbox_params)
                # Some of the parameters defining the 2D grid layout of
                # the ERTBOX differs from previous zone.
                # Need to do the coordinate transformation of
                # the observations again.
                nx = ertbox_params.nx
                ny = ertbox_params.ny
                nz = ertbox_params.nz
                if DEBUG_PRINT:
                    print(f"   Dimension of 3D parameter: ({nx},{ny},{nz})")

                print(
                    "   Transform observation position to local ertbox coordinates "
                    "for this zone"
                )
                (
                    obs_xpos_transformed,
                    obs_ypos_transformed,
                    obs_anisotropy_angle_transformed,
                ) = transform_to_local_coordinates_3D(
                    ertbox_params, obs_xpos, obs_ypos, obs_local_anisotropy_angle
                )
                # check that transformed points are within the ertbox assuming
                # no obs is outside ertbox area
                xmax = nx * ertbox_params.xlength
                ymax = ny * ertbox_params.ylength
                x_coord_check = np.all(
                    (obs_xpos_transformed >= 0) & (obs_xpos_transformed <= xmax)
                )
                y_coord_check = np.all(
                    (obs_ypos_transformed >= 0) & (obs_ypos_transformed <= ymax)
                )
                if not x_coord_check:
                    raise ValueError(
                        "Some observation points are outside of ertbox in "
                        "local x coordinates"
                    )
                if not y_coord_check:
                    raise ValueError(
                        "Some observation points are outside of ertbox in "
                        "local y coordinates."
                    )

                if USE_LOCALIZATION:
                    print("   Calculate RHO for one layer of field parameter")
                    # Calculate rho for one layer of field parameters.
                    # Localization only calculate lateral distance so
                    # for a 3D parameter
                    # rho(obs_number, field_param(i,j,k1))=
                    # rho(obs, field_param(i,j,k2)
                    # for k1, k2 in interval [0, nz-1]
                    # where nz is number of layers for the ERTBOX grid for
                    # the field parameter

                    rho_2D = calculate_rho_for_summary_obs_for_one_layer_of_3D_field(
                        ertbox_params,
                        obs_xpos_transformed,
                        obs_ypos_transformed,
                        obs_local_main_ranges,
                        obs_local_perp_ranges,
                        obs_anisotropy_angle_transformed,
                        scaling_function_name=scaling_function_name,
                    )

                    if DEBUG_PRINT:
                        rho_2D_old = (
                            calculate_rho_for_obs_for_one_layer_of_3D_field_old(
                                ertbox_params,
                                obs_xpos_transformed,
                                obs_ypos_transformed,
                                obs_local_main_ranges,
                                obs_local_perp_ranges,
                                obs_anisotropy_angle_transformed,
                                scaling_function_name=scaling_function_name,
                            )
                        )
                        # Check that calculated rho is equal for both functions
                        difference_rho = rho_2D_old - rho_2D
                        max_diff = np.max(np.abs(difference_rho))
                        print(
                            "   Compare two different calculations of "
                            "RHO for one layer:"
                        )
                        print(f"   max difference = {max_diff}")
                        if max_diff > 0.001:
                            raise ValueError(
                                "Error: RHO calculated by two different functions "
                                "are not equal."
                            )

                        # Will only work if ERTBOX is defined in file
                        # 'ERTBOX.roff' locally
                        # nstacked_2d_layers = 264
                        # write_rho(rho_2D, nx, ny, nstacked_2d_layers, nobs)
                else:
                    print("   Use RHO equal to 1. No localization used.")
                    rho_2D = np.ones((nx, ny, nobs), dtype=np.float64)
            else:
                # No need to re-calculate observation coordinates since
                # same xy layout of ERTBOX for this zone as for previous zone,
                # and no need to re-calculate rho for one layer
                pass

            for field_param_name in field_param_list_current_zone:
                print(f"    Field name: {field_param_name} for zone {zone_name}")
                if SAVE_UPDATE_TO_STORAGE:
                    with open_storage(STORAGE_PATH, mode="w") as storage:
                        experiment = storage.get_experiment_by_name(EXPERIMENT_NAME)
                        prior_ensemble = experiment.get_ensemble_by_name(ENSEMBLE_NAME)
                        X_prior = prior_ensemble.load_parameters_numpy(
                            field_param_name, realizations
                        )

                        X_prior_3D, X_post_3D = update_3D_field_with_distance_esmda(
                            distance_based_esmda_smoother,
                            field_param_name,
                            X_prior,
                            Y,
                            rho_2D,
                            nlayer_per_batch,
                            nx,
                            ny,
                            nz,
                        )
                        # Store the field parameter to storage
                        post_ensemble = experiment.get_ensemble_by_name(
                            ENSEMBLE_NAME_UPDATE
                        )
                        print(f"    Store {field_param_name} to {ENSEMBLE_NAME_UPDATE}")
                        # print(f" {ensemble_size=}")
                        X_post = X_post_3D.reshape((nx * ny * nz, ensemble_size))
                        assert X_post.shape == X_prior.shape
                        nparam = X_post.shape[0]
                        print(
                            f"    Number of parameters in {field_param_name}: {nparam}"
                        )
                        print("")
                        # print(iens_active_index)
                        post_ensemble.save_parameters_numpy(
                            parameters=X_post,
                            param_group=field_param_name,
                            iens_active_index=iens_active_index,
                        )

                else:
                    with open_storage(STORAGE_PATH, mode="r") as storage:
                        experiment = storage.get_experiment_by_name(EXPERIMENT_NAME)
                        prior_ensemble = experiment.get_ensemble_by_name(ENSEMBLE_NAME)
                        X_prior = prior_ensemble.load_parameters_numpy(
                            field_param_name, realizations
                        )

                        X_prior_3D, X_post_3D = update_3D_field_with_distance_esmda(
                            distance_based_esmda_smoother,
                            field_param_name,
                            X_prior,
                            Y,
                            rho_2D,
                            nlayer_per_batch,
                            nx,
                            ny,
                            nz,
                        )

                # Write updted 3D field parameter to file
                if WRITE_STATS_PARAMS:
                    # Optionally, calculate mean stdev and write to file
                    # X_prior_3D (nx,ny,nz,nreal)
                    # X_post_3D (nx,ny,nz,nreal)
                    mean_prior_param = np.mean(X_prior_3D, axis=3)
                    stdev_prior_param = np.std(X_prior_3D, axis=3)
                    mean_post_param = np.mean(X_post_3D, axis=3)
                    stdev_post_param = np.std(X_post_3D, axis=3)
                    param_dict = {}
                    key = "local_mean_prior_" + field_param_name
                    param_dict[key] = mean_prior_param

                    key = "local_std_prior_" + field_param_name
                    param_dict[key] = stdev_prior_param

                    key = "local_mean_post_" + field_param_name
                    param_dict[key] = mean_post_param

                    key = "local_std_post_" + field_param_name
                    param_dict[key] = stdev_post_param

                    key = "local_mean_diff_" + field_param_name
                    param_dict[key] = mean_post_param - mean_prior_param

                    key = "local_std_diff_" + field_param_name
                    param_dict[key] = stdev_post_param - stdev_prior_param

                    write_3D_field_param_to_file(
                        param_dict,
                        nx,
                        ny,
                        nz,
                    )
                print("")

    # Update 2D field parameters:
    # Check all 2D field parameters and group them such that all 2D fields with the same
    # local coordinate system is grouped together to avoid having to transform
    # global position of observations to local coordinates more than necessary
    # and to avoid having to re-calculate RHO unnecessary.

    # Calculate Y, position, observation, localizatio parameters
    # and perturbed observation matrix D to be used for all surfaces
    # and for all scalar parameter updates
    (
        Y,
        observations,
        C_D,
        obs_xpos,
        obs_ypos,
        obs_main_ranges,
        obs_perp_ranges,
        obs_anisotropy_angle,
        D,
    ) = define_response_and_observations(
        iens_active_index,
        obs_and_response_df_for_2D_fields,
        start_seed=seed,
        use_obs_err_correlations=USE_SEISMIC_OBS_ERR_CORRELATION,
        seis_obs_and_response_dict_for_2D_fields=SEISMIC_OBS_AND_RESPONSE_DICT_FOR_2D_FIELDS,
        alpha=alpha,
        ertbox_params=ertbox_params_for_seismic_error_simulation,
    )

    if len(field_params_2D_list) > 0:
        print("")
        print("Start Update of all 2D field parameters.")
        print("")
        print(
            " Calculate response matrix Y, observation vector "
            "and localization attribute vectors"
        )
        print(
            " All 2D fields will use the same set of observations, Y, C_D, D and "
            "localization ranges."
        )
        nobs = Y.shape[0]
        print(f" Surfaces are conditioned to {nobs} observations.")
        if DEBUG_PRINT:
            print(f" Y matrix shape for use when updating surfaces:  {Y.shape}")
            print(
                " Observations shape for use when updating surfaces:  "
                f"{observations.shape}"
            )

        # Initialize smoother again for this set of observations and observation error
        distance_based_esmda_smoother = DistanceESMDA(
            covariance=C_D, observations=observations, alpha=alpha, seed=seed
        )
        distance_based_esmda_smoother.prepare_assimilation(
            Y, truncation=truncation, D=D
        )
        if SAVE_UPDATE_TO_STORAGE:
            with open_storage(STORAGE_PATH, mode="w") as storage:
                experiment = storage.get_experiment_by_name(EXPERIMENT_NAME)
                param_config_all = experiment.parameter_configuration
                prior_ensemble = experiment.get_ensemble_by_name(ENSEMBLE_NAME)

                groups_of_2D_field_names = check_and_group_2D_fields(
                    param_config_all, field_params_2D_list
                )
                print("Groups of 2D fields:")
                for group_number, group_list in groups_of_2D_field_names.items():
                    print(f"  Group: {group_number}")
                    for s in group_list:
                        print(f"  {s}")
                    print("\n")
                for group_number, surface_list in groups_of_2D_field_names.items():
                    # Calculate rho once per group since same obs and
                    # same local coordinate system for all surface in a group.
                    print(f" Surface group number: {group_number}")
                    surface_name = surface_list[0]
                    rho_2D, surface_coord_dict = (
                        transform_coord_and_define_rho_for_2D_fields(
                            surface_name,
                            param_config_all,
                            obs_xpos,
                            obs_ypos,
                            obs_main_ranges,
                            obs_perp_ranges,
                            obs_anisotropy_angle,
                        )
                    )

                    # Update 2D fields within the group using same Y and RHO matrix
                    for surface_name in surface_list:
                        print(f"  Surface field: {surface_name}")
                        X_prior = prior_ensemble.load_parameters_numpy(
                            surface_name, realizations
                        )
                        print(f"  Assimilate 2D field parameter {surface_name}")
                        X_post = distance_based_esmda_smoother.assimilate_batch(
                            X_batch=X_prior, Y=Y, rho_batch=rho_2D
                        )

                        # Store the field parameter to storage
                        post_ensemble = experiment.get_ensemble_by_name(
                            ENSEMBLE_NAME_UPDATE
                        )
                        print(f"  Store {surface_name} to {ENSEMBLE_NAME_UPDATE}")
                        nparam = X_post.shape[0]
                        print(f"  Number of parameters in {surface_name}: {nparam}")
                        print("")
                        post_ensemble.save_parameters_numpy(
                            parameters=X_post,
                            param_group=surface_name,
                            iens_active_index=iens_active_index,
                        )
                        if WRITE_STATS_PARAMS:
                            print(
                                "  Calculate statistics and write 2D field "
                                "params to file"
                            )
                            prefix = "local"
                            write_2D_field_statistics(
                                surface_name,
                                X_prior,
                                X_post,
                                surface_coord_dict,
                                prefix,
                                output_path=OUTPUT_PATH,
                            )

        else:
            with open_storage(STORAGE_PATH, mode="r") as storage:
                experiment = storage.get_experiment_by_name(EXPERIMENT_NAME)
                param_config_all = experiment.parameter_configuration
                prior_ensemble = experiment.get_ensemble_by_name(ENSEMBLE_NAME)

                groups_of_2D_field_names = check_and_group_2D_fields(
                    param_config_all, field_params_2D_list
                )
                print(f"{groups_of_2D_field_names=}")

                for group_number, surface_list in groups_of_2D_field_names.items():
                    # Calculate rho once per group since same obs and same
                    # local coordinate system for all surface in a group.
                    print(f"  Surface group number: {group_number}")
                    surface_name = surface_list[0]
                    rho_2D, surface_coord_dict = (
                        transform_coord_and_define_rho_for_2D_fields(
                            surface_name,
                            param_config_all,
                            obs_xpos,
                            obs_ypos,
                            obs_main_ranges,
                            obs_perp_ranges,
                            obs_anisotropy_angle,
                        )
                    )

                    # Update 2D fields within the group using same Y and RHO matrix
                    for surface_name in surface_list:
                        print(f"  Surface field: {surface_name}")
                        X_prior = prior_ensemble.load_parameters_numpy(
                            surface_name, realizations
                        )
                        print(f"  Assimilate 2D field parameter {field_param_name}")
                        X_post = distance_based_esmda_smoother.assimilate_batch(
                            X_batch=X_prior, Y=Y, rho_batch=rho_2D
                        )

                        if WRITE_STATS_PARAMS:
                            print(
                                "  Calculate statistics and write 2D "
                                "field params to file"
                            )
                            prefix = "local"
                            write_2D_field_statistics(
                                surface_name,
                                X_prior,
                                X_post,
                                surface_coord_dict,
                                prefix,
                                output_path=OUTPUT_PATH,
                            )

    # Update scalarparameter groups using ordinary global ESMDA
    # TODO: Note this implementation is slow since it initialize ESMDA for every
    # scalar parameter. Better to update all scalar parameters in one update
    # operation with ESMDA instead.
    # NOTE: it will use same observations, response matrix Y
    # and perturbed observation matrix D and same covariance matrix
    # C_D as for 2D fields.
    if SAVE_UPDATE_TO_STORAGE:
        method = SCALAR_UPDATE_METHOD
        update_scalars_all(
            method,
            scalar_param_list,
            iens_active_index,
            observations,
            C_D,
            Y,
            alpha,
            SEED,
            D,
        )
        update_storage_with_const_params(non_updatable_list, iens_active_index)


def transform_coord_and_define_rho_for_2D_fields(
    field_param_name: str,
    param_config_all,
    obs_xpos: npt.NDArray[np.double],
    obs_ypos: npt.NDArray[np.double],
    obs_main_ranges: npt.NDArray[np.double],
    obs_perp_ranges: npt.NDArray[np.double],
    obs_anisotropy_angle: npt.NDArray[np.double],
) -> tuple[npt.NDArray[np.double], dict]:
    print(f"  Surface field: {field_param_name}")
    surface_coord_dict = {}
    surface_coord_dict["nx"] = param_config_all[field_param_name].ncol
    surface_coord_dict["ny"] = param_config_all[field_param_name].nrow
    yflip = param_config_all[field_param_name].yflip
    surface_coord_dict["yflip"] = yflip
    surface_coord_dict["xinc"] = param_config_all[field_param_name].xinc
    surface_coord_dict["yinc"] = param_config_all[field_param_name].yinc
    surface_coord_dict["xorigo"] = param_config_all[field_param_name].xori
    surface_coord_dict["yorigo"] = param_config_all[field_param_name].yori
    surface_coord_dict["rotation"] = param_config_all[field_param_name].rotation
    # TODO  Workaround due to missing info for param_config for surface
    # Assume origo and rotation point for the 2D map is the same.
    #    surface_coord_dict["xrotationpoint"] = param_config_all[field_param_name].xrot
    #    surface_coord_dict["yrotationpoint"] = param_config_all[field_param_name].yrot
    surface_coord_dict["xrotationpoint"] = surface_coord_dict["xorigo"]
    surface_coord_dict["yrotationpoint"] = surface_coord_dict["yorigo"]

    assert yflip == 1  # TODO Maybe this can be -1  and we need to set yinc to -yinc
    assert surface_coord_dict["yinc"] > 0
    assert surface_coord_dict["xinc"] > 0

    nx = surface_coord_dict["nx"]
    ny = surface_coord_dict["ny"]
    nparam = nx * ny

    print(
        "  Transform observation position and localization anisotropy angle to "
        "local surface coordinate."
    )
    obs_xpos_transf, obs_ypos_transf, obs_anisotropy_angle_transf = (
        transform_to_local_coordinates_2D(
            surface_coord_dict, obs_xpos, obs_ypos, obs_anisotropy_angle
        )
    )

    print(f"  Define rho for 2D field {field_param_name}")
    if USE_LOCALIZATION:
        rho = calculate_rho_for_summary_obs_for_2D_field(
            surface_coord_dict,
            obs_xpos_transf,
            obs_ypos_transf,
            obs_main_ranges,
            obs_perp_ranges,
            obs_anisotropy_angle_transf,
            scaling_function_name=SCALING_FUNCTION_NAME,
        )

    else:
        nobs = len(obs_xpos)
        rho = np.ones(shape=(nparam, nobs), dtype=np.float64)

    if DEBUG_PRINT:
        for obs_nr in range(10):
            filename = (
                "rho_for_2D_fields_" + field_param_name + "_" + str(obs_nr) + ".irap"
            )
            rho_out = rho[:, obs_nr]
            rho_out_2D = rho_out.reshape((nx, ny))
            write_2D_field_param_to_file(filename, surface_coord_dict, rho_out_2D)

    return rho, surface_coord_dict


def update_scalars(
    method: str,
    scalar_param_list: list[str],
    iens_active_index: npt.NDArray[np.int32],
    observations: npt.NDArray[np.double],
    C_D: npt.NDArray[np.double],
    Y: npt.NDArray[np.double],
    alpha: float,
    seed: int,
    D: npt.NDArray[np.double] = None,
):
    if method == "ESMDA":
        for group_name in scalar_param_list:
            print(f"  Scalar parameter group: {group_name}")

            esmda_smoother = ESMDA(
                covariance=C_D,
                observations=observations,
                alpha=alpha,
                seed=seed,
            )
            realizations = list(iens_active_index)
            with open_storage(STORAGE_PATH, mode="w") as storage:
                experiment = storage.get_experiment_by_name(EXPERIMENT_NAME)
                prior_ensemble = experiment.get_ensemble_by_name(ENSEMBLE_NAME)
                X_prior_group = prior_ensemble.load_parameters_numpy(
                    group_name, realizations
                )
                nparam = X_prior_group.shape[0]
                # Parameters in X_prior_group are standard normal N(0,1)
                # and not transformed and should not be transformed
                # before update and not before saving to storage
                #

                # Update with ESMDA
                X_post = esmda_smoother.assimilate(X=X_prior_group, Y=Y, D=D)

                print(f"    Store {group_name} to {ENSEMBLE_NAME_UPDATE}")
                print(f"    Number of parameters in {group_name}: {nparam}")
                print("")
                post_ensemble = experiment.get_ensemble_by_name(ENSEMBLE_NAME_UPDATE)
                post_ensemble.save_parameters_numpy(
                    parameters=X_post,
                    param_group=group_name,
                    iens_active_index=iens_active_index,
                )


def update_scalars_all(
    method: str,
    scalar_param_list: list[str],
    iens_active_index: npt.NDArray[np.int32],
    observations: npt.NDArray[np.double],
    C_D: npt.NDArray[np.double],
    Y: npt.NDArray[np.double],
    alpha: float,
    seed: int,
    D: npt.NDArray[np.double] = None,
):
    if method == "ESMDA":
        print("Update scalar parameters with ordinary ESMDA")
        nparam = len(scalar_param_list)
        nreal = Y.shape[1]
        realizations = list(iens_active_index)
        with open_storage(STORAGE_PATH, mode="r") as storage:
            experiment = storage.get_experiment_by_name(EXPERIMENT_NAME)
            prior_ensemble = experiment.get_ensemble_by_name(ENSEMBLE_NAME)

            print(f"  Number of scalar parameters to update: {nparam}")
            X_prior = np.zeros((nparam, nreal), dtype=np.float64)
            for n, group_name in enumerate(scalar_param_list):
                X_prior_group = prior_ensemble.load_parameters_numpy(
                    group_name, realizations
                )
                X_prior[n, :] = X_prior_group[0, :]

        esmda_smoother = ESMDA(
            covariance=C_D,
            observations=observations,
            alpha=alpha,
            seed=seed,
        )
        with open_storage(STORAGE_PATH, mode="w") as storage:
            experiment = storage.get_experiment_by_name(EXPERIMENT_NAME)
            post_ensemble = experiment.get_ensemble_by_name(ENSEMBLE_NAME_UPDATE)

            # Parameters in X_prior_group are standard normal N(0,1)
            # and not transformed and should not be transformed
            # before update and not before saving to storage

            # Update with ESMDA
            X_post = esmda_smoother.assimilate(X=X_prior, Y=Y, D=D)

            print(f"  Store scalar parameters to {ENSEMBLE_NAME_UPDATE}")
            for n, group_name in enumerate(scalar_param_list):
                X_post_group = X_post[n, :]
                post_ensemble.save_parameters_numpy(
                    parameters=X_post_group,
                    param_group=group_name,
                    iens_active_index=iens_active_index,
                )
    elif method == "ADAPTIVE":
        print("Update scalar parameters with Adaptive localization with ESMDA")
        nparam = len(scalar_param_list)
        nreal = Y.shape[1]
        realizations = list(iens_active_index)
        with open_storage(STORAGE_PATH, mode="r") as storage:
            experiment = storage.get_experiment_by_name(EXPERIMENT_NAME)
            prior_ensemble = experiment.get_ensemble_by_name(ENSEMBLE_NAME)

            print(f"  Number of scalar parameters to update: {nparam}")
            X_prior = np.zeros((nparam, nreal), dtype=np.float64)
            for n, group_name in enumerate(scalar_param_list):
                X_prior_group = prior_ensemble.load_parameters_numpy(
                    group_name, realizations
                )
                X_prior[n, :] = X_prior_group[0, :]

        adaptive_esmda_smoother = AdaptiveESMDA(
            covariance=C_D,
            observations=observations,
            seed=seed,
        )
        corr_threshold = adaptive_esmda_smoother.correlation_threshold(nreal)
        # Calculate D
        if D is None:
            D = adaptive_esmda_smoother.perturb_observations(
                ensemble_size=nreal, alpha=alpha
            )
        assert D.shape[0] == observations.shape[0]
        assert D.shape[1] == Y.shape[1]
        X_post = adaptive_esmda_smoother.assimilate(
            X=X_prior,
            Y=Y,
            D=D,
            overwrite=False,
            alpha=alpha,
            correlation_threshold=corr_threshold,
            n_jobs=1,
        )
        with open_storage(STORAGE_PATH, mode="w") as storage:
            experiment = storage.get_experiment_by_name(EXPERIMENT_NAME)
            post_ensemble = experiment.get_ensemble_by_name(ENSEMBLE_NAME_UPDATE)

            # Parameters in X_prior_group are standard normal N(0,1)
            # and not transformed and should not be transformed
            # before update and not before saving to storage
            print(f"  Store scalar parameters to {ENSEMBLE_NAME_UPDATE}")
            for n, group_name in enumerate(scalar_param_list):
                X_post_group = X_post[n, :]
                post_ensemble.save_parameters_numpy(
                    parameters=X_post_group,
                    param_group=group_name,
                    iens_active_index=iens_active_index,
                )
    else:
        raise ValueError(f"Scalar update method {method} is not implemented.")


def update_storage_with_const_params(
    non_updatable_list: list[str],
    iens_active_index: npt.NDArray[np.int32],
):
    with open_storage(STORAGE_PATH, mode="w") as storage:
        experiment = storage.get_experiment_by_name(EXPERIMENT_NAME)
        prior_ensemble = experiment.get_ensemble_by_name(ENSEMBLE_NAME)
        post_ensemble = experiment.get_ensemble_by_name(ENSEMBLE_NAME_UPDATE)
        complete_df: pl.Dataframe | None = None
        for group_name in non_updatable_list:
            data = prior_ensemble.load_parameters(group_name, iens_active_index)
            if isinstance(data, pl.DataFrame):
                if complete_df is None:
                    complete_df = data
                else:
                    complete_df = complete_df.join(data, on="realization")
            else:
                post_ensemble.save_parameters(dataset=data)

        if complete_df is not None:
            post_ensemble.save_parameters(complete_df)


def write_rho(rho_2D: npt.NDArray[np.double], nx: int, ny: int, nz: int, nobs: int):
    """
    Write RHO for a set of observations.
    Input is RHO for one grid layer of a 3D field parameter for a set of observations.
    Output is a stack of layers of 2D rho with one observation per grid layer
    put into a 3D grid. Each grid layer of the the output 3D parameter corresponds
    to RHO for one observation. Is only used to visualize RHO for one obs at a time
    by seleting layers in the output grid when visualizing.
    """
    max_obs_nr = min(nz, nobs)
    print(f"Write rho for observation number 0 to {max_obs_nr} observations")
    rho_per_layer_stacked_in_3D_param = np.zeros((nx, ny, nz), dtype=np.float32)

    rho_per_layer_stacked_in_3D_param[:, :, :max_obs_nr] = rho_2D[:, :, :max_obs_nr]
    name = "rho_stacked"
    filename = OUTPUT_PATH + "/" + name + ".roff"
    xtgeo_prop = xtgeo.GridProperty(
        ncol=nx,
        nrow=ny,
        nlay=nz,
        name=name,
        grid=ERTBOX_GRID,
        values=rho_per_layer_stacked_in_3D_param,
    )

    print(f"Write file: {filename}")
    xtgeo_prop.to_file(filename, fformat="roff")

    if nobs > nz:
        max_obs_nr = min(nz, nobs - nz)
        print(f"Write RHO for observation {nz} to {max_obs_nr + nz}")
        rho_per_layer_stacked_in_3D_param = np.zeros((nx, ny, nz), dtype=np.float32)
        rho_per_layer_stacked_in_3D_param[:, :, :max_obs_nr] = rho_2D[
            :, :, nz : (nz + max_obs_nr)
        ]
        name = "rho_stacked2"
        filename = OUTPUT_PATH + "/" + name + ".roff"
        xtgeo_prop = xtgeo.GridProperty(
            ncol=nx,
            nrow=ny,
            nlay=nz,
            name=name,
            grid=ERTBOX_GRID,
            values=rho_per_layer_stacked_in_3D_param,
        )
    print(f"Write file: {filename}")
    xtgeo_prop.to_file(filename, fformat="roff")


def get_data_frame_with_observations_for_2D_fields(obs_data_frame) -> pl.DataFrame:
    # Add a new column for the product of main_range and perp_range
    if not isinstance(obs_data_frame, pl.DataFrame):
        raise TypeError("Input data must be a Polars DataFrame.")

    print(f"{obs_data_frame.shape}")
    df = obs_data_frame.with_columns(
        (pl.col("main_range") * pl.col("perp_range")).alias("influence_area")
    )

    # Group by 'observation_key' and select the line with the largest 'influence_area'
    # result_df = df.group_by("observation_key",maintain_order=True)
    result_df = df.group_by("observation_key", maintain_order=True).agg(
        pl.all().sort_by("influence_area", descending=True).first()
    )

    return result_df


def split_observations_into_one_dataframe_per_zone(
    df_obs, input_zone_names: list[str]
) -> dict[str, pl.DataFrame]:
    defined_zone_names = copy.copy(input_zone_names)
    defined_zone_names.sort()

    # Compute `influence_area` column
    df = df_obs.with_columns(
        (pl.col("main_range") * pl.col("perp_range")).alias("influence_area")
    )

    # Split the DataFrame into one DataFrame per `zone_name`
    # Get a list of the zone names
    zone_names = df["zone_name"].unique().to_list()
    zone_names.sort()
    print(f"Zone names found in dataframe: {zone_names}")
    print(f"Zone names defined: {defined_zone_names}")
    if zone_names != defined_zone_names:
        raise ValueError(
            f"Zone names defined for observations: {zone_names} \n"
            f"and zone names defined for the ERT model:  {defined_zone_names}\n"
            "are different."
        )

    # Create a dictionary to store the DataFrames per zone_name
    zone_dataframes = {
        zone: df.filter(pl.col("zone_name") == zone) for zone in zone_names
    }

    # Check if observation_key values are unique per zone
    # Remove columns that are duplicates (other names of the same data) due to
    # earlier merging of dataframe from storage and from input file.
    # The "summary_vector is the same as "response_key",
    # The "date" is the same as "index"
    # The "value" is the same as "observations"
    # The "error" is the same as "std"
    # zone_name is not used after splitting the data frame into one per zone_name
    columns_to_remove = ["zone_name"]
    zone_dataframes_reduced = {}
    for zone_name, df in zone_dataframes.items():
        has_no_duplicated_obs, list_of_duplicated_obs = check_unique_observation_key(df)
        if not has_no_duplicated_obs:
            print("Duplicated observation:")
            print(f" {list_of_duplicated_obs}")
            raise ValueError(
                "Duplicated observations found in "
                f"observation dataframe for zone {zone_name}"
            )
        df_reduced = df.drop(columns_to_remove)
        zone_dataframes_reduced[zone_name] = df_reduced
    return zone_dataframes_reduced


def check_unique_observation_key(df: pl.DataFrame) -> tuple[bool, list]:
    # Group by "observation_key" and count the occurrences
    counts = df.group_by("observation_key").len()

    # Find rows where count > 1 (i.e., non-unique values)
    non_unique = counts.filter(counts["len"] > 1)

    # Extract the non-unique keys as a list
    non_unique_keys = non_unique["observation_key"].to_list()

    # Return True if all keys are unique, otherwise False
    # with the list of non-unique values
    return len(non_unique_keys) == 0, non_unique_keys


def define_observations_for_2D_fields(df_per_zone: dict) -> pl.DataFrame:
    # List of all input DataFrames
    dataframe_list = [df_per_zone[zone_name] for zone_name in list(df_per_zone.keys())]
    df1 = dataframe_list[0]
    # Concatenate all DataFrames into one with an identifier for the source
    combined_df = pl.concat(dataframe_list)

    # Group by "observation_key" and select the row with the max "influence_area"
    # per group
    result_df = (
        combined_df.sort(
            "influence_area", descending=True
        )  # Sort by "influence_area" in descending order
        .group_by("observation_key")
        .agg(pl.col("*").first())  # Select the first row per group after sorting
    )

    # Ensure the result DataFrame has exactly the same columns as the input DataFrames
    result_df = result_df.select(df1.columns)
    return result_df


def ertbox_has_same_xy_layout(
    ertbox_param1: ErtboxParameters, ertbox_param2: ErtboxParameters
) -> bool:
    """
    Check if two different ertbox dict has same xy layout.
    Return True of same, False if not same.
    Parameter nz and zinc is irrelevant here.
    Handedness must be the same to get the same order of the field values.
    """
    if (
        (ertbox_param1.origin == ertbox_param2.origin)
        and (ertbox_param1.rotation_angle == ertbox_param2.rotation_angle)
        and (ertbox_param1.xinc == ertbox_param2.xinc)
        and (ertbox_param1.yinc == ertbox_param2.yinc)
        and (ertbox_param1.nx == ertbox_param2.nx)
        and (ertbox_param1.ny == ertbox_param2.ny)
    ):
        return True
    return False


def define_group_of_zones_with_same_observations(
    observation_df_per_zone: dict[str, pl.DataFrame],
) -> dict[int, list[str]]:
    # Create the output dictionary
    obs_group_dict = {}
    group_number = 1
    processed_zones = set()

    # Compare all DataFrames
    for zone_name, df in observation_df_per_zone.items():
        if zone_name in processed_zones:
            continue  # Skip already grouped zones

        # Initialize a new group for this zone
        identical_zones = [zone_name]

        # Compare this DataFrame with all others
        for other_zone_name, other_df in observation_df_per_zone.items():
            if other_zone_name != zone_name and other_zone_name not in processed_zones:
                if df.equals(other_df):
                    identical_zones.append(other_zone_name)

        # Assign this group to the output dictionary
        obs_group_dict[group_number] = identical_zones
        group_number += 1

        # Mark all zones in this group as processed
        processed_zones.update(identical_zones)

    return obs_group_dict


def test_transform_coordinates():
    n = 4
    ertbox_params = ErtboxParameters()
    ertbox_params.nx = NX
    ertbox_params.ny = NY
    ertbox_params.nz = NZ
    ertbox_params.xlength = NX * XINC
    ertbox_params.yength = NY * YINC
    ertbox_params.xinc = XINC
    ertbox_params.yinc = YINC
    ertbox_params.rotation_angle = ROTATION
    ertbox_params.origin = (XORIGO, YORIGO)

    xpos = np.zeros(n, dtype=np.float64)
    ypos = np.zeros(n, dtype=np.float64)
    angle = np.zeros(n, dtype=np.float64)

    # Lower left corner
    xpos[0] = ertbox_params.origin[0]
    ypos[0] = ertbox_params.origin[1]
    angle[0] = ROTATION

    # Upper left corner
    xpos[1] = 456070
    ypos[1] = 5935985
    angle[1] = ROTATION + 90.0

    # Upper right corner
    xpos[2] = 462062
    ypos[2] = 5939430
    angle[2] = 0.0

    # Lower right corner
    xpos[3] = 467497
    ypos[3] = 5929973
    angle[3] = 0.0

    # ERTBOX coordinates
    xpos_transf, ypos_transf, angle_transf = transform_to_local_coordinates_3D(
        ertbox_params, xpos, ypos, angle
    )
    print("Transformed coordinates into ERTBOX coordinates for test points:")
    for i in range(n):
        print(f"{i}  {xpos_transf[i]}, {ypos_transf[i]}, {angle_transf[i]}")

    print("")
    #
    with open_storage(STORAGE_PATH, "r") as storage:
        experiment = storage.get_experiment_by_name(EXPERIMENT_NAME)
        param_config_all = experiment.parameter_configuration
        groups = list(param_config_all.keys())
        surface_field_names = get_2D_field_parameter_names(groups, param_config_all)
        for group_name in groups:
            if group_name in surface_field_names:
                surf_config = param_config_all[group_name]
                print("")
                print(group_name)
                print(f"{surf_config=}")
                yflip = surf_config.yflip

                surface_coord_dict = {
                    "name": surf_config.name,
                    "nx": surf_config.ncol,
                    "ny": surf_config.nrow,
                    "yflip": yflip,
                    "xorigo": surf_config.xori,
                    "yorigo": surf_config.yori,
                    "xinc": surf_config.xinc,
                    "yinc": surf_config.yinc,
                    "rotation": surf_config.rotation,
                    "xrotationpoint": surf_config.xori,
                    "yrotationpoint": surf_config.yori,
                }
                # Local surface coordinates
                print(f" {json.dumps(surface_coord_dict, indent=4)}")
                xpos_transf, ypos_transf, angle_transf = (
                    transform_to_local_coordinates_2D(
                        surface_coord_dict, xpos, ypos, angle
                    )
                )
                print(
                    "Transformed coordinates into surface coordinates for test points:"
                )
                for i in range(n):
                    print(f"{i}  {xpos_transf[i]}, {ypos_transf[i]}, {angle_transf[i]}")


def test_calc_rho_one_layer():
    ertbox_params = ErtboxParameters()
    ertbox_params.nx = NX
    ertbox_params.ny = NY
    ertbox_params.nz = NZ
    ertbox_params.xlength = NX * XINC
    ertbox_params.yength = NY * YINC
    ertbox_params.xinc = XINC
    ertbox_params.yinc = YINC
    ertbox_params.rotation_angle = ROTATION
    ertbox_params.origin = (XORIGO, YORIGO)

    xsize = ertbox_params.xlength
    ysize = ertbox_params.ylength

    nobs = 4
    obs_xpos = np.zeros(nobs, dtype=np.float64)
    obs_ypos = np.zeros(nobs, dtype=np.float64)
    obs_perp_range = np.zeros(nobs, dtype=np.float64)
    obs_main_range = np.zeros(nobs, dtype=np.float64)
    obs_anisotropy_angle = np.zeros(nobs, dtype=np.float64)

    obs_xpos[0] = xsize / 2
    obs_ypos[0] = ysize / 2
    obs_anisotropy_angle[0] = 45.0
    obs_main_range[0] = 1500
    obs_perp_range[0] = 1250

    obs_xpos[1] = xsize
    obs_ypos[1] = 0
    obs_anisotropy_angle[1] = -45.0
    obs_main_range[1] = xsize
    obs_perp_range[1] = xsize / 10

    obs_xpos[2] = 0
    obs_ypos[2] = ysize
    obs_anisotropy_angle[2] = -45.0
    obs_main_range[2] = xsize
    obs_perp_range[2] = xsize / 10

    obs_xpos[3] = xsize
    obs_ypos[3] = ysize
    obs_anisotropy_angle[3] = 45.0
    obs_main_range[3] = xsize
    obs_perp_range[3] = xsize / 10

    rho_one_layer = calculate_rho_for_summary_obs_for_one_layer_of_3D_field(
        ertbox_params,
        obs_xpos,
        obs_ypos,
        obs_main_range,
        obs_perp_range,
        obs_anisotropy_angle,
        scaling_function_name="gaussian",
    )
    print(f"{rho_one_layer.shape=}")
    #    print(f"{rho_one_layer=}")
    nx = ertbox_params.nx
    ny = ertbox_params.ny
    nz = ertbox_params.nz
    values3d = np.zeros((nx, ny, nz), dtype=np.float32)
    for n in range(nobs):
        name = "rho_" + str(n)
        rho_2d = rho_one_layer[:, n].reshape((nx, ny), order="F")
        for k in range(nz):
            values3d[:, :, k] = rho_2d[:, :]
        rho_property = xtgeo.GridProperty(
            ncol=nx, nrow=ny, nlay=nz, name="rho", values=values3d
        )
        filename = name + ".roff"
        print(f"Write file: {filename}")
        rho_property.to_file(filename, fformat="roff", name=name)


def merge_storage_and_rms_created_dataframes_of_obs(
    observations_and_responses, df_obs_with_localization_param
):
    # Merge the two dataframes
    df_obs_merged = observations_and_responses.join(
        df_obs_with_localization_param, on="observation_key", how="inner"
    )
    # The 12 last columns is moved to column 9 to 22
    nlast_columns = 12
    nfirst_columns = 8
    last_columns = df_obs_merged.columns[-nlast_columns:]
    other_columns = df_obs_merged.columns[:-nlast_columns]
    new_order = (
        other_columns[:nfirst_columns] + last_columns + other_columns[nfirst_columns:]
    )
    df_obs = df_obs_merged.select(new_order)

    if DEBUG_PRINT:
        columns_to_print = [
            "observation_key",
            "observations",
            "value",
            "east",
            "north",
            "radius",
            "xpos",
            "ypos",
            "main_range",
            "perp_range",
            "anisotropy_angle",
            "zone_name",
        ]
        selected_columns = [col for col in df_obs.columns if col in columns_to_print]
        print("obs merged")
        print(df_obs.select(selected_columns))
        os.exit(0)
    return df_obs

def add_columns_for_localization_range_and_zone(observations_and_responses):
    # This function will duplicate the summary observations with one per zone
    # and add main_range, perp_range, zone_name to make it compatible with the rest of the code
    # This function removes all observations without localization parameters
    df = observations_and_responses.with_columns(
        [
            pl.col("radius").alias("main_range"),
            pl.col("radius").alias("perp_range"),
            pl.lit(0).alias("anisotropy_angle"),
            pl.lit("Valysar").alias("zone_name")
        ]
    )
    columns_to_remove = ["radius"]
    df = df.drop(columns_to_remove)
        # The 4 last columns is moved to column 8 to 11
    nlast_columns = 4
    nfirst_columns = 7
    last_columns = df.columns[-nlast_columns:]
    other_columns = df.columns[:-nlast_columns]
    new_order = (
        other_columns[:nfirst_columns] + last_columns + other_columns[nfirst_columns:]
    )
    df_obs = df.select(new_order)

    # Duplicate the dataframe with identical copies, one marked with Valysar, Therys and Volon
    zone_names = ["Valysar", "Therys", "Volon"]
    duplicates = [
        df.with_columns(pl.lit(zone).alias("zone_name"))
        for zone in zone_names
    ]
    df_obs = pl.concat(duplicates)

    # Remove observations without localization parameters
    df_obs_local = df_obs.filter(pl.col("east").is_not_null())

    # Remove observations with localization parameters
    df_obs_not_local = df_obs.filter(pl.col("east").is_null())

    if DEBUG_PRINT:
        columns_to_print = [
            "observation_key",
            "observations",
            "east",
            "north",
            "main_range",
            "perp_range",
            "anisotropy_angle",
            "zone_name",
        ]
        selected_columns = [col for col in df_obs.columns if col in columns_to_print]
        print("Selected columns with summary obs for observations with localization parameters:")
        print(df_obs_local.select(selected_columns))

#        print("Selected columns with summary obs for observations with no localization parameters:")
#        print(df_obs_not_local.select(selected_columns))


    return df_obs_local, df_obs_not_local

def main():
    if not USE_LOCALIZATION:
        print("NOTE: RUN WITH RHO = 1  (NO LOCALIZATION)")
    if SKIP_GLOBAL_UPDATE:
        print("Skip running case with global ESMDA")
    if SKIP_DL_UPDATE:
        print("Skip running case with distance-based localization")
    if not USE_RFT_OBS:
        print("Skip using RFT obs")
    if not USE_SEIS_OBS:
        print("Skip using seismic obs")
    if not WRITE_STATS_PARAMS:
        print("Skip writing updated fields to files")
    if not SAVE_UPDATE_TO_STORAGE:
        print("Updated parameters are not saved to ERT storage")

    Nmax_rows = 500
    Nmax_cols = 15
    pl.Config.set_tbl_rows(Nmax_rows)
    pl.Config.set_tbl_cols(Nmax_cols)
    if TESTS:
        import sys

        test_transform_coordinates()
        test_calc_rho_one_layer()
        sys.exit(0)

    #    if DEBUG_PRINT:
    with open_storage(STORAGE_PATH, mode="r") as storage:
        print("Available experiment names:")
        for x in storage.experiments:
            print(f"  {x.name}")
        print("")

    if SAVE_UPDATE_TO_STORAGE:
        # Create empty ensembles for the updated parameters, one for
        # distance-based localization (DL) and one for ordinary ESMDA (global
        # NOTE: The iteration must be set to a number > 0 to ensure that
        # the ensemble is treated as an ensemble with updated parameters
        # when running ERT GUI with ensemble evaluation to ensure that
        # ERT will export ALL parameters (both the scalar and 2D and 3D
        # field parameters to RUNPATH before starting the forward models
        # and also ensure that ERT environment variable for ITER is not 0
        # (which means initial ensemble) since we don't want the
        # forward model to create initial ensemble but to use the
        # parameters in storage for this ensemble as updated
        # parameters to be fetched from RUNPATH into the jobs in RMS for
        # 2D fields (surfaces) and 3D fields (APS field parameters)
        with open_storage(STORAGE_PATH, mode="w") as storage:
            experiment = storage.get_experiment_by_name(EXPERIMENT_NAME)
            prior_ensemble = experiment.get_ensemble_by_name(ENSEMBLE_NAME)
            ensemble_size = prior_ensemble.ensemble_size
            print(
                f"Step 0: Create empty ensemble {ENSEMBLE_NAME_UPDATE} "
                "of updated parameters"
            )
            experiment.create_ensemble(
                ensemble_size=ensemble_size,
                name=ENSEMBLE_NAME_UPDATE,
                prior_ensemble=prior_ensemble,
                iteration=1,
            )
            if not SKIP_GLOBAL_UPDATE:
                print(
                    f"        Create empty ensemble {ENSEMBLE_NAME_UPDATE_GLOBAL} "
                    "of updated parameters"
                )

                experiment.create_ensemble(
                    ensemble_size=ensemble_size,
                    name=ENSEMBLE_NAME_UPDATE_GLOBAL,
                    prior_ensemble=prior_ensemble,
                    iteration=1,
                )

    print(f"Step 1: Get prior ensemble {ENSEMBLE_NAME} and observations from storage")
    with open_storage(STORAGE_PATH, mode="r") as storage:
        experiment = storage.get_experiment_by_name(EXPERIMENT_NAME)
        ensemble = experiment.get_ensemble_by_name(ENSEMBLE_NAME)
        param_config_all = experiment.parameter_configuration
        groups = list(param_config_all.keys())
        realizations = ensemble.get_realization_list_with_responses()
        selected_obs_keys = ensemble.experiment.observation_keys

        iens_active_index = np.array(ensemble.get_realization_list_with_responses())
        observations_and_responses = ensemble.get_observations_and_responses(
            selected_obs_keys, iens_active_index
        )

        print("Step 2: Get list of 3D field parameters and ertbox per field if any.")
        # dict with zone name as key and value that is a list of
        # 3D field parameter names
        field_param_3d_list_per_zone_dict = get_3D_field_parameter_names_per_zone(
            groups, param_config_all, ZONE_PER_3D_FIELD_PARAM_GROUP
        )
        ertbox_per_field_param_dict = get_ertbox_per_field_param_group(
            groups, param_config_all
        )
        if DEBUG_PRINT:
            print(" Dict of 3D field parameters:")
            print(f" {json.dumps(field_param_3d_list_per_zone_dict, indent=3)}")

        print("Step 3: Get list of 2D field parameters if any.")
        field_param_2d_list = get_2D_field_parameter_names(groups, param_config_all)
        if DEBUG_PRINT:
            print(" List of 2D field parameters:")
            print(f" {json.dumps(field_param_2d_list, indent=3)}")

        print("Step 4: Get list of scalar parameters if any")
        scalar_param_list, non_updatable_list = get_scalar_parameter_names(
            groups, param_config_all
        )
        if DEBUG_PRINT:
            print(" List of scalar parametr groups:")
            print(f" {json.dumps(scalar_param_list, indent=3)}")

#    print(
#        "Step 5: Get observations with position and localization"
#        " parameters from file generated by an RMS script."
#    )
#    df_obs_with_local = read_observations_with_localization_attributes(
#        OBS_SUMMARY_WITH_LOCAL_ATTRIBUTES_FILE
#    )

    print(
        "Step 6: Add new columns to the dataframe from ERT storage to make it compatible with the code"
    )

#    if not SKIP_MERGE_SUMMARY_OBS_FROM_RMS_AND_ERT:
#        df_obs = merge_storage_and_rms_created_dataframes_of_obs(
#            observations_and_responses, df_obs_with_local
#        )
#    else:
    df_obs, df_obs_not_local = add_columns_for_localization_range_and_zone(observations_and_responses)

    if USE_SEIS_OBS:
        print(
            "Step 7: Read seismic obs from config path and response from scratch disk."
        )
        print("Assign localization range for each seismic obs")
        df_seis_obs_and_response_per_zone = read_seis_obs_and_response_for_3D_fields_per_zone(  # noqa
            iens_active_index=iens_active_index,
            seismic_obs_and_response_per_zone=SEISMIC_OBS_AND_RESPONSE_DICT_PER_ZONE,
        )
        if DEBUG_PRINT:
            print("Selected columns of observation dataframe from seismic:")
            selected_columns = [
                "response_key",
                "observation_key",
                "observations",
                "std",
                "xpos",
                "ypos",
                "main_range",
                "perp_range",
                "anisotropy_angle",
                "zone_name",
            ]
            for zone_name in list(SEISMIC_OBS_AND_RESPONSE_DICT_PER_ZONE.keys()):
                print(f"Zone name: {zone_name}")
                print(
                    df_seis_obs_and_response_per_zone[zone_name].select(
                        selected_columns
                    )
                )

        df_seis_obs_and_response_for_2D_fields = read_seis_obs_and_response_for_2D_fields(  # noqa
            iens_active_index=iens_active_index,
            seismic_obs_and_response_for_2D_fields=SEISMIC_OBS_AND_RESPONSE_DICT_FOR_2D_FIELDS,
        )
        if DEBUG_PRINT:
            print("Selected columns of observation dataframe from seismic:")
            selected_columns = [
                "response_key",
                "observation_key",
                "observations",
                "std",
                "xpos",
                "ypos",
                "main_range",
                "perp_range",
                "anisotropy_angle",
                "zone_name",
            ]
            print("Seismic obs data frame for 2D fields")
            print(df_seis_obs_and_response_for_2D_fields.select(selected_columns))

    if USE_RFT_OBS:
        print("Step 8: Read RFT obs and response from scratch disk")
        df_rft_obs_and_response = read_rft_obs_and_response(
            iens_active_index,
            RFT_OBS_AND_RESPONSE_DICT,
        )
        if DEBUG_PRINT:
            print("Selected columns of observation dataframe from rft:")
            selected_columns = [
                "observations",
                "std",
                "xpos",
                "ypos",
                "main_range",
                "perp_range",
                "anisotropy_angle",
                "zone_name",
                "0",
                "99",
            ]
            for zone_name in list(RFT_OBS_AND_RESPONSE_DICT.keys()):
                print(f"zone: {zone_name}:")
                print(df_rft_obs_and_response[zone_name].select(selected_columns))

    if USE_TRACER_OBS:
        print("Step 9: Read TRACER obs")
        df_tr = read_tracer_obs(
            TRACER_OBS_FILENAME, TRACER_RESPONSE_FILENAME, iens_active_index
        )

    print("Step 10: Split summary observations into one set per zone")
    # dict with observation dataframe per zone
    df_obs_per_zone_dict = split_observations_into_one_dataframe_per_zone(df_obs, ZONES)

    print(
        "Step 11: Define a dataframe for 2D field parameters where the largest "
        "influence range per summary observation is used as "
        "localization parameters."
    )
    # observation dataframe to be used for 2D fields
    df_obs_for_2D_fields = define_observations_for_2D_fields(df_obs_per_zone_dict)
    if DEBUG_PRINT:
        print("Selected columns of observation dataframe for 2D field parameters:")
        selected_columns = [
            "response_key",
            "index",
            "observation_key",
            "east",
            "north",
            "main_range",
            "perp_range",
            "anisotropy_angle",
        ]
        print(df_obs_for_2D_fields.select(selected_columns))

    if USE_SEIS_OBS:
        print("Step 12: Combine summary obs with seismic obs")
        # Will define one dataframe per zone for obs and response for
        # conditioning of 3D fields
        # Will define one dataframe with all obs for conditioning of all 2D fields"

        # Re-arrange colums of obs dataframe for seismic before merging
        zones = list(df_obs_per_zone_dict.keys())
        df_obs_one_zone = df_obs_per_zone_dict[zones[0]]
        df_seis_obs_and_response_per_zone_rearranged = {}
        for zone_name, df in df_seis_obs_and_response_per_zone.items():
            df_reordered = df.select(df_obs_one_zone.columns)
            df_seis_obs_and_response_per_zone_rearranged[zone_name] = df_reordered

        # Merge dataframe with summary obs and seismic obs for 3D fields (zones)
        df_obs_with_seis_per_zone_dict = {}
        for zone_name in zones:
            print(f" Add seismic obs to summary observations for zone {zone_name}")
            df_summary = df_obs_per_zone_dict[zone_name]
            df_seis = df_seis_obs_and_response_per_zone_rearranged[zone_name]
            df = pl.concat([df_summary, df_seis], how="vertical")
            df_obs_with_seis_per_zone_dict[zone_name] = df
        df_obs_per_zone_dict = df_obs_with_seis_per_zone_dict

        # Re-arrange colums of obs dataframe for seismic for 2D fields before merging
        # and merge summary obs and seismic obs for conditioning of 2D fields
        print(" Add seismic obs to summary observations for 2D fields")
        df_rearranged = df_seis_obs_and_response_for_2D_fields.select(
            df_obs_for_2D_fields.columns
        )
        df_obs_for_2D_fields = pl.concat(
            [df_obs_for_2D_fields, df_rearranged], how="vertical"
        )

    if USE_RFT_OBS:
        print("Step 13: Combine obs with RFT obs")
        # Will define one dataframe per zone for obs and response for
        # conditioning of 3D fields
        # Will define one dataframe with all obs for conditioning of all 2D fields"
        # Re-arrange colums of obs dataframe for rft before merging
        zones = list(df_obs_per_zone_dict.keys())
        df_obs_one_zone = df_obs_per_zone_dict[zones[0]]
        df_rft_obs_and_response_rearranged = {}
        for zone_name, df in df_rft_obs_and_response.items():
            df_reordered = df.select(df_obs_one_zone.columns)
            df_rft_obs_and_response_rearranged[zone_name] = df_reordered

        # Merge dataframe with summary obs and rft obs for 3D fields (zones)
        df_obs_with_rft_per_zone_dict = {}
        for zone_name in zones:
            print(f" Add rft obs to summary observations for zone {zone_name}")
            df_summary = df_obs_per_zone_dict[zone_name]
            df_rft = df_rft_obs_and_response_rearranged[zone_name]
            df = pl.concat([df_summary, df_rft], how="vertical")
            df_obs_with_rft_per_zone_dict[zone_name] = df
        df_obs_per_zone_dict = df_obs_with_rft_per_zone_dict

        # Merge dataframe with summary obs and seismic obs for 2D fields (surfaces)
        for zone_name, df_rft in df_rft_obs_and_response_rearranged.items():
            print(f" Add rft obs for zone {zone_name} to observations for 2D fields")
            df_obs_for_2D_fields = pl.concat(
                [df_obs_for_2D_fields, df_rft], how="vertical"
            )

    if USE_TRACER_OBS:
        print("Step 14: Combine obs with TRACER obs")

        # Merge dataframe with summary obs, RFT obs,
        # tracer obs and seismic for 3D fields (zones)
        df_merged_dict = {}
        for key, df in df_obs_per_zone_dict.items():
            print(f" Add tracer obs to observations for {key}")
            df = pl.concat([df, df_tr], how="vertical")
            df_merged_dict[key] = df
        df_obs_per_zone_dict = df_merged_dict

        # Merge dataframe with summary obs and RFT obs for 2D fields (surfaces)
        print(" Add RFT obs to observations for 2D fields")
        df_obs_for_2D_fields = pl.concat([df_obs_for_2D_fields, df_tr], how="vertical")

    if DEBUG_PRINT:
        selected_columns = [
            "observation_key",
            "observations",
            "std",
            "east",
            "north",
            "main_range",
            "perp_range",
            "anisotropy_angle",
        ]
        print("Data frames for 3D fields per zone using selected columns:")
        for zone_name, df in df_obs_per_zone_dict.items():
            print(f"{zone_name} {df.select(selected_columns)}")

        print(
            " Dataframe for 2D fields using selected columns: "
            f"{df_obs_for_2D_fields.select(selected_columns)}"
        )

    if not SKIP_DL_UPDATE:
        print("")
        print("Step 15: Run update with DistanceESMDA (one iteration)")
        update_with_distance_esmda_new(
            iens_active_index,
            realizations,
            df_obs_per_zone_dict,
            df_obs_for_2D_fields,
            ALPHA,
            SEED,
            field_param_3d_list_per_zone_dict,
            field_param_2d_list,
            scalar_param_list,
            non_updatable_list,
            ertbox_per_field_param_dict,
            NLAYER_PER_BATCH,
            SCALING_FUNCTION_NAME,
            TRUNCATION,
        )

    # Run the same with ordinary ESMDA with 1 iteration
    if not SKIP_GLOBAL_UPDATE:
        print("")
        print("Step 16: Run update with ordinary ESMDA (one iteration)")
        update_with_esmda(
            iens_active_index,
            realizations,
            df_obs_per_zone_dict,
            df_obs_for_2D_fields,
            SEED,
            ALPHA,
            field_param_3d_list_per_zone_dict,
            field_param_2d_list,
            scalar_param_list,
            non_updatable_list,
            ertbox_per_field_param_dict,
        )


# --- main ---
if __name__ == "__main__":
    #    calculate_rho_for_summary_obs_for_one_layer_of_3D_field = \
    #     monitor_performance(calculate_rho_for_summary_obs_for_one_layer_of_3D_field)
    #    calculate_rho_for_summary_obs_for_2D_field = \
    #     monitor_performance(calculate_rho_for_summary_obs_for_2D_field)
    #    update_3D_field_with_distance_esmda = \
    #     monitor_performance(update_3D_field_with_distance_esmda)
    #    update_2D_field_with_distance_esmda = \
    #     monitor_performance(update_2D_field_with_distance_esmda)
    update_with_esmda = monitor_performance(update_with_esmda)
    update_with_distance_esmda_new = monitor_performance(update_with_distance_esmda_new)

    print("Use of this test script:")
    print(" Workflow steps:")
    example_config_for_rms_script = (
        "example_config_to_get_pos_and_loc_params_from_rms.yml"
    )
    rms_python_job_script = "create_obs_with_localisation_attributes.py"
    print(" In RMS project with Drogon case, define a workflow with one python job.")
    print(f" Use the script : {rms_python_job_script} in the RMS workflow.")
    print(" Edit the input config file for this script to set localization ranges.")
    print(
        f" Example of input to the RMS python script is {example_config_for_rms_script}"
    )
    print(
        " When the file with observations with localization ranges "
        "and position is created,"
    )
    print(
        " edit the file path, and input file for current script and other "
        "global settings."
    )
    print(" Run the current script. With DEBUG_PRINT set, more files are created.")
    print(" In particular files for RHO are created to visualize RHO per obs.")
    print(" Visualize: Open a Drogon RMS project. Add a new workflow to import")
    print(" the mean and std and difference 3D parameters.")
    print(" The ROFF files must be imported into the GridModel 'ERTBOX' ")
    print(
        " Current version of Drogon has one common ERTBOX for all "
        "zones in the geomodel."
    )
    print(
        " The relevant ERTBOX parameters are hardcoded in current version "
        "of the test script."
    )
    print("")
    print("Results to be produced:")
    print(" Mean and Std over ensemble of field parameters.")
    print(" Difference between posterior and prior mean and the same for std.")
    print(" For 3D fields, (FIELD keyword in ERT)  ROFF binary format is used.")
    print(" For 2D fields, (SURFACE keyword in ERT) IRAP ASCII format is used.")
    print(
        " Optionally ROFF files and IRAP files for RHO is created with DEBUG_PRINT on."
    )
    print("")
    print("---------------------------------------------------------------------")
    print("")
    main()
