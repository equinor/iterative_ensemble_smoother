{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2571751c-1de1-41ed-9c99-df0c142d8792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from iterative_ensemble_smoother.experimental import DistanceESMDA\n",
    "from iterative_ensemble_smoother.esmda import ESMDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5145d165-23c9-4290-be6a-ef143b043a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import iterative_ensemble_smoother"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52439e7b-81be-4654-ba71-c8801fc68f12",
   "metadata": {},
   "source": [
    "# 1D Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16fb8e2-3e19-4a8e-a340-4098843ab1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System properties\n",
    "N_m = 100  # Number of model parameters (grid points)\n",
    "N_e = 50   # Ensemble size\n",
    "j_obs = 50 # Index of the single observation\n",
    "N_d = 1    # Number of observations\n",
    "\n",
    "# Assimilation properties\n",
    "alpha_i = 1 # TODO: Implement multiple iterations\n",
    "obs_error_var = 0.01 # Variance of observation error\n",
    "SEED = 42\n",
    "\n",
    "# Define the \"true\" model parameters\n",
    "true_parameters = np.zeros(N_m)\n",
    "\n",
    "# Define the \"true\" observation vector (1D)\n",
    "true_observations = np.array([1.0])\n",
    "\n",
    "# Observation error covariance `C_D`. Since N_d=1, it's a 1-element vector.\n",
    "C_D = np.array([obs_error_var])\n",
    "\n",
    "# --- 2. Generate Initial Ensemble and Predictions ---\n",
    "\n",
    "# Create initial ensemble of parameters `X` (prior)\n",
    "# Shape: (N_m, N_e)\n",
    "rng = np.random.default_rng(SEED)\n",
    "X_initial = rng.normal(loc=0.0, scale=0.5, size=(N_m, N_e))\n",
    "\n",
    "# Predict observations `Y` using the identity model `g(x) = x`\n",
    "# We only observe the state at `j_obs`.\n",
    "# Shape: (N_d, N_e) -> (1, 50)\n",
    "Y = X_initial[[j_obs], :]\n",
    "\n",
    "\n",
    "# --- 3. Construct Localization, Covariance, and Smoother ---\n",
    "\n",
    "# Localization matrix `rho`\n",
    "# Shape: (N_m, N_d) -> (100, 1)\n",
    "localization_radius = 10.0\n",
    "model_grid = np.arange(N_m)\n",
    "distances = np.abs(model_grid - j_obs)\n",
    "# Using a simple Gaussian decay for rho\n",
    "rho = np.exp(-0.5 * (distances / localization_radius)**2).reshape(-1, 1)\n",
    "\n",
    "smoother = DistanceESMDA(\n",
    "    covariance=C_D,\n",
    "    observations=true_observations,\n",
    "    alpha=1,\n",
    "    seed=SEED\n",
    ")\n",
    "smoother_ESMDA = ESMDA(\n",
    "    covariance=C_D,\n",
    "    observations=true_observations,\n",
    "    alpha=1,\n",
    "    seed=SEED)\n",
    "\n",
    "D = smoother.perturb_observations(\n",
    "    ensemble_size=N_e, alpha=alpha_i\n",
    ")\n",
    "\n",
    "# --- 4. Run Assimilation and Analyze ---\n",
    "\n",
    "# Run the assimilation\n",
    "X_posterior = smoother.assimilate(X=X_initial, Y=Y, rho=rho)\n",
    "X_posterior_ESMDA = smoother_ESMDA.assimilate(X=X_initial, Y=Y)\n",
    "\n",
    "# Calculate the mean of the prior and posterior ensembles\n",
    "prior_mean = np.mean(X_initial, axis=1)\n",
    "posterior_mean = np.mean(X_posterior, axis=1)\n",
    "posterior_mean_ESMDA = np.mean(X_posterior_ESMDA, axis=1)\n",
    "\n",
    "# --- 5. Plot the Results ---\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True)\n",
    "\n",
    "# Plot 1: Ensemble means\n",
    "# Use a colorblind-safe palette and distinct line styles\n",
    "# From Paul Tol's color schemes: https://personal.sron.nl/~pault/\n",
    "color_localized = '#0072B2'  # Blue\n",
    "color_non_localized = '#D55E00' # Orange\n",
    "\n",
    "ax1.plot(model_grid, true_parameters, color='black', linestyle='-', linewidth=2, label='True Parameters')\n",
    "ax1.plot(model_grid, prior_mean, color='gray', linestyle='--', linewidth=1.5, label='Prior Mean')\n",
    "ax1.plot(model_grid, posterior_mean, color=color_localized, linestyle='-', linewidth=2.5, label='Localized Posterior Mean')\n",
    "ax1.plot(model_grid, posterior_mean_ESMDA, color=color_non_localized, linestyle='-.', linewidth=2.5, label='Non-Localized Posterior Mean')\n",
    "ax1.axvline(j_obs, color='black', linestyle=':', linewidth=1.5, label=f'Observation Location (x={j_obs})')\n",
    "\n",
    "ax1.set_ylabel('Parameter Value')\n",
    "ax1.set_title('Effect of Localized vs. Non-Localized Data Assimilation')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot 2: Localization weights\n",
    "ax2.plot(model_grid, rho, color='black', label='Localization Weight (rho)')\n",
    "ax2.set_xlabel('Model Parameter Index (Grid Point)')\n",
    "ax2.set_ylabel('Weight')\n",
    "ax2.set_title('Localization Function')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# What to expect:\n",
    "# The prior mean (gray dashed line) should be a noisy line centered around 0.\n",
    "#\n",
    "# The Localized Posterior Mean (red solid line) should be pulled from approx. 0 towards the\n",
    "# observation value of 1.0. This update should be strong at the observation location (x=50)\n",
    "# and decay smoothly to zero away from it, following the shape of the localization function.\n",
    "#\n",
    "# The Non-Localized Posterior Mean (green dash-dot line) will show updates across the ENTIRE\n",
    "# domain, not just near the observation. This is because a small ensemble size creates spurious\n",
    "# correlations between the observation and distant, unrelated parameters.\n",
    "#\n",
    "# Consequently, the non-localized update will look noisy and physically unrealistic across the\n",
    "# domain. This demonstrates why localization is essential to generate plausible reservoir models,\n",
    "# as applying updates without it can lead to results that differ significantly from the prior models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661a67b0-67d9-4ab0-a650-cda668946475",
   "metadata": {},
   "source": [
    "# 2D Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8df4568-f563-4f3d-8e6e-a43ddd096289",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nx, Ny = 10, 10\n",
    "N_m = Nx * Ny\n",
    "N_e = 50\n",
    "x_obs, y_obs = 5, 5\n",
    "\n",
    "seed = 42\n",
    "\n",
    "alpha_i = 1\n",
    "obs_error_var = 0.01\n",
    "\n",
    "true_parameters = np.zeros(N_m)\n",
    "true_observations = np.array([1.0])\n",
    "C_D = np.array([obs_error_var])\n",
    "\n",
    "# --- Generate Initial Ensemble and Predictions ---\n",
    "rng = np.random.default_rng(seed)\n",
    "X_prior = rng.normal(loc=0.0, scale=0.5, size=(N_m, N_e))\n",
    "\n",
    "# Convert the 2D observation index to a flat 1D index for slicing\n",
    "flat_obs_index = y_obs * Nx + x_obs\n",
    "Y = X_prior[[flat_obs_index], :]\n",
    "\n",
    "# --- Construct 2D Localization `rho` ---\n",
    "localization_radius = 1\n",
    "\n",
    "# Create a 2D coordinate grid\n",
    "xx, yy = np.meshgrid(np.arange(Nx), np.arange(Ny))\n",
    "# Calculate 2D Euclidean distance from every point to the observation\n",
    "distances_2d = np.sqrt((xx - x_obs)**2 + (yy - y_obs)**2)\n",
    "# Flatten the 2D distance map to a 1D vector to match the parameter vector\n",
    "distances = distances_2d.flatten()\n",
    "\n",
    "rho = np.exp(-0.5 * (distances / localization_radius) ** 2).reshape(-1, 1)\n",
    "\n",
    "# --- Run Assimilations ---\n",
    "esmda_distance = DistanceESMDA(\n",
    "    covariance=C_D, observations=true_observations, alpha=alpha_i, seed=rng\n",
    ")\n",
    "X_posterior = esmda_distance.assimilate(X=X_prior, Y=Y, rho=rho)\n",
    "\n",
    "esmda = ESMDA(\n",
    "    covariance=C_D, observations=true_observations, alpha=alpha_i, seed=rng\n",
    ")\n",
    "X_posterior_global = esmda.assimilate(X=X_prior, Y=Y)\n",
    "\n",
    "# --- Reshape Data for Plotting ---\n",
    "# Reshape the 1D vectors back into 2D grids for visualization\n",
    "prior_mean = np.mean(X_prior, axis=1)\n",
    "prior_mean_2d = prior_mean.reshape((Ny, Nx))\n",
    "posterior_mean = np.mean(X_posterior, axis=1)\n",
    "posterior_mean_2d = posterior_mean.reshape((Ny, Nx))\n",
    "\n",
    "posterior_mean_global = np.mean(X_posterior_global, axis=1)\n",
    "posterior_mean_global_2d = posterior_mean_global.reshape((Ny, Nx))\n",
    "rho_2d = rho.reshape((Ny, Nx))\n",
    "\n",
    "# It's often more insightful to visualize the UPDATE (posterior - prior)\n",
    "update_localized_2d = posterior_mean_2d - prior_mean_2d\n",
    "update_global_2d = posterior_mean_global_2d - prior_mean_2d\n",
    "\n",
    "# --- Create the Plots ---\n",
    "# Use a 2x3 grid to show the most important comparisons\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 10))\n",
    "\n",
    "# Find the min/max across both update fields for a fair comparison\n",
    "update_min = min(update_localized_2d.min(), update_global_2d.min())\n",
    "update_max = max(update_localized_2d.max(), update_global_2d.max())\n",
    "\n",
    "# Localization Function (rho)\n",
    "im = axes[0].imshow(rho_2d, cmap='viridis')\n",
    "axes[0].set_title('Localization Function (rho)')\n",
    "\n",
    "# Localized Update\n",
    "im = axes[1].imshow(update_localized_2d, cmap='coolwarm', vmin=update_min, vmax=update_max)\n",
    "axes[1].set_title('Localized Update')\n",
    "\n",
    "# Non-Localized Update\n",
    "im = axes[2].imshow(update_global_2d, cmap='coolwarm', vmin=update_min, vmax=update_max)\n",
    "axes[2].set_title('Non-Localized Update')\n",
    "\n",
    "# Mark the observation location on all plots and hide ticks\n",
    "for ax in axes.flat:\n",
    "    ax.plot(x_obs, y_obs, 'r+', markersize=12, markeredgewidth=2) # Red cross\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da966e58-c4da-4470-bad0-32dec9b07452",
   "metadata": {},
   "source": [
    "# 3D case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eb8b0a-1942-44db-bf9d-f79e9239bb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nx, Ny, Nz = 92, 146, 66\n",
    "N_m = Nx * Ny * Nz\n",
    "N_e = 100\n",
    "x_obs, y_obs, z_obs = 50, 50, 2\n",
    "\n",
    "seed = 42\n",
    "alpha_i = 1\n",
    "obs_error_var = 0.01\n",
    "\n",
    "true_parameters = np.zeros(N_m)\n",
    "true_observations = np.array([1.0])\n",
    "C_D = np.array([obs_error_var])\n",
    "\n",
    "# --- Generate Initial Ensemble and Predictions ---\n",
    "rng = np.random.default_rng(seed)\n",
    "X_prior = rng.normal(loc=0.0, scale=0.5, size=(N_m, N_e))\n",
    "\n",
    "# Convert the 3D observation index to a flat 1D index for slicing\n",
    "flat_obs_index = (z_obs * Nx * Ny) + (y_obs * Nx) + x_obs\n",
    "Y = X_prior[[flat_obs_index], :]\n",
    "\n",
    "# gmour 2025.07.24\n",
    "# --- Construct 3D Localization `rho` ---\n",
    "localization_radius_x = 2.0\n",
    "localization_radius_y = 3.0\n",
    "localization_radius_z = 5.0\n",
    "\n",
    "# Create 3D coordinate grids\n",
    "zz, yy, xx = np.meshgrid(np.arange(Nz), np.arange(Ny), np.arange(Nx), indexing='ij')\n",
    "\n",
    "# gmour 2025.07.24\n",
    "# Calculate 3D Euclidean distance from every point to the observation\n",
    "distances_3d = np.sqrt(((xx - x_obs)/localization_radius_x)**2 + ((yy - y_obs)/localization_radius_y)**2 + ((zz - z_obs)/localization_radius_z)**2)\n",
    "distances = distances_3d.flatten()\n",
    "\n",
    "rho = np.exp(-0.5 * distances ** 2).reshape(-1, 1)\n",
    "\n",
    "# --- Run Assimilations ---\n",
    "# This part remains the same as the assimilation logic is dimension-agnostic\n",
    "esmda_distance = DistanceESMDA(\n",
    "    covariance=C_D, observations=true_observations, alpha=alpha_i, seed=rng\n",
    ")\n",
    "X_posterior = esmda_distance.assimilate(X=X_prior, Y=Y, rho=rho)\n",
    "\n",
    "esmda = ESMDA(\n",
    "    covariance=C_D, observations=true_observations, alpha=alpha_i, seed=rng\n",
    ")\n",
    "X_posterior_global = esmda.assimilate(X=X_prior, Y=Y)\n",
    "\n",
    "# --- Reshape Data and Prepare for 3D Visualization ---\n",
    "prior_mean = np.mean(X_prior, axis=1)\n",
    "posterior_mean = np.mean(X_posterior, axis=1)\n",
    "\n",
    "# Reshape the 1D vectors into 3D volumes\n",
    "rho_3d = rho.reshape((Nz, Ny, Nx))\n",
    "update_localized_3d = (posterior_mean - prior_mean).reshape((Nz, Ny, Nx))\n",
    "update_global_3d = (np.mean(X_posterior_global, axis=1) - prior_mean).reshape((Nz, Ny, Nx))\n",
    "\n",
    "# --- Create 3D Cross-Sectional Plots ---\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 9))\n",
    "fig.suptitle('3D Cross-Sectional Views of Localization and Updates', fontsize=16)\n",
    "\n",
    "# --- Row 1: Slices of the Localization Function (rho) ---\n",
    "im_rho = axes[0, 0].imshow(rho_3d[z_obs, :, :], cmap='viridis', vmin=0, vmax=1)\n",
    "axes[0, 0].set_title(f'Rho - XY Slice at z={z_obs}')\n",
    "\n",
    "axes[0, 1].imshow(rho_3d[:, y_obs, :], cmap='viridis', vmin=0, vmax=1)\n",
    "axes[0, 1].set_title(f'Rho - XZ Slice at y={y_obs}')\n",
    "\n",
    "axes[0, 2].imshow(rho_3d[:, :, x_obs], cmap='viridis', vmin=0, vmax=1)\n",
    "axes[0, 2].set_title(f'Rho - YZ Slice at x={x_obs}')\n",
    "\n",
    "fig.colorbar(im_rho, ax=axes[0, :], location='right', shrink=0.8, label='Localization Weight')\n",
    "\n",
    "# --- Slices of the Update Fields ---\n",
    "# Find a single, symmetric color scale for both update plots\n",
    "update_max_abs = np.max([np.abs(update_localized_3d), np.abs(update_global_3d)])\n",
    "vmin_update, vmax_update = -update_max_abs, update_max_abs\n",
    "\n",
    "# Localized Update\n",
    "im_update = axes[1, 0].imshow(update_localized_3d[z_obs, :, :], cmap='coolwarm', vmin=vmin_update, vmax=vmax_update)\n",
    "axes[1, 0].set_title(f'Localized Update - XY Slice')\n",
    "\n",
    "axes[1, 1].imshow(update_localized_3d[:, y_obs, :], cmap='coolwarm', vmin=vmin_update, vmax=vmax_update)\n",
    "axes[1, 1].set_title(f'Localized Update - XZ Slice')\n",
    "\n",
    "axes[1, 2].imshow(update_localized_3d[:, :, x_obs], cmap='coolwarm', vmin=vmin_update, vmax=vmax_update)\n",
    "axes[1, 2].set_title(f'Localized Update - YZ Slice')\n",
    "\n",
    "# --- Add Annotations to All Plots ---\n",
    "# Add crosshairs to pinpoint the observation location in each slice\n",
    "axes[0, 0].axhline(y_obs, color='r', linestyle=':', lw=1); axes[0, 0].axvline(x_obs, color='r', linestyle=':', lw=1)\n",
    "axes[0, 1].axhline(z_obs, color='r', linestyle=':', lw=1); axes[0, 1].axvline(x_obs, color='r', linestyle=':', lw=1)\n",
    "axes[0, 2].axhline(z_obs, color='r', linestyle=':', lw=1); axes[0, 2].axvline(y_obs, color='r', linestyle=':', lw=1)\n",
    "\n",
    "axes[1, 0].axhline(y_obs, color='k', linestyle=':', lw=1); axes[1, 0].axvline(x_obs, color='k', linestyle=':', lw=1)\n",
    "axes[1, 1].axhline(z_obs, color='k', linestyle=':', lw=1); axes[1, 1].axvline(x_obs, color='k', linestyle=':', lw=1)\n",
    "axes[1, 2].axhline(z_obs, color='k', linestyle=':', lw=1); axes[1, 2].axvline(y_obs, color='k', linestyle=':', lw=1)\n",
    "\n",
    "# Hide axis ticks for a cleaner look\n",
    "for ax in axes.flat:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef00a8e-0921-43ac-8d35-10d6feda8f22",
   "metadata": {},
   "source": [
    "# Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17ed7c6-701a-468b-a5ad-ddf2d032dad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing large-scale performance...\n",
      "==================================================\n",
      "\n",
      "Test Case 1: N_params=40,000, N_wells=5, Obs_per_well=2000, Total_obs=10000\n",
      "Grid: 200x200, Wells: 5, Obs/well: 2000\n",
      "  Trial 1/1...\n",
      "    Generating ensemble...\n",
      "    Selecting well locations and observations...\n",
      "    Creating localization matrix...\n",
      "    Running assimilation...\n",
      "    Time: 12.35s\n",
      "    Creating visualization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b332aa59ba6748cea5573a5f765d8527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='slice_index', max=9999), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Average time: 12.35 ± 0.00s\n",
      "  Time per parameter: 308.81 μs/param\n",
      "  Time per well: 2470.47 ms/well\n",
      "  Time per observation: 1.24 ms/obs\n",
      "Function time_large_scale_performance:\n",
      "  Total time: 12.59s\n",
      "\n",
      "============================================================\n",
      "LARGE-SCALE PERFORMANCE ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Case 1: 40,000 params, 5 wells, 2000 obs/well\n",
      "  Total obs: 10000\n",
      "  Total time: 12.35 ± 0.00 seconds\n",
      "  Throughput: 3238 params/second\n",
      "  Time per param: 308.81 μs\n",
      "  Time per well: 2470.47 ms\n",
      "  Time per obs: 1.24 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "from iterative_ensemble_smoother.experimental import DistanceESMDA\n",
    "\n",
    "def gaspari_cohn_localization(distances, radius):\n",
    "    \"\"\"Gaspari-Cohn localization with compact support at 2*radius\"\"\"\n",
    "    r = distances / radius\n",
    "    rho = np.zeros_like(r)\n",
    "    \n",
    "    # Compact support: zero beyond 2*radius\n",
    "    mask1 = r <= 1\n",
    "    mask2 = (r > 1) & (r <= 2)\n",
    "    \n",
    "    # Gaspari-Cohn function with smooth derivatives\n",
    "    rho[mask1] = 1 - (5/3)*r[mask1]**2 + (5/8)*r[mask1]**3 + (1/2)*r[mask1]**4 - (1/4)*r[mask1]**5\n",
    "    rho[mask2] = 4 - 5*r[mask2] + (5/3)*r[mask2]**2 + (5/8)*r[mask2]**3 - (1/2)*r[mask2]**4 + (1/12)*r[mask2]**5 - 2/(3*r[mask2])\n",
    "    \n",
    "    return rho\n",
    "\n",
    "def create_multi_well_localization(well_locations, obs_per_well, grid_size, radii_azimuth):\n",
    "    \"\"\"Create localization matrix where all observations at same well share localization\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    well_locations : array_like\n",
    "        Indices of well locations\n",
    "    obs_per_well : int\n",
    "        Number of observations per well\n",
    "    grid_size : int\n",
    "        Size of square grid (grid_size x grid_size)\n",
    "    radius : float\n",
    "        Localization radius (function has compact support at 2*radius)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    rho : ndarray\n",
    "        Localization matrix of shape (n_params, n_obs)\n",
    "    \"\"\"\n",
    "    \n",
    "    n_wells = len(well_locations)\n",
    "    n_obs = n_wells * obs_per_well\n",
    "    n_params = grid_size * grid_size\n",
    "    \n",
    "    # Create coordinate grids\n",
    "    xx, yy = np.meshgrid(np.arange(grid_size), np.arange(grid_size))\n",
    "\n",
    "    xx_flat = xx.flatten()\n",
    "    yy_flat = yy.flatten()\n",
    "\n",
    "    rho = np.zeros((n_params, n_obs))\n",
    "    \n",
    "    for current_index, well_idx in enumerate(well_locations):\n",
    "        # Convert 1D grid index to 2D coordinates\n",
    "        # For a flattened grid: index = y * grid_size + x\n",
    "        # So: x = index % grid_size, y = index // grid_size\n",
    "        well_x = well_idx % grid_size\n",
    "        well_y = well_idx // grid_size\n",
    "\n",
    "        # Perform a simple 2D rotation. Equations available in:\n",
    "        # Emerick, A., & Reynolds, A. (2011). Combining sensitivities and prior information for covariance localization \n",
    "        # in the ensemble Kalman filter for petroleum reservoir applications. Computational Geosciences, 15(2), 251-269.\n",
    "        # Equations #18 and #19\n",
    "\n",
    "        rotated_x_dist = ((xx_flat - well_x) * np.cos(radii_azimuth[2]*np.pi/180) + \n",
    "                          (yy_flat - well_y) * np.sin(radii_azimuth[2]*np.pi/180))\n",
    "        rotated_y_dist = (-(xx_flat - well_x) * np.sin(radii_azimuth[2]*np.pi/180) + \n",
    "                          (yy_flat - well_y) * np.cos(radii_azimuth[2]*np.pi/180))\n",
    "        \n",
    "        distances = np.sqrt((rotated_x_dist / radii_azimuth[0])**2 + \n",
    "                            (rotated_y_dist / radii_azimuth[1])**2)\n",
    "\n",
    "        rho_well = gaspari_cohn_localization(distances, 1)\n",
    "\n",
    "        # Populate rho by replicating rho_well obs_per_well times:\n",
    "        rho[: , current_index * obs_per_well : (current_index + 1) * obs_per_well] = np.tile(\n",
    "            rho_well.reshape(-1, 1), (1, obs_per_well))\n",
    "    \n",
    "    return rho\n",
    "\n",
    "def visualize_localization_effect(X_prior, X_posterior, well_locations, rho, grid_size, case_name=\"\"):\n",
    "    \"\"\"Visualize the localization function and its effect on the update\"\"\"\n",
    "    \n",
    "    # Calculate ensemble means\n",
    "    prior_mean = np.mean(X_prior, axis=1)\n",
    "    posterior_mean = np.mean(X_posterior, axis=1)\n",
    "    \n",
    "    # Reshape to 2D grids\n",
    "    prior_mean_2d = prior_mean.reshape((grid_size, grid_size))\n",
    "    posterior_mean_2d = posterior_mean.reshape((grid_size, grid_size))\n",
    "    rho_2d = rho[:, 0].reshape((grid_size, grid_size))  # Use first column since all are the same\n",
    "    \n",
    "    # Calculate update\n",
    "    update_2d = posterior_mean_2d - prior_mean_2d\n",
    "    \n",
    "    # Convert well locations to 2D coordinates\n",
    "    well_coords = [(idx % grid_size, idx // grid_size) for idx in well_locations]\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "    # Plot 1: Localization function\n",
    "    im1 = axes[0].imshow(rho_2d, cmap='viridis', origin='lower')\n",
    "    axes[0].set_title('Localization Function (rho)')\n",
    "    axes[0].set_xlabel('X coordinate')\n",
    "    axes[0].set_ylabel('Y coordinate')\n",
    "    plt.colorbar(im1, ax=axes[0])\n",
    "    \n",
    "    # Mark well locations\n",
    "    for well_x, well_y in well_coords:\n",
    "        axes[0].plot(well_x, well_y, 'r+', markersize=15, markeredgewidth=3)\n",
    "    \n",
    "    # Plot 2: Localized update\n",
    "    update_max = max(abs(update_2d.min()), abs(update_2d.max()))\n",
    "    im2 = axes[1].imshow(update_2d, cmap='RdBu_r', origin='lower', \n",
    "                         vmin=-update_max, vmax=update_max)\n",
    "    axes[1].set_title('Localized Update')\n",
    "    axes[1].set_xlabel('X coordinate')\n",
    "    axes[1].set_ylabel('Y coordinate')\n",
    "    plt.colorbar(im2, ax=axes[1])\n",
    "    \n",
    "    # Mark well locations\n",
    "    for well_x, well_y in well_coords:\n",
    "        axes[1].plot(well_x, well_y, 'k+', markersize=15, markeredgewidth=3)\n",
    "    \n",
    "    plt.suptitle(f'Localization Effect {case_name}', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print some statistics\n",
    "    print(f\"    Localization statistics:\")\n",
    "    print(f\"      Sparsity: {np.mean(rho_2d == 0) * 100:.1f}% of parameters have zero localization\")\n",
    "    print(f\"      Max localization: {rho_2d.max():.3f}\")\n",
    "    print(f\"      Update range: [{update_2d.min():.4f}, {update_2d.max():.4f}]\")\n",
    "    print(f\"      Update std: {update_2d.std():.4f}\")\n",
    "    print(f\"      Rho shape: {rho.shape}\")\n",
    "\n",
    "def visualize_localization_effect_interactive(X_prior, X_posterior, well_locations, rho, grid_size, case_name=\"\"):\n",
    "    \"\"\"Interactive visualization of the localization function and its effect on the update\"\"\"\n",
    "\n",
    "    # Calculate ensemble means\n",
    "    prior_mean = np.mean(X_prior, axis=1)\n",
    "    posterior_mean = np.mean(X_posterior, axis=1)\n",
    "\n",
    "    # Reshape to 2D grids\n",
    "    prior_mean_2d = prior_mean.reshape((grid_size, grid_size))\n",
    "    posterior_mean_2d = posterior_mean.reshape((grid_size, grid_size))\n",
    "\n",
    "    # Calculate update\n",
    "    update_2d = posterior_mean_2d - prior_mean_2d\n",
    "\n",
    "    # Convert well locations to 2D coordinates\n",
    "    well_coords = [(idx % grid_size, idx // grid_size) for idx in well_locations]\n",
    "\n",
    "    def plot_slice(slice_index):\n",
    "        rho_2d = rho[:, slice_index].reshape((grid_size, grid_size))\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "        # Plot 1: Localization function\n",
    "        im1 = axes[0].imshow(rho_2d, cmap='viridis', origin='lower')\n",
    "        axes[0].set_title(f'Localization Function (rho[:, {slice_index}])')\n",
    "        axes[0].set_xlabel('X coordinate')\n",
    "        axes[0].set_ylabel('Y coordinate')\n",
    "        plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "        for well_x, well_y in well_coords:\n",
    "            axes[0].plot(well_x, well_y, 'r+', markersize=15, markeredgewidth=3)\n",
    "\n",
    "        # Plot 2: Localized update\n",
    "        update_max = max(abs(update_2d.min()), abs(update_2d.max()))\n",
    "        im2 = axes[1].imshow(update_2d, cmap='RdBu_r', origin='lower',\n",
    "                             vmin=-update_max, vmax=update_max)\n",
    "        axes[1].set_title('Localized Update')\n",
    "        axes[1].set_xlabel('X coordinate')\n",
    "        axes[1].set_ylabel('Y coordinate')\n",
    "        plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "        for well_x, well_y in well_coords:\n",
    "            axes[1].plot(well_x, well_y, 'k+', markersize=15, markeredgewidth=3)\n",
    "\n",
    "        plt.suptitle(f'Localization Effect {case_name}', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Print statistics\n",
    "        print(f\"    Localization statistics for slice {slice_index}:\")\n",
    "        print(f\"      Sparsity: {np.mean(rho_2d == 0) * 100:.1f}% of parameters have zero localization\")\n",
    "        print(f\"      Max localization: {rho_2d.max():.3f}\")\n",
    "        print(f\"      Update range: [{update_2d.min():.4f}, {update_2d.max():.4f}]\")\n",
    "        print(f\"      Update std: {update_2d.std():.4f}\")\n",
    "        print(f\"      Rho shape: {rho.shape}\")\n",
    "\n",
    "    # Create interactive slider\n",
    "    interact(plot_slice, slice_index=IntSlider(min=0, max=rho.shape[1]-1, step=1, value=0))\n",
    "\n",
    "\n",
    "def time_large_scale_performance():\n",
    "    \"\"\"Test performance on large-scale problems\"\"\"\n",
    "    \n",
    "    # Test cases: (n_params, n_wells, obs_per_well)\n",
    "    test_cases = [\n",
    "        #(200*200, 40000, 1),\n",
    "        #(115*115, 115*115, 1),\n",
    "        #(418*418, 115*115, 1),\n",
    "        #(660*660, 115*115, 1),\n",
    "        (933*933, 60*60,1),\n",
    "        #(100*100, 5, 1000),\n",
    "        #(1000*1000, 50, 20),\n",
    "        #(1000*1000, 100, 10),\n",
    "    ]\n",
    "\n",
    "    \n",
    "    n_ensemble = 100\n",
    "    n_trials = 1  # Might want to reduce this for very large problems\n",
    "                                # main, normal, azimuth\n",
    "    localization_radii_azimuth = [50, 30, 45]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(\"Testing large-scale performance...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, (n_params, n_wells, obs_per_well) in enumerate(test_cases):\n",
    "        n_obs = n_wells * obs_per_well\n",
    "        print(f\"\\nTest Case {i+1}: N_params={n_params:,}, N_wells={n_wells}, Obs_per_well={obs_per_well}, Total_obs={n_obs}\")\n",
    "        \n",
    "        grid_size = int(np.sqrt(n_params))\n",
    "        print(f\"Grid: {grid_size}x{grid_size}, Wells: {n_wells}, Obs/well: {obs_per_well}\")\n",
    "        \n",
    "        # Verify it's a perfect square\n",
    "        assert grid_size * grid_size == n_params, f\"Not a perfect square: {n_params}\"\n",
    "        \n",
    "        trial_times = []\n",
    "        \n",
    "        for trial in range(n_trials):\n",
    "            print(f\"  Trial {trial + 1}/{n_trials}...\")\n",
    "            \n",
    "            # Setup\n",
    "            #rng = np.random.default_rng(42 + trial)\n",
    "            rng = np.random.default_rng()\n",
    "            \n",
    "            # Generate ensemble\n",
    "            print(\"    Generating ensemble...\")\n",
    "            X_prior = rng.normal(0, 0.5, size=(n_params, n_ensemble))\n",
    "            \n",
    "            # Generate well locations and observations\n",
    "            print(\"    Selecting well locations and observations...\")\n",
    "            \n",
    "            # First, select well locations (unique spatial positions)\n",
    "            well_locations = rng.choice(n_params, size=n_wells, replace=False)\n",
    "            \n",
    "            # Then, create multiple observations per well\n",
    "            # For simplicity, all observations at a well have the same location\n",
    "            # In reality, they might be slightly offset or represent different variables\n",
    "            obs_indices = []\n",
    "            for well_loc in well_locations:\n",
    "                for _ in range(obs_per_well):\n",
    "                    obs_indices.append(well_loc)\n",
    "\n",
    "            # Generate observations by sampling the forward model at well locations\n",
    "            # Note: obs_indices can be longer than n_params since we replicate well locations\n",
    "            # for multiple observations per well. NumPy allows repeated indices, so\n",
    "            # X_prior[obs_indices, :] creates multiple copies of the same model values.\n",
    "            obs_indices = np.array(obs_indices)\n",
    "            # Using identity model Y = g(X) = X\n",
    "            Y = X_prior[obs_indices, :]\n",
    "            \n",
    "            # Start timing the critical section\n",
    "            start_time = time.perf_counter()\n",
    "            \n",
    "            print(\"    Creating localization matrix...\")\n",
    "            \n",
    "            # Create localization matrix based on well locations\n",
    "            # All observations at same well share same localization\n",
    "            rho = create_multi_well_localization(\n",
    "                well_locations, obs_per_well, grid_size, localization_radii_azimuth\n",
    "            )\n",
    "\n",
    "            print(\"    Running assimilation...\")\n",
    "            \n",
    "            # Run assimilation with all observations\n",
    "            esmda = DistanceESMDA(\n",
    "                covariance=np.eye(n_obs) * 0.01,\n",
    "                observations=np.ones(n_obs),\n",
    "                alpha=1,\n",
    "                seed=rng\n",
    "            )\n",
    "\n",
    "            X_posterior = esmda.assimilate(X=X_prior, Y=Y, rho=rho)\n",
    "            \n",
    "            end_time = time.perf_counter()\n",
    "            \n",
    "            total_time = end_time - start_time\n",
    "            trial_times.append(total_time)\n",
    "            \n",
    "            print(f\"    Time: {total_time:.2f}s\")\n",
    "            \n",
    "            # Visualize the localization effect\n",
    "            print(\"    Creating visualization...\")\n",
    "            visualize_localization_effect_interactive(\n",
    "                X_prior, X_posterior, well_locations, rho, grid_size, \n",
    "                case_name=f\"(Case {i+1}: {n_wells} wells, {obs_per_well} obs/well, radii and azimuth={localization_radii_azimuth})\"\n",
    "            )\n",
    "            \n",
    "            # Clean up large arrays to free memory\n",
    "            del X_prior, X_posterior, Y, rho\n",
    "        \n",
    "        avg_time = np.mean(trial_times)\n",
    "        std_time = np.std(trial_times)\n",
    "        \n",
    "        result = {\n",
    "            'n_params': n_params,\n",
    "            'n_wells': n_wells,\n",
    "            'obs_per_well': obs_per_well,\n",
    "            'n_obs': n_obs,\n",
    "            'avg_time': avg_time,\n",
    "            'std_time': std_time,\n",
    "            'trial_times': trial_times\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"  Average time: {avg_time:.2f} ± {std_time:.2f}s\")\n",
    "        print(f\"  Time per parameter: {avg_time/n_params*1e6:.2f} μs/param\")\n",
    "        print(f\"  Time per well: {avg_time/n_wells*1e3:.2f} ms/well\")\n",
    "        print(f\"  Time per observation: {avg_time/n_obs*1e3:.2f} ms/obs\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_large_scale_results(results):\n",
    "    \"\"\"Analyze and visualize large-scale results\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"LARGE-SCALE PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"\\nCase {i+1}: {result['n_params']:,} params, {result['n_wells']} wells, {result['obs_per_well']} obs/well\")\n",
    "        print(f\"  Total obs: {result['n_obs']}\")\n",
    "        print(f\"  Total time: {result['avg_time']:.2f} ± {result['std_time']:.2f} seconds\")\n",
    "        print(f\"  Throughput: {result['n_params']/result['avg_time']:.0f} params/second\")\n",
    "        print(f\"  Time per param: {result['avg_time']/result['n_params']*1e6:.2f} μs\")\n",
    "        print(f\"  Time per well: {result['avg_time']/result['n_wells']*1e3:.2f} ms\")\n",
    "        print(f\"  Time per obs: {result['avg_time']/result['n_obs']*1e3:.2f} ms\")\n",
    "\n",
    "# Performance monitoring decorator (simplified)\n",
    "def monitor_performance(func):\n",
    "    \"\"\"Decorator to monitor time only\"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        result = func(*args, **kwargs)\n",
    "        \n",
    "        end_time = time.perf_counter()\n",
    "        \n",
    "        print(f\"Function {func.__name__}:\")\n",
    "        print(f\"  Total time: {end_time - start_time:.2f}s\")\n",
    "        \n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "# Modified main execution\n",
    "if __name__ == \"__main__\":\n",
    "    time_large_scale_performance = monitor_performance(time_large_scale_performance)\n",
    "    \n",
    "    results = time_large_scale_performance()\n",
    "    analyze_large_scale_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b844a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D Case new\n",
    "\n",
    "import time\n",
    "\n",
    "# This test uses a forward model that is an aritmetic average\n",
    "# of the field parameters into a coarse scale grid\n",
    "def simulate_realizations(nreal:int,\n",
    "        variogram,\n",
    "        nx: int, ny:int, nz: int,\n",
    "        xinc: float, yinc: float, zinc: float,\n",
    "        start_seed: int = 123456789\n",
    "    )->np.ndarray:\n",
    "    \"\"\"\n",
    "    Draw realizations of the 3D gaussian fields using specified variogram\n",
    "    Output has dimension (nparam, nreal) where nparam = nx*ny*nz\n",
    "    The flatten parameter vector is 'F'-ordered where (i,j,k) corresponds to index = i + j*nx + k*nx*ny\n",
    "    \"\"\"\n",
    "    sim.seed(start_seed)\n",
    "    fields = np.zeros((nx*ny*nz, nreal), dtype=np.float32)\n",
    "    for n in range(nreal):\n",
    "        if n % 10 == 0:\n",
    "            print(f\"Simulate realization: {n}\")\n",
    "        # Order is 'F' as output here\n",
    "        field_real_flatten = sim.simulate(variogram,nx,xinc,ny,yinc,nz,zinc)\n",
    "        fields[:,n] = field_real_flatten\n",
    "    return fields\n",
    "\n",
    "def forward_model_real(field3D:np.ndarray, nx: int, ny: int, nz:int, kx: int, ky:int, kz:int)->np.ndarray:\n",
    "    \"\"\"\n",
    "    Simple linear forward model (arithmetic average of field values within each coarse grid cell.\n",
    "    Calculate average of coarse grid blocks of size (mx,my,mz).\n",
    "    nx = mx*kx,   ny = my *ky , nz = mz * kz      where (kx, ky,kz)  is grid size of upscaled grid\n",
    "    \n",
    "    \"\"\"\n",
    "    mx = int(nx/kx)\n",
    "    my = int(ny/ky)\n",
    "    mz = int(nz/kz)\n",
    "    upscaled = np.zeros((kx,ky,kz), order = \"F\", dtype=np.float32)\n",
    "    for kk in range(kz):\n",
    "        zstart = kk * mz\n",
    "        zend = zstart + mz \n",
    "        for jj in range(ky):\n",
    "            ystart = jj * my\n",
    "            yend = ystart + my\n",
    "            for ii in range(kx):\n",
    "                xstart = ii * mx\n",
    "                xend = xstart + mx\n",
    "                selected = field3D[xstart:xend, ystart:yend, zstart:zend]\n",
    "                avg = np.mean(selected)\n",
    "                upscaled[ii,jj,kk] = avg\n",
    "    return upscaled\n",
    "\n",
    "def forward_model(fields:np.ndarray, nreal:int, nx:int, ny: int, nz: int, kx:int, ky:int, kz:int)->np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply the forward model to all realizations.\n",
    "    The response per realization (the upscaled values) are saved in flatten array.\n",
    "    Keep same index order ('F') for coarse grid values as for the input field.\n",
    "    \"\"\"\n",
    "    upscaled_fields = np.zeros((kx*ky*kz,nreal), dtype=np.float32)\n",
    "    for n in range(nreal):\n",
    "        field = fields[:,n]\n",
    "        field3D = field.reshape((nx, ny, nz), order=\"F\")\n",
    "        upscaled3D = forward_model_real(field3D, nx, ny, nz, kx, ky, kz)\n",
    "        upscaled_fields[:,n] = upscaled3D.flatten(order=\"F\")\n",
    "    return upscaled_fields\n",
    "\n",
    "\n",
    "def distance_from_grid_cells_to_obs_position(X_global, Y_global, x_obs, y_obs):\n",
    "    # A mesh containing difference vector (dx, dy) between grid cell center and obs position (x,y)\n",
    "    dX = X_global - x_obs\n",
    "    dY = Y_global - y_obs\n",
    "    return dX, dY\n",
    "\n",
    "def gaussian_decay(D):\n",
    "    return np.exp(-3*D*D)\n",
    "\n",
    "def exponential_decay(D):\n",
    "    return np.exp(-3*D)\n",
    "\n",
    "def field_parameter_grid_cell_center_xy_coordinates(\n",
    "    xorigo:float, yorigo:float,\n",
    "    xinc:float, yinc:float, rotation:float,\n",
    "    nx:int, ny:int):\n",
    "    \"\"\"\n",
    "    Calculate global x and y coordinates grid cell center points of field parameter grid as mesh.\n",
    "    Input:\n",
    "    Field parameter grid has a global rotation point, typically this is lower left corner point\n",
    "    if the grid is unrotated. Rotation is anticlockwise in degrees. Only the lateral position of\n",
    "    the grid cell center points are of interest for distance based localization.\n",
    "    Output:\n",
    "    Numpy mesh for x and y location of cell center points in global coordinates\n",
    "    \"\"\"\n",
    "    x_local = np.arange(0.5*xinc, xinc*(nx+0.49),xinc, dtype=np.float32)\n",
    "    y_local = np.arange(0.5*yinc, yinc*(ny+0.49),yinc, dtype=np.float32)\n",
    "\n",
    "    cosangle = math.cos(rotation*np.pi/180.0)\n",
    "    sinangle = math.sin(rotation*np.pi/180.0)\n",
    "\n",
    "    # (x,y) in local coordinate system following simulation box\n",
    "    yy, xx = np.meshgrid(y_local, x_local, indexing=\"ij\")\n",
    "\n",
    "    # Transform to global coordinate system\n",
    "    x_global = xx * cosangle - yy * sinangle  + xorigo\n",
    "    y_global = xx * sinangle + yy * cosangle  + yorigo\n",
    "    return x_global, y_global\n",
    "\n",
    "def calculate_scaling_factor_for_one_layer(\n",
    "    obs_xpos:float, obs_ypos:float,\n",
    "    grid_x_coord:np.ndarray, grid_y_coord:np.ndarray,\n",
    "    main_range:float, perp_range:float, ellipse_rotation:float, \n",
    "    scaling_function, cutoff_value:float = 0.001\n",
    ")->np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate scaling factors for all pairs of field parameters and one observation point\n",
    "    of a layer in a 3D field parameter grid (box grid).\n",
    "    Input:\n",
    "    numpy mesh for x and y coordinates of cell centers of field parameter grid position\n",
    "    Position of observation must be in the same coordinates.\n",
    "    localization parameters for distance and rotation of elliptic influence area.\n",
    "    Rotation of ellipse is relative to the coordinate system used.\n",
    "    If used coordinate system is local (simulation box grid), the rotation angle is relative\n",
    "    to the local coordinate x-axis. If coordinate system is global (e.g.UTM), the rotation\n",
    "    angle is relative to the west-east x-axis.\n",
    "    Output:\n",
    "    scaling factors for all field parameters for one grid layer of field parameters\n",
    "    around observation point given.\n",
    "    \"\"\"\n",
    "    dX = grid_x_coord -obs_xpos\n",
    "    dY = grid_y_coord -obs_ypos\n",
    "    rotation = ellipse_rotation*np.pi/180.0\n",
    "    cosangle = math.cos(rotation)\n",
    "    sinangle = math.sin(rotation)\n",
    "#    print(f\"shape of dX: {dX.shape} \")\n",
    "#    print(f\"shape of dY: {dY.shape} \")\n",
    "    dX_ellipse = (dX * cosangle + dY * sinangle) / main_range\n",
    "    dY_ellipse = (-dX * sinangle + dY * cosangle) / perp_range\n",
    "    distances_2d = np.sqrt(dX_ellipse * dX_ellipse  + dY_ellipse * dY_ellipse)\n",
    "    distances = distances_2d.flatten()\n",
    "    scaling_factors = scaling_function(distances)\n",
    "\n",
    "    # Apply cutoff\n",
    "    scaling_factors[scaling_factors < cutoff_value] = 0.0 \n",
    "#    print(f\"Shape of scaling_factor: {scaling_factor_all.shape} \")\n",
    "    return scaling_factors\n",
    "\n",
    "def calculate_rho_for_2d_obs_field(\n",
    "    grid_xlength:float, grid_ylength:float,\n",
    "    param_grid_nx:int, param_grid_ny:int,\n",
    "    obs_grid_nx:int, obs_grid_ny:int,\n",
    "    local_main_range:int, local_perp_range:int, local_rotation:float,\n",
    "    tapering_function_name:str, cutoff_value:float = 0.0)->np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate localization scaling factor pairs of field parameters and observations.\n",
    "    Input:\n",
    "    The field parameter is specified on a box grid with origo in global coordinates\n",
    "    and grid orientation relative to global coordinates (e.g UTM). Rotation angle is\n",
    "    positive anti-clockwise and in degrees. The size of the rectangular box is specified.\n",
    "    The observations are assumed to be located on a 2D grid oriented and located and with\n",
    "    position as the field parameter grid and with same size.\n",
    "    The grid resolution (increments) of the 2D grid with observations can be different\n",
    "    from the field parameter grid.\n",
    "    The specified localization ranges define the influence range of the localization and\n",
    "    are parameters used by the specified localization function.\n",
    "    Output:\n",
    "    Update the matrix rho corresponding to one layer of the 3D field parameter grid\n",
    "    with scaling factors for each pair of field parameters and observations.\n",
    "    \"\"\"\n",
    "\n",
    "    #def calculate_scaling_factor_for_one_layer(\n",
    "    #obs_xpos:float, obs_ypos:float,\n",
    "    #grid_x_coord:np.ndarray, grid_y_coord:np.ndarray,\n",
    "    #main_range:float, perp_range:float, ellipse_rotation:float, cutoff_value:float,\n",
    "    #scaling_function\n",
    "    nparam_one_layer = param_grid_nx * param_grid_ny\n",
    "    nobs = obs_grid_nx * obs_grid_ny\n",
    "    if param_grid_nx == obs_grid_nx and param_grid_ny == obs_grid_ny:\n",
    "        if local_main_range == 0.0 or local_perp_range == 0.0:\n",
    "            # localization with 0 influence range means that only the observation\n",
    "            # located in same position as the field parameter contributes to the\n",
    "            # update of the field parameter. (Co-located field parameter and observation)\n",
    "            print(f\"Field parameters (x,y) position and (i,j) indices are co-located with the observation field\")\n",
    "            rho_for_one_layer = np.identity(nparam_one_layer,dtype=np.float32)\n",
    "            return rho_for_one_layer\n",
    "    # TODO:  Can maybe speed up this\n",
    "    xinc = grid_xlength/param_grid_nx\n",
    "    yinc = grid_ylength/param_grid_ny\n",
    "    xinc_obs = grid_xlength/obs_grid_nx\n",
    "    yinc_obs = grid_ylength/obs_grid_ny\n",
    "\n",
    "    x_local = np.arange(param_grid_nx)\n",
    "    x_local = (x_local + 0.5) * xinc\n",
    "    y_local = np.arange(param_grid_ny)\n",
    "    y_local = (y_local + 0.5) * yinc\n",
    "    grid_y_coord, grid_x_coord = np.meshgrid(y_local, x_local, indexing=\"ij\")\n",
    "\n",
    "    if tapering_function_name == \"exponential\":\n",
    "        scaling_function = exponential_decay\n",
    "    elif tapering_function_name == \"gaussian\":\n",
    "        scaling_function = gaussian_decay\n",
    "    \n",
    "    # Position relative to the local coordinate system (simbox coordinate system)\n",
    "    rho_for_one_layer = np.zeros((nparam_one_layer, nobs))\n",
    "    for j in range(obs_grid_ny):\n",
    "        y_pos_obs = (j + 0.5) * yinc_obs\n",
    "        for i in range(obs_grid_nx):\n",
    "            obs_nr = i + j * obs_grid_nx\n",
    "            x_pos_obs = (i + 0.5) * xinc_obs\n",
    "            rho_for_one_layer[:,obs_nr] = calculate_scaling_factor_for_one_layer(\n",
    "                x_pos_obs, y_pos_obs,\n",
    "                grid_x_coord, grid_y_coord,\n",
    "                local_main_range, local_perp_range, local_rotation, \n",
    "                scaling_function, cutoff_value=cutoff_value)\n",
    "\n",
    "    return rho_for_one_layer\n",
    "\n",
    "def define_example_observation_field(\n",
    "    grid_xlength:float, grid_ylength:float, \n",
    "    obs_grid_nx:int, obs_grid_ny:int,\n",
    ")->np.ndarray:\n",
    "    # Define observation values for the observation grid\n",
    "    nobs = obs_grid_nx * obs_grid_ny\n",
    "    obs_xinc = grid_xlength / obs_grid_nx\n",
    "    obs_yinc = grid_ylength / obs_grid_ny\n",
    "    print(f\"Observation distance in x direction: {obs_xinc}\")\n",
    "    print(f\"Observation distance in y direction: {obs_yinc}\")\n",
    "    print(f\"obs_grid_nx: {obs_grid_nx}\")\n",
    "    print(f\"obs_grid_ny: {obs_grid_ny}\")\n",
    "\n",
    "    true_observations = np.zeros(nobs, dtype=np.float32)\n",
    "    obs_value_inc_i = 0.45/(obs_grid_nx - 1)\n",
    "    obs_value_inc_j = 0.45/(obs_grid_ny - 1)\n",
    "    print(f\"Define observation values (Use a spatial trend). Number of obs = {nobs}\")\n",
    "    obs_nr = 0\n",
    "    for j in range(obs_grid_ny):\n",
    "        for i in range(obs_grid_nx):\n",
    "            true_observations[obs_nr] = 0.1 + obs_value_inc_i * i + obs_value_inc_j * j\n",
    "            obs_nr += 1\n",
    "    return true_observations\n",
    "\n",
    "def obs_error_covariance(\n",
    "    obs_error_std:float,\n",
    "    obs_grid_nx:int, obs_grid_ny:int,\n",
    "    grid_xlength:float = 0, grid_ylength:float = 0,\n",
    "    use_obs_correlations: bool = False,\n",
    "    obs_correlation_length_x:float = 0.0,\n",
    "    obs_correlation_length_y:float = 0.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Define observation covariance matrix.\n",
    "    Return covariance matrix of observations. Default is diagonal covariance.\n",
    "    \"\"\"\n",
    "    print(f\"Use correlated observations: {use_obs_correlations}\")\n",
    "    nobs = obs_grid_nx * obs_grid_ny\n",
    "    obs_error_var = obs_error_std * obs_error_std\n",
    "    # Correlated obs\n",
    "    obs_variances = np.ones(nobs,dtype=np.float64)\n",
    "    obs_variances = obs_error_var * obs_variances\n",
    "    C_D = np.diag(obs_variances)\n",
    "    if use_obs_correlations:\n",
    "        power = 1.85\n",
    "        print(\"Define spatial correlation between observation errors.\")\n",
    "        print(f\"Obs correlation length in x = {obs_correlation_length_x}\")\n",
    "        print(f\"Obs correlation length in y = {obs_correlation_length_y}\")\n",
    "        print(f\"Obs correlation length in number of observation distances in x direction = {int(obs_correlation_length_x/obs_grid_nx)}\")\n",
    "        print(f\"Obs correlation length in number of observation distances in y direction = {int(obs_correlation_length_y/obs_grid_ny)}\")\n",
    "        print(f\"Spatial correlation defined by correlation function exp(-3*d^power)with power: {power} and d is normalized distance\")\n",
    "        xinc_obs = grid_xlength/obs_grid_nx\n",
    "        yinc_obs = grid_ylength/obs_grid_ny\n",
    "        for j1 in range(obs_grid_ny):\n",
    "            ypos1 = (j1 + 0.5) * yinc_obs\n",
    "            for i1 in range(obs_grid_nx):\n",
    "                obs_nr1 = i1 + j1 * obs_grid_nx\n",
    "                xpos1 = (i1 + 0.5) * xinc_obs\n",
    "                for j2 in range(obs_grid_ny):\n",
    "                    ypos2 = (j2 + 0.5) * yinc_obs\n",
    "                    for i2 in range(obs_grid_nx):\n",
    "                        obs_nr2 = i2 + j2 * obs_grid_nx\n",
    "                        xpos2 = (i2 + 0.5) * xinc_obs\n",
    "                        dx = xpos1 - xpos2\n",
    "                        dy = ypos1 - ypos2\n",
    "                        d = math.sqrt((dx/obs_correlation_length_y)**2 + (dy/obs_correlation_length_y)**2)\n",
    "                        corr = math.exp(-3.0*(math.pow(d,power)))\n",
    "                        C_D[obs_nr1,obs_nr2] = obs_error_var * corr\n",
    "                        C_D[obs_nr2,obs_nr1] = C_D[obs_nr1, obs_nr2]\n",
    "    return C_D\n",
    "\n",
    "def monitor_performance(func):\n",
    "    \"\"\"Decorator to monitor time only\"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        result = func(*args, **kwargs)\n",
    "        \n",
    "        end_time = time.perf_counter()\n",
    "        \n",
    "        print(f\"Function {func.__name__}:\")\n",
    "        print(f\"  Total time: {end_time - start_time:.2f}s\")\n",
    "        \n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def run_update(\n",
    "        X_prior:np.ndarray,\n",
    "        Y:np.ndarray,\n",
    "        rho_one_layer:np.ndarray,\n",
    "        nparam_total:int, nreal:int, nobs:int,\n",
    "        nbatch:int, nlayer_per_batch:int,nlayer_last_batch:int,\n",
    "        nparam_per_layer:int,\n",
    "        smoother_object,\n",
    "        use_localization:bool = True\n",
    "    ):\n",
    "    print(f\"Start running update batch by batch of field parameters with localization\")\n",
    "\n",
    "    X_posterior = np.zeros((nparam_total,nreal),dtype=np.float32)\n",
    "    for batch_nr in range(nbatch):\n",
    "        print(f\"Start batch number: {batch_nr}\")\n",
    "        print(f\" Run forward model to predict observations\")\n",
    "\n",
    "        nparam_per_batch = nparam_per_layer * nlayer_per_batch\n",
    "        rho = np.zeros((nparam_per_batch,nobs),dtype=np.float32)\n",
    "        batch_param_start = batch_nr * nparam_per_batch\n",
    "        batch_param_end = batch_param_start + nparam_per_batch\n",
    "        # Since localization only use lateral distance\n",
    "        # rho for one layer is copied into all the Nlayer_per_batch\n",
    "        for layer in range(nlayer_per_batch):\n",
    "            from_param = layer * nparam_per_layer\n",
    "            to_param = from_param + nparam_per_layer\n",
    "            rho[from_param:to_param,:] = rho_one_layer[:]\n",
    "\n",
    "        X_prior_batch = X_prior[batch_param_start:batch_param_end,:]\n",
    "        if use_localization:\n",
    "            print(f\" Assimilate using distance-based localisation\")\n",
    "            X_posterior_batch = smoother_object.assimilate(X=X_prior_batch, Y=Y, rho=rho, truncation=0.999)\n",
    "            #X_posterior_batch = esmda_distance.assimilate(X=X_prior_batch, Y=Y, rho=rho, truncation=0.999)\n",
    "            X_posterior[batch_param_start:batch_param_end,:] = X_posterior_batch\n",
    "        else:\n",
    "            print(f\" Assimilate using one iteration of ESMDA\")\n",
    "            X_posterior_batch = esmda_global.assimilate(X=X_prior_batch, Y=Y)\n",
    "            X_posterior[batch_param_start:batch_param_end,:] = X_posterior_batch\n",
    "\n",
    "    if nlayer_last_batch > 0:\n",
    "        print(f\"Start batch number: {nbatch}\")\n",
    "\n",
    "\n",
    "        nparam_last_batch = nlayer_last_batch * nparam_per_layer\n",
    "        rho = np.zeros((nparam_last_batch,nobs),dtype=np.float32)\n",
    "        batch_param_start = batch_param_end\n",
    "        batch_param_end = nparam_total\n",
    "        # Since localization only use lateral distance\n",
    "        # rho for one layer is copied into all the Nlayer_per_batch\n",
    "        for layer in range(nlayer_last_batch):\n",
    "            from_param = layer * nparam_per_layer\n",
    "            to_param = from_param + nparam_per_layer\n",
    "            rho[from_param:to_param,:] = rho_one_layer[:]\n",
    "\n",
    "        X_prior_batch = X_prior[batch_param_start:batch_param_end,:]\n",
    "        if use_localization:\n",
    "            print(f\" Assimilate using distance-based localisation\")\n",
    "            X_posterior_batch = smoother_object.assimilate(X=X_prior_batch, Y=Y, rho=rho, truncation=0.999)\n",
    "            #X_posterior_batch = esmda_distance.assimilate(X=X_prior_batch, Y=Y, rho=rho, truncation=0.999)\n",
    "            X_posterior[batch_param_start:batch_param_end,:] = X_posterior_batch\n",
    "        else:\n",
    "            print(f\" Assimilate using one iteration of ESMDA\")\n",
    "            X_posterior_batch = smoother_object.assimilate(X=X_prior_batch, Y=Y)\n",
    "            X_posterior[batch_param_start:batch_param_end,:] = X_posterior_batch\n",
    "\n",
    "    return X_posterior\n",
    "\n",
    "\n",
    "\n",
    "# --- Main --- \n",
    "import gaussianfft as sim\n",
    "import math\n",
    "Mx = 2       # Number of field parameters values in one coarse grid cell\n",
    "My = 2\n",
    "Mz = 66\n",
    "Kx = 46     # Dimensions of the 3D upscaled grid\n",
    "Ky = 73\n",
    "Kz = 1\n",
    "Nx = Mx * Kx  # Dimensions of the 3D parameter grid\n",
    "Ny = My * Ky\n",
    "Nz = Mz * Kz\n",
    "Nlayer_per_batch = 66\n",
    "Nbatch = int(Nz/Nlayer_per_batch)\n",
    "Nlayer_last_batch = Nz - Nlayer_per_batch * Nbatch\n",
    "N_m = Nx*Ny*Nz\n",
    "Xinc = 75.0   # Field parameter grid cell size\n",
    "Yinc = 75.0\n",
    "Zinc = 1.0\n",
    "Xinc_upscaled = Xinc * Mx   # Coarse scale grid cell size\n",
    "Yinc_upscaled = Yinc * My\n",
    "Zinc_upscaled = Zinc * Mz\n",
    "Rotation = 0.0         # Rotation of the field parameter grid\n",
    "Xorigo = 1000.0        # rotation point in global coordinates\n",
    "Yorigo = 1200.0\n",
    "Zorigo =  2000.0\n",
    "Variotype = \"gen_exponential\"  # Spatial covariance of field parameters\n",
    "Main_range = Xinc*Nx*0.1\n",
    "Perp_range = Main_range\n",
    "Vert_range = Zinc*Nz*0.02\n",
    "Azimuth = 0.0\n",
    "Dip = 0.0\n",
    "Power = 1.9 # Used in correlation function for obserations, fields and localization tapering function\n",
    "N_e = 100  # Number of realizations\n",
    "\n",
    "Obs_error_std = 0.05  # Observation error standard deviation\n",
    "Use_obs_correlations = False\n",
    "Obs_correlation_length_x =  Main_range\n",
    "Obs_correlation_length_y =  Perp_range\n",
    "\n",
    "# Localization range parameters\n",
    "local_tapering_function_name = \"gaussian\"\n",
    "x_loc_range = Main_range\n",
    "y_loc_range = Perp_range\n",
    "loc_ellipse_rotation = 0.0\n",
    "cutoff_value = 0.01\n",
    "print(f\"Field size (nx, ny, nz) = ({Nx},{Ny},{Nz})\")\n",
    "print(f\"Localization x-range: {x_loc_range}\")\n",
    "print(f\"Localization y-range: {y_loc_range}\")\n",
    "print(f\"Localization ellipse rotation angle: {loc_ellipse_rotation}\")\n",
    "\n",
    "# --- 1. Generate Initial Ensemble ---\n",
    "\n",
    "# Draw prior realizations of the field parameters\n",
    "variogram = sim.variogram(Variotype, Main_range, Perp_range,Vert_range,Azimuth,Dip,Power)\n",
    "print(f\"Simulate prior fields\")\n",
    "X_prior = simulate_realizations(N_e,variogram,Nx, Ny, Nz,\n",
    "    Xinc, Yinc, Zinc, seed)\n",
    "\n",
    "\n",
    "# --- 2. Define observations ---\n",
    "# Observation field (e.g seismic on regular 2D grid with \n",
    "# grid size and orientation as grid for field parameters,\n",
    "# but in general with different grid increments.\n",
    "grid_xlength = Xinc * Nx\n",
    "grid_ylength = Yinc * Ny\n",
    "obs_grid_nx = Kx  # Number of grid cells in x direction of observation field (e.g. seismic)\n",
    "obs_grid_ny = Ky  # Number of grid cells in y direction of observation field (e.g. seismic)\n",
    "nobs = obs_grid_nx * obs_grid_ny # Assume one active obs per observation grid cell\n",
    "observation_field = define_example_observation_field(\n",
    "    grid_xlength, grid_ylength, \n",
    "    obs_grid_nx, obs_grid_ny)\n",
    "\n",
    "C_D = obs_error_covariance(\n",
    "    Obs_error_std,\n",
    "    obs_grid_nx, obs_grid_ny,\n",
    "    grid_xlength, grid_ylength,\n",
    "    Use_obs_correlations,\n",
    "    Obs_correlation_length_x,\n",
    "    Obs_correlation_length_y)\n",
    "\n",
    "# Initialize smoothers with the current run's seeded RNG\n",
    "rng = np.random.default_rng(seed)\n",
    "alpha_i = 1\n",
    "\n",
    "\n",
    "nparam_per_layer = Nx * Ny\n",
    "print(f\"Calculate rho for one field parameter grid layer\")\n",
    "rho_one_layer = calculate_rho_for_2d_obs_field(\n",
    "    grid_xlength, grid_ylength,\n",
    "    Nx, Ny,\n",
    "    obs_grid_nx, obs_grid_ny,\n",
    "    x_loc_range, y_loc_range, loc_ellipse_rotation,\n",
    "    local_tapering_function_name, cutoff_value=cutoff_value)\n",
    "\n",
    "# Forward model applied to each realization\n",
    "# In this case arithmetic upscaling of field parameter intervals of size\n",
    "# [Mx, My, Mz] into 1 coarse grid cell. The values of the coarse grid cells\n",
    "# are predictions of what we define as observations in this example.\n",
    "Y = forward_model(X_prior,N_e, Nx, Ny, Nz, obs_grid_nx, obs_grid_ny,1)\n",
    "\n",
    "print(f\"Initialize smoother for distance based localisation:\")\n",
    "esmda_distance = DistanceESMDA(\n",
    "        covariance=C_D, observations=observation_field, alpha=alpha_i, seed=rng)\n",
    "run_update =  monitor_performance(run_update)\n",
    "X_posterior = run_update(\n",
    "        X_prior, Y,\n",
    "        rho_one_layer,\n",
    "        N_m, N_e, nobs,\n",
    "        Nbatch, Nlayer_per_batch,Nlayer_last_batch,\n",
    "        nparam_per_layer,\n",
    "        esmda_distance,\n",
    "        use_localization=True)\n",
    "\n",
    "\n",
    "print(f\"Initialize smoother for global esmda:\")\n",
    "# Workaround since ESMDA counts number of calls on assimilate and \n",
    "# but here we want to run it once (only one alpha value, one iteration)\n",
    "#but it is called multiple times due to the number of batches\n",
    "alpha = np.zeros((Nbatch + 1),dtype=np.float32) \n",
    "alpha[:] = alpha_i                              \n",
    "\n",
    "esmda_global = ESMDA(\n",
    "        covariance=C_D, observations=observation_field, alpha=alpha, seed=rng)\n",
    "\n",
    "X_posterior_global = run_update(\n",
    "        X_prior, Y,\n",
    "        rho_one_layer,\n",
    "        N_m, N_e, nobs,\n",
    "        Nbatch, Nlayer_per_batch,Nlayer_last_batch,\n",
    "        nparam_per_layer,\n",
    "        esmda_global,\n",
    "        use_localization=False)\n",
    "\n",
    "\n",
    "# Calculate misfit\n",
    "observation_field_all_real = np.zeros((nobs,N_e))\n",
    "for n in range(N_e):\n",
    "    observation_field_all_real[:,n] = observation_field[:]\n",
    "\n",
    "tmp_diff = (Y - observation_field_all_real)\n",
    "misfit_prior = np.sqrt(np.mean(tmp_diff * tmp_diff,axis=1))\n",
    "misfit_prior_2d = misfit_prior.reshape(obs_grid_ny, obs_grid_nx)\n",
    "obs_field_2d = observation_field.reshape(obs_grid_ny, obs_grid_nx)\n",
    "\n",
    "print(\"Mean and stdev of prior fields\")\n",
    "prior_mean = np.mean(X_prior, axis=1)\n",
    "prior_std  = np.std(X_prior, axis=1)\n",
    "\n",
    "print(\"Mean and stdev of fields updated with localisation\")\n",
    "posterior_mean = np.mean(X_posterior, axis=1)\n",
    "posterior_std = np.std(X_posterior, axis=1)\n",
    "\n",
    "print(\"Mean and stdev of fields updated without localisation\")\n",
    "posterior_global_mean = np.mean(X_posterior_global, axis=1)\n",
    "posterior_global_std = np.std(X_posterior_global, axis=1)\n",
    "\n",
    "# Converted to 3D array\n",
    "prior_mean_3d = prior_mean.reshape((Nz,Ny,Nx))\n",
    "prior_std_3d = prior_std.reshape((Nz,Ny,Nx))\n",
    "\n",
    "update_localized_3d = (posterior_mean - prior_mean).reshape((Nz, Ny, Nx))\n",
    "update_localized_std_3d = posterior_std.reshape((Nz,Ny,Nx))\n",
    "\n",
    "update_global_3d    = (posterior_global_mean - prior_mean).reshape((Nz, Ny, Nx))\n",
    "update_global_std_3d    = posterior_global_std.reshape((Nz,Ny,Nx))\n",
    "\n",
    "\n",
    "print(f\"Run forward model on updated fields with localization\")\n",
    "Ypost = forward_model(X_posterior,N_e, Nx, Ny, Nz, Kx, Ky, Kz)\n",
    "\n",
    "tmp_diff = (Ypost - observation_field_all_real)\n",
    "misfit_post = np.sqrt(np.mean(tmp_diff * tmp_diff,axis=1))\n",
    "misfit_post_2d = misfit_post.reshape(obs_grid_ny,obs_grid_nx)\n",
    "\n",
    "\n",
    "# --- Slices of the Update Fields using global update ---\n",
    "#     # Find a single, symmetric color scale for both update plots\n",
    "update_max_abs = np.max([np.abs(update_localized_3d), np.abs(prior_mean_3d)])\n",
    "vmin_update, vmax_update = -update_max_abs, update_max_abs\n",
    "\n",
    "std_vmax = np.max(update_localized_std_3d)\n",
    "std_vmax_global = np.max(update_global_std_3d)\n",
    "std_vmax_prior = np.max(prior_std_3d)\n",
    "std_mean = np.mean(update_localized_std_3d)\n",
    "std_mean_global = np.mean(update_global_std_3d)\n",
    "std_mean_prior = np.mean(prior_std_3d)\n",
    "std_local = max(std_vmax, std_vmax_prior)\n",
    "rms_misfit_prior = np.sqrt(np.mean(misfit_prior_2d * misfit_prior_2d))\n",
    "rms_misfit_post = np.sqrt(np.mean(misfit_post_2d * misfit_post_2d))\n",
    "\n",
    "print(f\"Max std for prior:  {std_vmax_prior}\")\n",
    "print(f\"Max Std without localization:  {std_vmax_global}\")\n",
    "print(f\"Max Std with localization:  {std_vmax}\")\n",
    "print(f\"Mean Std for prior:  {std_mean_prior}\")\n",
    "print(f\"Mean Std without localization:  {std_mean_global}\")\n",
    "print(f\"Mean Std with localization:  {std_mean}\")\n",
    "print(f\"RootMeanSquare misfit prior: {rms_misfit_prior}\")\n",
    "print(f\"RootMeanSquare misfit posterior: {rms_misfit_post}\")\n",
    "\n",
    "\n",
    "# Cross sections \n",
    "k_indx = int(Nz/2)\n",
    "i_indx = int(Nx/2)\n",
    "j_indx = int(Ny/2)\n",
    "\n",
    "std_vmax = np.max(update_localized_std_3d)\n",
    "std_vmax_global = np.max(update_global_std_3d)\n",
    "std_vmax_prior = np.max(prior_std_3d)\n",
    "\n",
    "# --- Create 3D Cross-Sectional Plots ---\n",
    "fig, axes = plt.subplots(9, 3, figsize=(15, 18))\n",
    "fig.suptitle('3D Cross-Sectional Views of Localization and Updates', fontsize=14)\n",
    "\n",
    "row_number = 0\n",
    "\n",
    "# Localized Update\n",
    "im_update = axes[row_number, 0].imshow(update_localized_3d[k_indx, :, :], cmap='gist_rainbow', vmin=vmin_update, vmax=vmax_update)\n",
    "axes[row_number, 0].set_title(f'Local Update - XY Slice at k = {k_indx}')\n",
    "\n",
    "axes[row_number, 1].imshow(update_localized_3d[:, j_indx, :], cmap='gist_rainbow', vmin=vmin_update, vmax=vmax_update)\n",
    "axes[row_number, 1].set_title(f'Local Update - XZ Slice at j = {j_indx}')\n",
    "\n",
    "axes[row_number, 2].imshow(update_localized_3d[:, :, i_indx], cmap='gist_rainbow', vmin=vmin_update, vmax=vmax_update)\n",
    "axes[row_number, 2].set_title(f'Local Update - YZ Slice at i = {i_indx}')\n",
    "\n",
    "fig.colorbar(im_update, ax=axes[row_number, :], location='right', shrink=0.8, label=f'Values')\n",
    "row_number += 1\n",
    "\n",
    "# --- Slices of the Update Fields using global update ---\n",
    "\n",
    "# Prior\n",
    "im_prior = axes[row_number, 0].imshow(prior_mean_3d[k_indx, :, :], cmap='gist_rainbow', vmin=vmin_update, vmax=vmax_update)\n",
    "axes[row_number, 0].set_title(f'Prior - XY Slice at k = {k_indx}')\n",
    "\n",
    "axes[row_number, 1].imshow(prior_mean_3d[:, j_indx, :], cmap='gist_rainbow', vmin=vmin_update, vmax=vmax_update)\n",
    "axes[row_number, 1].set_title(f'Prior - XZ Slice at j = {j_indx}')\n",
    "\n",
    "axes[row_number, 2].imshow(prior_mean_3d[:, :, i_indx], cmap='gist_rainbow', vmin=vmin_update, vmax=vmax_update)\n",
    "axes[row_number, 2].set_title(f'Prior - YZ Slice at i = {i_indx}')\n",
    "fig.colorbar(im_prior, ax=axes[row_number, :], location='right', shrink=0.8, label=f'Values')\n",
    "row_number += 1\n",
    "\n",
    "# Global Update\n",
    "im_global = axes[row_number, 0].imshow(update_global_3d[k_indx, :, :], cmap='gist_rainbow', vmin=vmin_update, vmax=vmax_update)\n",
    "axes[row_number, 0].set_title(f'Global Update - XY Slice at k = {k_indx}')\n",
    "\n",
    "axes[row_number, 1].imshow(update_global_3d[:, j_indx, :], cmap='gist_rainbow', vmin=vmin_update, vmax=vmax_update)\n",
    "axes[row_number, 1].set_title(f'Global Update - XZ Slice at j = {j_indx}')\n",
    "\n",
    "axes[row_number, 2].imshow(update_global_3d[:, :, i_indx], cmap='gist_rainbow', vmin=vmin_update, vmax=vmax_update)\n",
    "axes[row_number, 2].set_title(f'Global Update - YZ Slice at i = {i_indx}')\n",
    "fig.colorbar(im_global, ax=axes[row_number, :], location='right', shrink=0.8, label=f'Values')\n",
    "row_number += 1\n",
    "\n",
    "max_std = 1.1\n",
    "# Prior std\n",
    "im_prior_std = axes[row_number, 0].imshow(prior_std_3d[k_indx,:, :], cmap='gist_rainbow', vmin=0, vmax=max_std)\n",
    "axes[row_number, 0].set_title(f'Prior Std - XY Slice at k = {k_indx}')\n",
    "\n",
    "axes[row_number, 1].imshow(prior_std_3d[:, j_indx, :], cmap='gist_rainbow', vmin=0, vmax=max_std)\n",
    "axes[row_number, 1].set_title(f'Prior Std - XZ Slice at j = {j_indx}')\n",
    "\n",
    "axes[row_number, 2].imshow(prior_std_3d[:, :, i_indx], cmap='gist_rainbow', vmin=0, vmax=max_std)\n",
    "axes[row_number, 2].set_title(f'Prior Std - YZ Slice at i = {i_indx}')\n",
    "fig.colorbar(im_prior_std, ax=axes[row_number, :], location='right', shrink=0.8, label=f'Std values')\n",
    "row_number += 1\n",
    "\n",
    "# Posterior std with localisation\n",
    "im_post_std = axes[row_number, 0].imshow(update_localized_std_3d[k_indx,:, :], cmap='gist_rainbow', vmin=0, vmax=max_std)\n",
    "axes[row_number, 0].set_title(f'Local Post Std - XY Slice at k = {k_indx}')\n",
    "\n",
    "axes[row_number, 1].imshow(update_localized_std_3d[:, j_indx, :], cmap='gist_rainbow', vmin=0, vmax=max_std)\n",
    "axes[row_number, 1].set_title(f'Local Post Std - XZ Slice at j = {j_indx}')\n",
    "\n",
    "axes[row_number, 2].imshow(update_localized_std_3d[:, :, i_indx], cmap='gist_rainbow', vmin=0, vmax=max_std)\n",
    "axes[row_number, 2].set_title(f'Local Post Std - YZ Slice at i = {i_indx}')\n",
    "fig.colorbar(im_post_std, ax=axes[row_number, :], location='right', shrink=0.8, label=f'Std values')\n",
    "row_number += 1\n",
    "\n",
    "# Posterior std without localisation\n",
    "im_post_global_std = axes[row_number, 0].imshow(update_global_std_3d[k_indx, :, :], cmap='gist_rainbow', vmin=0.0, vmax=max_std)\n",
    "axes[row_number, 0].set_title(f'Global Post Std - XY Slice at k = {k_indx}')\n",
    "\n",
    "axes[row_number, 1].imshow(update_global_std_3d[:, j_indx, :], cmap='gist_rainbow', vmin=0.0, vmax=max_std)\n",
    "axes[row_number, 1].set_title(f'Global Post Std - XZ Slice at j = {j_indx}')\n",
    "\n",
    "axes[row_number, 2].imshow(update_global_std_3d[:, :, i_indx], cmap='gist_rainbow', vmin=0.0, vmax=max_std)\n",
    "axes[row_number, 2].set_title(f'Global Post Std - YZ Slice at i = {i_indx}')\n",
    "fig.colorbar(im_post_global_std, ax=axes[row_number, :], location='right', shrink=0.8, label=f'Std values')\n",
    "row_number += 1\n",
    "\n",
    "# Observations (upscaled values)\n",
    "im_obs_values = axes[row_number, 0].imshow(obs_field_2d[:, :], cmap='gist_rainbow', vmin=0.0, vmax=1.0)\n",
    "axes[row_number, 0].set_title(f'Observation field - XY')\n",
    "fig.colorbar(im_obs_values, ax=axes[row_number, :], location='right', shrink=0.8, label=f'Obs values')\n",
    "row_number += 1\n",
    "\n",
    "# Misfit between obs and prior prediction of obs\n",
    "im_misfit_prior = axes[row_number, 0].imshow(misfit_prior_2d[:, :], cmap='gist_rainbow', vmin=0.1, vmax=1.0)\n",
    "axes[row_number, 0].set_title(f'Misfit Prior - XY')\n",
    "fig.colorbar(im_misfit_prior, ax=axes[row_number, :], location='right', shrink=0.8, label=f'Misfit')\n",
    "row_number += 1\n",
    "\n",
    "# Misfit between obs and posterior prediction of obs\n",
    "im_misfit_post = axes[row_number, 0].imshow(misfit_post_2d[:, :], cmap='gist_rainbow', vmin=0.1, vmax=1.0)\n",
    "axes[row_number, 0].set_title(f'Misfit Post - XY')\n",
    "fig.colorbar(im_misfit_post, ax=axes[row_number, :], location='right', shrink=0.8, label=f'Misfit')\n",
    "row_number += 1\n",
    "\n",
    "# Hide axis ticks for a cleaner look\n",
    "for ax in axes.flat:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc580612-af63-46cb-a25e-e0376347ba9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
