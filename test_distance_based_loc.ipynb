{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2571751c-1de1-41ed-9c99-df0c142d8792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from iterative_ensemble_smoother.experimental import DistanceESMDA\n",
    "from iterative_ensemble_smoother.esmda import ESMDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5145d165-23c9-4290-be6a-ef143b043a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import iterative_ensemble_smoother"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52439e7b-81be-4654-ba71-c8801fc68f12",
   "metadata": {},
   "source": [
    "# 1D Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16fb8e2-3e19-4a8e-a340-4098843ab1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System properties\n",
    "N_m = 100  # Number of model parameters (grid points)\n",
    "N_e = 50   # Ensemble size\n",
    "j_obs = 50 # Index of the single observation\n",
    "N_d = 1    # Number of observations\n",
    "\n",
    "# Assimilation properties\n",
    "alpha_i = 1 # TODO: Implement multiple iterations\n",
    "obs_error_var = 0.01 # Variance of observation error\n",
    "SEED = 42\n",
    "\n",
    "# Define the \"true\" model parameters\n",
    "true_parameters = np.zeros(N_m)\n",
    "\n",
    "# Define the \"true\" observation vector (1D)\n",
    "true_observations = np.array([1.0])\n",
    "\n",
    "# Observation error covariance `C_D`. Since N_d=1, it's a 1-element vector.\n",
    "C_D = np.array([obs_error_var])\n",
    "\n",
    "# --- 2. Generate Initial Ensemble and Predictions ---\n",
    "\n",
    "# Create initial ensemble of parameters `X` (prior)\n",
    "# Shape: (N_m, N_e)\n",
    "rng = np.random.default_rng(SEED)\n",
    "X_initial = rng.normal(loc=0.0, scale=0.5, size=(N_m, N_e))\n",
    "\n",
    "# Predict observations `Y` using the identity model `g(x) = x`\n",
    "# We only observe the state at `j_obs`.\n",
    "# Shape: (N_d, N_e) -> (1, 50)\n",
    "Y = X_initial[[j_obs], :]\n",
    "\n",
    "\n",
    "# --- 3. Construct Localization, Covariance, and Smoother ---\n",
    "\n",
    "# Localization matrix `rho`\n",
    "# Shape: (N_m, N_d) -> (100, 1)\n",
    "localization_radius = 10.0\n",
    "model_grid = np.arange(N_m)\n",
    "distances = np.abs(model_grid - j_obs)\n",
    "# Using a simple Gaussian decay for rho\n",
    "rho = np.exp(-0.5 * (distances / localization_radius)**2).reshape(-1, 1)\n",
    "\n",
    "smoother = DistanceESMDA(\n",
    "    covariance=C_D,\n",
    "    observations=true_observations,\n",
    "    alpha=1,\n",
    "    seed=SEED\n",
    ")\n",
    "smoother_ESMDA = ESMDA(\n",
    "    covariance=C_D,\n",
    "    observations=true_observations,\n",
    "    alpha=1,\n",
    "    seed=SEED)\n",
    "\n",
    "D = smoother.perturb_observations(\n",
    "    ensemble_size=N_e, alpha=alpha_i\n",
    ")\n",
    "\n",
    "# --- 4. Run Assimilation and Analyze ---\n",
    "\n",
    "# Run the assimilation\n",
    "X_posterior = smoother.assimilate(X=X_initial, Y=Y, rho=rho)\n",
    "X_posterior_ESMDA = smoother_ESMDA.assimilate(X=X_initial, Y=Y)\n",
    "\n",
    "# Calculate the mean of the prior and posterior ensembles\n",
    "prior_mean = np.mean(X_initial, axis=1)\n",
    "posterior_mean = np.mean(X_posterior, axis=1)\n",
    "posterior_mean_ESMDA = np.mean(X_posterior_ESMDA, axis=1)\n",
    "\n",
    "# --- 5. Plot the Results ---\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True)\n",
    "\n",
    "# Plot 1: Ensemble means\n",
    "# Use a colorblind-safe palette and distinct line styles\n",
    "# From Paul Tol's color schemes: https://personal.sron.nl/~pault/\n",
    "color_localized = '#0072B2'  # Blue\n",
    "color_non_localized = '#D55E00' # Orange\n",
    "\n",
    "ax1.plot(model_grid, true_parameters, color='black', linestyle='-', linewidth=2, label='True Parameters')\n",
    "ax1.plot(model_grid, prior_mean, color='gray', linestyle='--', linewidth=1.5, label='Prior Mean')\n",
    "ax1.plot(model_grid, posterior_mean, color=color_localized, linestyle='-', linewidth=2.5, label='Localized Posterior Mean')\n",
    "ax1.plot(model_grid, posterior_mean_ESMDA, color=color_non_localized, linestyle='-.', linewidth=2.5, label='Non-Localized Posterior Mean')\n",
    "ax1.axvline(j_obs, color='black', linestyle=':', linewidth=1.5, label=f'Observation Location (x={j_obs})')\n",
    "\n",
    "ax1.set_ylabel('Parameter Value')\n",
    "ax1.set_title('Effect of Localized vs. Non-Localized Data Assimilation')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot 2: Localization weights\n",
    "ax2.plot(model_grid, rho, color='black', label='Localization Weight (rho)')\n",
    "ax2.set_xlabel('Model Parameter Index (Grid Point)')\n",
    "ax2.set_ylabel('Weight')\n",
    "ax2.set_title('Localization Function')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# What to expect:\n",
    "# The prior mean (gray dashed line) should be a noisy line centered around 0.\n",
    "#\n",
    "# The Localized Posterior Mean (red solid line) should be pulled from approx. 0 towards the\n",
    "# observation value of 1.0. This update should be strong at the observation location (x=50)\n",
    "# and decay smoothly to zero away from it, following the shape of the localization function.\n",
    "#\n",
    "# The Non-Localized Posterior Mean (green dash-dot line) will show updates across the ENTIRE\n",
    "# domain, not just near the observation. This is because a small ensemble size creates spurious\n",
    "# correlations between the observation and distant, unrelated parameters.\n",
    "#\n",
    "# Consequently, the non-localized update will look noisy and physically unrealistic across the\n",
    "# domain. This demonstrates why localization is essential to generate plausible reservoir models,\n",
    "# as applying updates without it can lead to results that differ significantly from the prior models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661a67b0-67d9-4ab0-a650-cda668946475",
   "metadata": {},
   "source": [
    "# 2D Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8df4568-f563-4f3d-8e6e-a43ddd096289",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nx, Ny = 10, 10\n",
    "N_m = Nx * Ny\n",
    "N_e = 50\n",
    "x_obs, y_obs = 5, 5\n",
    "\n",
    "seed = 42\n",
    "\n",
    "alpha_i = 1\n",
    "obs_error_var = 0.01\n",
    "\n",
    "true_parameters = np.zeros(N_m)\n",
    "true_observations = np.array([1.0])\n",
    "C_D = np.array([obs_error_var])\n",
    "\n",
    "# --- Generate Initial Ensemble and Predictions ---\n",
    "rng = np.random.default_rng(seed)\n",
    "X_prior = rng.normal(loc=0.0, scale=0.5, size=(N_m, N_e))\n",
    "\n",
    "# Convert the 2D observation index to a flat 1D index for slicing\n",
    "flat_obs_index = y_obs * Nx + x_obs\n",
    "Y = X_prior[[flat_obs_index], :]\n",
    "\n",
    "# --- Construct 2D Localization `rho` ---\n",
    "localization_radius = 1\n",
    "\n",
    "# Create a 2D coordinate grid\n",
    "xx, yy = np.meshgrid(np.arange(Nx), np.arange(Ny))\n",
    "# Calculate 2D Euclidean distance from every point to the observation\n",
    "distances_2d = np.sqrt((xx - x_obs)**2 + (yy - y_obs)**2)\n",
    "# Flatten the 2D distance map to a 1D vector to match the parameter vector\n",
    "distances = distances_2d.flatten()\n",
    "\n",
    "rho = np.exp(-0.5 * (distances / localization_radius) ** 2).reshape(-1, 1)\n",
    "\n",
    "# --- Run Assimilations ---\n",
    "esmda_distance = DistanceESMDA(\n",
    "    covariance=C_D, observations=true_observations, alpha=alpha_i, seed=rng\n",
    ")\n",
    "X_posterior = esmda_distance.assimilate(X=X_prior, Y=Y, rho=rho)\n",
    "\n",
    "esmda = ESMDA(\n",
    "    covariance=C_D, observations=true_observations, alpha=alpha_i, seed=rng\n",
    ")\n",
    "X_posterior_global = esmda.assimilate(X=X_prior, Y=Y)\n",
    "\n",
    "# --- Reshape Data for Plotting ---\n",
    "# Reshape the 1D vectors back into 2D grids for visualization\n",
    "prior_mean = np.mean(X_prior, axis=1)\n",
    "prior_mean_2d = prior_mean.reshape((Ny, Nx))\n",
    "posterior_mean = np.mean(X_posterior, axis=1)\n",
    "posterior_mean_2d = posterior_mean.reshape((Ny, Nx))\n",
    "\n",
    "posterior_mean_global = np.mean(X_posterior_global, axis=1)\n",
    "posterior_mean_global_2d = posterior_mean_global.reshape((Ny, Nx))\n",
    "rho_2d = rho.reshape((Ny, Nx))\n",
    "\n",
    "# It's often more insightful to visualize the UPDATE (posterior - prior)\n",
    "update_localized_2d = posterior_mean_2d - prior_mean_2d\n",
    "update_global_2d = posterior_mean_global_2d - prior_mean_2d\n",
    "\n",
    "# --- Create the Plots ---\n",
    "# Use a 2x3 grid to show the most important comparisons\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 10))\n",
    "\n",
    "# Find the min/max across both update fields for a fair comparison\n",
    "update_min = min(update_localized_2d.min(), update_global_2d.min())\n",
    "update_max = max(update_localized_2d.max(), update_global_2d.max())\n",
    "\n",
    "# Localization Function (rho)\n",
    "im = axes[0].imshow(rho_2d, cmap='viridis')\n",
    "axes[0].set_title('Localization Function (rho)')\n",
    "\n",
    "# Localized Update\n",
    "im = axes[1].imshow(update_localized_2d, cmap='coolwarm', vmin=update_min, vmax=update_max)\n",
    "axes[1].set_title('Localized Update')\n",
    "\n",
    "# Non-Localized Update\n",
    "im = axes[2].imshow(update_global_2d, cmap='coolwarm', vmin=update_min, vmax=update_max)\n",
    "axes[2].set_title('Non-Localized Update')\n",
    "\n",
    "# Mark the observation location on all plots and hide ticks\n",
    "for ax in axes.flat:\n",
    "    ax.plot(x_obs, y_obs, 'r+', markersize=12, markeredgewidth=2) # Red cross\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da966e58-c4da-4470-bad0-32dec9b07452",
   "metadata": {},
   "source": [
    "# 3D case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eb8b0a-1942-44db-bf9d-f79e9239bb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nx, Ny, Nz = 10, 10, 10\n",
    "N_m = Nx * Ny * Nz\n",
    "N_e = 50\n",
    "x_obs, y_obs, z_obs = 5, 5, 5\n",
    "\n",
    "seed = 42\n",
    "alpha_i = 1\n",
    "obs_error_var = 0.01\n",
    "\n",
    "true_parameters = np.zeros(N_m)\n",
    "true_observations = np.array([1.0])\n",
    "C_D = np.array([obs_error_var])\n",
    "\n",
    "# --- Generate Initial Ensemble and Predictions ---\n",
    "rng = np.random.default_rng(seed)\n",
    "X_prior = rng.normal(loc=0.0, scale=0.5, size=(N_m, N_e))\n",
    "\n",
    "# Convert the 3D observation index to a flat 1D index for slicing\n",
    "flat_obs_index = (z_obs * Nx * Ny) + (y_obs * Nx) + x_obs\n",
    "Y = X_prior[[flat_obs_index], :]\n",
    "\n",
    "# gmour 2025.07.24\n",
    "# --- Construct 3D Localization `rho` ---\n",
    "localization_radius_x = 2.0\n",
    "localization_radius_y = 3.0\n",
    "localization_radius_z = 5.0\n",
    "\n",
    "# Create 3D coordinate grids\n",
    "zz, yy, xx = np.meshgrid(np.arange(Nz), np.arange(Ny), np.arange(Nx), indexing='ij')\n",
    "\n",
    "# gmour 2025.07.24\n",
    "# Calculate 3D Euclidean distance from every point to the observation\n",
    "distances_3d = np.sqrt(((xx - x_obs)/localization_radius_x)**2 + ((yy - y_obs)/localization_radius_y)**2 + ((zz - z_obs)/localization_radius_z)**2)\n",
    "distances = distances_3d.flatten()\n",
    "\n",
    "rho = np.exp(-0.5 * distances ** 2).reshape(-1, 1)\n",
    "\n",
    "# --- Run Assimilations ---\n",
    "# This part remains the same as the assimilation logic is dimension-agnostic\n",
    "esmda_distance = DistanceESMDA(\n",
    "    covariance=C_D, observations=true_observations, alpha=alpha_i, seed=rng\n",
    ")\n",
    "X_posterior = esmda_distance.assimilate(X=X_prior, Y=Y, rho=rho)\n",
    "\n",
    "esmda = ESMDA(\n",
    "    covariance=C_D, observations=true_observations, alpha=alpha_i, seed=rng\n",
    ")\n",
    "X_posterior_global = esmda.assimilate(X=X_prior, Y=Y)\n",
    "\n",
    "# --- Reshape Data and Prepare for 3D Visualization ---\n",
    "prior_mean = np.mean(X_prior, axis=1)\n",
    "posterior_mean = np.mean(X_posterior, axis=1)\n",
    "\n",
    "# Reshape the 1D vectors into 3D volumes\n",
    "rho_3d = rho.reshape((Nz, Ny, Nx))\n",
    "update_localized_3d = (posterior_mean - prior_mean).reshape((Nz, Ny, Nx))\n",
    "update_global_3d = (np.mean(X_posterior_global, axis=1) - prior_mean).reshape((Nz, Ny, Nx))\n",
    "\n",
    "# --- Create 3D Cross-Sectional Plots ---\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 9))\n",
    "fig.suptitle('3D Cross-Sectional Views of Localization and Updates', fontsize=16)\n",
    "\n",
    "# --- Row 1: Slices of the Localization Function (rho) ---\n",
    "im_rho = axes[0, 0].imshow(rho_3d[z_obs, :, :], cmap='viridis', vmin=0, vmax=1)\n",
    "axes[0, 0].set_title(f'Rho - XY Slice at z={z_obs}')\n",
    "\n",
    "axes[0, 1].imshow(rho_3d[:, y_obs, :], cmap='viridis', vmin=0, vmax=1)\n",
    "axes[0, 1].set_title(f'Rho - XZ Slice at y={y_obs}')\n",
    "\n",
    "axes[0, 2].imshow(rho_3d[:, :, x_obs], cmap='viridis', vmin=0, vmax=1)\n",
    "axes[0, 2].set_title(f'Rho - YZ Slice at x={x_obs}')\n",
    "\n",
    "fig.colorbar(im_rho, ax=axes[0, :], location='right', shrink=0.8, label='Localization Weight')\n",
    "\n",
    "# --- Slices of the Update Fields ---\n",
    "# Find a single, symmetric color scale for both update plots\n",
    "update_max_abs = np.max([np.abs(update_localized_3d), np.abs(update_global_3d)])\n",
    "vmin_update, vmax_update = -update_max_abs, update_max_abs\n",
    "\n",
    "# Localized Update\n",
    "im_update = axes[1, 0].imshow(update_localized_3d[z_obs, :, :], cmap='coolwarm', vmin=vmin_update, vmax=vmax_update)\n",
    "axes[1, 0].set_title(f'Localized Update - XY Slice')\n",
    "\n",
    "axes[1, 1].imshow(update_localized_3d[:, y_obs, :], cmap='coolwarm', vmin=vmin_update, vmax=vmax_update)\n",
    "axes[1, 1].set_title(f'Localized Update - XZ Slice')\n",
    "\n",
    "axes[1, 2].imshow(update_localized_3d[:, :, x_obs], cmap='coolwarm', vmin=vmin_update, vmax=vmax_update)\n",
    "axes[1, 2].set_title(f'Localized Update - YZ Slice')\n",
    "\n",
    "# --- Add Annotations to All Plots ---\n",
    "# Add crosshairs to pinpoint the observation location in each slice\n",
    "axes[0, 0].axhline(y_obs, color='r', linestyle=':', lw=1); axes[0, 0].axvline(x_obs, color='r', linestyle=':', lw=1)\n",
    "axes[0, 1].axhline(z_obs, color='r', linestyle=':', lw=1); axes[0, 1].axvline(x_obs, color='r', linestyle=':', lw=1)\n",
    "axes[0, 2].axhline(z_obs, color='r', linestyle=':', lw=1); axes[0, 2].axvline(y_obs, color='r', linestyle=':', lw=1)\n",
    "\n",
    "axes[1, 0].axhline(y_obs, color='k', linestyle=':', lw=1); axes[1, 0].axvline(x_obs, color='k', linestyle=':', lw=1)\n",
    "axes[1, 1].axhline(z_obs, color='k', linestyle=':', lw=1); axes[1, 1].axvline(x_obs, color='k', linestyle=':', lw=1)\n",
    "axes[1, 2].axhline(z_obs, color='k', linestyle=':', lw=1); axes[1, 2].axvline(y_obs, color='k', linestyle=':', lw=1)\n",
    "\n",
    "# Hide axis ticks for a cleaner look\n",
    "for ax in axes.flat:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef00a8e-0921-43ac-8d35-10d6feda8f22",
   "metadata": {},
   "source": [
    "# Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c17ed7c6-701a-468b-a5ad-ddf2d032dad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing large-scale performance...\n",
      "==================================================\n",
      "\n",
      "Test Case 1: N_params=40,000, N_wells=5, Obs_per_well=2000, Total_obs=10000\n",
      "Grid: 200x200, Wells: 5, Obs/well: 2000\n",
      "  Trial 1/1...\n",
      "    Generating ensemble...\n",
      "    Selecting well locations and observations...\n",
      "    Creating localization matrix...\n",
      "    Running assimilation...\n",
      "    Time: 12.35s\n",
      "    Creating visualization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b332aa59ba6748cea5573a5f765d8527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='slice_index', max=9999), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Average time: 12.35 ± 0.00s\n",
      "  Time per parameter: 308.81 μs/param\n",
      "  Time per well: 2470.47 ms/well\n",
      "  Time per observation: 1.24 ms/obs\n",
      "Function time_large_scale_performance:\n",
      "  Total time: 12.59s\n",
      "\n",
      "============================================================\n",
      "LARGE-SCALE PERFORMANCE ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Case 1: 40,000 params, 5 wells, 2000 obs/well\n",
      "  Total obs: 10000\n",
      "  Total time: 12.35 ± 0.00 seconds\n",
      "  Throughput: 3238 params/second\n",
      "  Time per param: 308.81 μs\n",
      "  Time per well: 2470.47 ms\n",
      "  Time per obs: 1.24 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "from iterative_ensemble_smoother.experimental import DistanceESMDA\n",
    "\n",
    "def gaspari_cohn_localization(distances, radius):\n",
    "    \"\"\"Gaspari-Cohn localization with compact support at 2*radius\"\"\"\n",
    "    r = distances / radius\n",
    "    rho = np.zeros_like(r)\n",
    "    \n",
    "    # Compact support: zero beyond 2*radius\n",
    "    mask1 = r <= 1\n",
    "    mask2 = (r > 1) & (r <= 2)\n",
    "    \n",
    "    # Gaspari-Cohn function with smooth derivatives\n",
    "    rho[mask1] = 1 - (5/3)*r[mask1]**2 + (5/8)*r[mask1]**3 + (1/2)*r[mask1]**4 - (1/4)*r[mask1]**5\n",
    "    rho[mask2] = 4 - 5*r[mask2] + (5/3)*r[mask2]**2 + (5/8)*r[mask2]**3 - (1/2)*r[mask2]**4 + (1/12)*r[mask2]**5 - 2/(3*r[mask2])\n",
    "    \n",
    "    return rho\n",
    "\n",
    "def create_multi_well_localization(well_locations, obs_per_well, grid_size, radii_azimuth):\n",
    "    \"\"\"Create localization matrix where all observations at same well share localization\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    well_locations : array_like\n",
    "        Indices of well locations\n",
    "    obs_per_well : int\n",
    "        Number of observations per well\n",
    "    grid_size : int\n",
    "        Size of square grid (grid_size x grid_size)\n",
    "    radius : float\n",
    "        Localization radius (function has compact support at 2*radius)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    rho : ndarray\n",
    "        Localization matrix of shape (n_params, n_obs)\n",
    "    \"\"\"\n",
    "    \n",
    "    n_wells = len(well_locations)\n",
    "    n_obs = n_wells * obs_per_well\n",
    "    n_params = grid_size * grid_size\n",
    "    \n",
    "    # Create coordinate grids\n",
    "    xx, yy = np.meshgrid(np.arange(grid_size), np.arange(grid_size))\n",
    "\n",
    "    xx_flat = xx.flatten()\n",
    "    yy_flat = yy.flatten()\n",
    "\n",
    "    rho = np.zeros((n_params, n_obs))\n",
    "    \n",
    "    for current_index, well_idx in enumerate(well_locations):\n",
    "        # Convert 1D grid index to 2D coordinates\n",
    "        # For a flattened grid: index = y * grid_size + x\n",
    "        # So: x = index % grid_size, y = index // grid_size\n",
    "        well_x = well_idx % grid_size\n",
    "        well_y = well_idx // grid_size\n",
    "\n",
    "        # Perform a simple 2D rotation. Equations available in:\n",
    "        # Emerick, A., & Reynolds, A. (2011). Combining sensitivities and prior information for covariance localization \n",
    "        # in the ensemble Kalman filter for petroleum reservoir applications. Computational Geosciences, 15(2), 251-269.\n",
    "        # Equations #18 and #19\n",
    "\n",
    "        rotated_x_dist = ((xx_flat - well_x) * np.cos(radii_azimuth[2]*np.pi/180) + \n",
    "                          (yy_flat - well_y) * np.sin(radii_azimuth[2]*np.pi/180))\n",
    "        rotated_y_dist = (-(xx_flat - well_x) * np.sin(radii_azimuth[2]*np.pi/180) + \n",
    "                          (yy_flat - well_y) * np.cos(radii_azimuth[2]*np.pi/180))\n",
    "        \n",
    "        distances = np.sqrt((rotated_x_dist / radii_azimuth[0])**2 + \n",
    "                            (rotated_y_dist / radii_azimuth[1])**2)\n",
    "\n",
    "        rho_well = gaspari_cohn_localization(distances, 1)\n",
    "\n",
    "        # Populate rho by replicating rho_well obs_per_well times:\n",
    "        rho[: , current_index * obs_per_well : (current_index + 1) * obs_per_well] = np.tile(\n",
    "            rho_well.reshape(-1, 1), (1, obs_per_well))\n",
    "    \n",
    "    return rho\n",
    "\n",
    "def visualize_localization_effect(X_prior, X_posterior, well_locations, rho, grid_size, case_name=\"\"):\n",
    "    \"\"\"Visualize the localization function and its effect on the update\"\"\"\n",
    "    \n",
    "    # Calculate ensemble means\n",
    "    prior_mean = np.mean(X_prior, axis=1)\n",
    "    posterior_mean = np.mean(X_posterior, axis=1)\n",
    "    \n",
    "    # Reshape to 2D grids\n",
    "    prior_mean_2d = prior_mean.reshape((grid_size, grid_size))\n",
    "    posterior_mean_2d = posterior_mean.reshape((grid_size, grid_size))\n",
    "    rho_2d = rho[:, 0].reshape((grid_size, grid_size))  # Use first column since all are the same\n",
    "    \n",
    "    # Calculate update\n",
    "    update_2d = posterior_mean_2d - prior_mean_2d\n",
    "    \n",
    "    # Convert well locations to 2D coordinates\n",
    "    well_coords = [(idx % grid_size, idx // grid_size) for idx in well_locations]\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "    # Plot 1: Localization function\n",
    "    im1 = axes[0].imshow(rho_2d, cmap='viridis', origin='lower')\n",
    "    axes[0].set_title('Localization Function (rho)')\n",
    "    axes[0].set_xlabel('X coordinate')\n",
    "    axes[0].set_ylabel('Y coordinate')\n",
    "    plt.colorbar(im1, ax=axes[0])\n",
    "    \n",
    "    # Mark well locations\n",
    "    for well_x, well_y in well_coords:\n",
    "        axes[0].plot(well_x, well_y, 'r+', markersize=15, markeredgewidth=3)\n",
    "    \n",
    "    # Plot 2: Localized update\n",
    "    update_max = max(abs(update_2d.min()), abs(update_2d.max()))\n",
    "    im2 = axes[1].imshow(update_2d, cmap='RdBu_r', origin='lower', \n",
    "                         vmin=-update_max, vmax=update_max)\n",
    "    axes[1].set_title('Localized Update')\n",
    "    axes[1].set_xlabel('X coordinate')\n",
    "    axes[1].set_ylabel('Y coordinate')\n",
    "    plt.colorbar(im2, ax=axes[1])\n",
    "    \n",
    "    # Mark well locations\n",
    "    for well_x, well_y in well_coords:\n",
    "        axes[1].plot(well_x, well_y, 'k+', markersize=15, markeredgewidth=3)\n",
    "    \n",
    "    plt.suptitle(f'Localization Effect {case_name}', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print some statistics\n",
    "    print(f\"    Localization statistics:\")\n",
    "    print(f\"      Sparsity: {np.mean(rho_2d == 0) * 100:.1f}% of parameters have zero localization\")\n",
    "    print(f\"      Max localization: {rho_2d.max():.3f}\")\n",
    "    print(f\"      Update range: [{update_2d.min():.4f}, {update_2d.max():.4f}]\")\n",
    "    print(f\"      Update std: {update_2d.std():.4f}\")\n",
    "    print(f\"      Rho shape: {rho.shape}\")\n",
    "\n",
    "def visualize_localization_effect_interactive(X_prior, X_posterior, well_locations, rho, grid_size, case_name=\"\"):\n",
    "    \"\"\"Interactive visualization of the localization function and its effect on the update\"\"\"\n",
    "\n",
    "    # Calculate ensemble means\n",
    "    prior_mean = np.mean(X_prior, axis=1)\n",
    "    posterior_mean = np.mean(X_posterior, axis=1)\n",
    "\n",
    "    # Reshape to 2D grids\n",
    "    prior_mean_2d = prior_mean.reshape((grid_size, grid_size))\n",
    "    posterior_mean_2d = posterior_mean.reshape((grid_size, grid_size))\n",
    "\n",
    "    # Calculate update\n",
    "    update_2d = posterior_mean_2d - prior_mean_2d\n",
    "\n",
    "    # Convert well locations to 2D coordinates\n",
    "    well_coords = [(idx % grid_size, idx // grid_size) for idx in well_locations]\n",
    "\n",
    "    def plot_slice(slice_index):\n",
    "        rho_2d = rho[:, slice_index].reshape((grid_size, grid_size))\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "        # Plot 1: Localization function\n",
    "        im1 = axes[0].imshow(rho_2d, cmap='viridis', origin='lower')\n",
    "        axes[0].set_title(f'Localization Function (rho[:, {slice_index}])')\n",
    "        axes[0].set_xlabel('X coordinate')\n",
    "        axes[0].set_ylabel('Y coordinate')\n",
    "        plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "        for well_x, well_y in well_coords:\n",
    "            axes[0].plot(well_x, well_y, 'r+', markersize=15, markeredgewidth=3)\n",
    "\n",
    "        # Plot 2: Localized update\n",
    "        update_max = max(abs(update_2d.min()), abs(update_2d.max()))\n",
    "        im2 = axes[1].imshow(update_2d, cmap='RdBu_r', origin='lower',\n",
    "                             vmin=-update_max, vmax=update_max)\n",
    "        axes[1].set_title('Localized Update')\n",
    "        axes[1].set_xlabel('X coordinate')\n",
    "        axes[1].set_ylabel('Y coordinate')\n",
    "        plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "        for well_x, well_y in well_coords:\n",
    "            axes[1].plot(well_x, well_y, 'k+', markersize=15, markeredgewidth=3)\n",
    "\n",
    "        plt.suptitle(f'Localization Effect {case_name}', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Print statistics\n",
    "        print(f\"    Localization statistics for slice {slice_index}:\")\n",
    "        print(f\"      Sparsity: {np.mean(rho_2d == 0) * 100:.1f}% of parameters have zero localization\")\n",
    "        print(f\"      Max localization: {rho_2d.max():.3f}\")\n",
    "        print(f\"      Update range: [{update_2d.min():.4f}, {update_2d.max():.4f}]\")\n",
    "        print(f\"      Update std: {update_2d.std():.4f}\")\n",
    "        print(f\"      Rho shape: {rho.shape}\")\n",
    "\n",
    "    # Create interactive slider\n",
    "    interact(plot_slice, slice_index=IntSlider(min=0, max=rho.shape[1]-1, step=1, value=0))\n",
    "\n",
    "\n",
    "def time_large_scale_performance():\n",
    "    \"\"\"Test performance on large-scale problems\"\"\"\n",
    "    \n",
    "    # Test cases: (n_params, n_wells, obs_per_well)\n",
    "    test_cases = [\n",
    "        (200*200, 5, 200),\n",
    "        #(100*100, 5, 1000),\n",
    "        #(1000*1000, 50, 20),\n",
    "        #(1000*1000, 100, 10),\n",
    "    ]\n",
    "    \n",
    "    n_ensemble = 100\n",
    "    n_trials = 1  # Might want to reduce this for very large problems\n",
    "                                # main, normal, azimuth\n",
    "    localization_radii_azimuth = [50, 30, 45]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(\"Testing large-scale performance...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, (n_params, n_wells, obs_per_well) in enumerate(test_cases):\n",
    "        n_obs = n_wells * obs_per_well\n",
    "        print(f\"\\nTest Case {i+1}: N_params={n_params:,}, N_wells={n_wells}, Obs_per_well={obs_per_well}, Total_obs={n_obs}\")\n",
    "        \n",
    "        grid_size = int(np.sqrt(n_params))\n",
    "        print(f\"Grid: {grid_size}x{grid_size}, Wells: {n_wells}, Obs/well: {obs_per_well}\")\n",
    "        \n",
    "        # Verify it's a perfect square\n",
    "        assert grid_size * grid_size == n_params, f\"Not a perfect square: {n_params}\"\n",
    "        \n",
    "        trial_times = []\n",
    "        \n",
    "        for trial in range(n_trials):\n",
    "            print(f\"  Trial {trial + 1}/{n_trials}...\")\n",
    "            \n",
    "            # Setup\n",
    "            #rng = np.random.default_rng(42 + trial)\n",
    "            rng = np.random.default_rng()\n",
    "            \n",
    "            # Generate ensemble\n",
    "            print(\"    Generating ensemble...\")\n",
    "            X_prior = rng.normal(0, 0.5, size=(n_params, n_ensemble))\n",
    "            \n",
    "            # Generate well locations and observations\n",
    "            print(\"    Selecting well locations and observations...\")\n",
    "            \n",
    "            # First, select well locations (unique spatial positions)\n",
    "            well_locations = rng.choice(n_params, size=n_wells, replace=False)\n",
    "            \n",
    "            # Then, create multiple observations per well\n",
    "            # For simplicity, all observations at a well have the same location\n",
    "            # In reality, they might be slightly offset or represent different variables\n",
    "            obs_indices = []\n",
    "            for well_loc in well_locations:\n",
    "                for _ in range(obs_per_well):\n",
    "                    obs_indices.append(well_loc)\n",
    "\n",
    "            # Generate observations by sampling the forward model at well locations\n",
    "            # Note: obs_indices can be longer than n_params since we replicate well locations\n",
    "            # for multiple observations per well. NumPy allows repeated indices, so\n",
    "            # X_prior[obs_indices, :] creates multiple copies of the same model values.\n",
    "            obs_indices = np.array(obs_indices)\n",
    "            # Using identity model Y = g(X) = X\n",
    "            Y = X_prior[obs_indices, :]\n",
    "            \n",
    "            # Start timing the critical section\n",
    "            start_time = time.perf_counter()\n",
    "            \n",
    "            print(\"    Creating localization matrix...\")\n",
    "            \n",
    "            # Create localization matrix based on well locations\n",
    "            # All observations at same well share same localization\n",
    "            rho = create_multi_well_localization(\n",
    "                well_locations, obs_per_well, grid_size, localization_radii_azimuth\n",
    "            )\n",
    "\n",
    "            print(\"    Running assimilation...\")\n",
    "            \n",
    "            # Run assimilation with all observations\n",
    "            esmda = DistanceESMDA(\n",
    "                covariance=np.eye(n_obs) * 0.01,\n",
    "                observations=np.ones(n_obs),\n",
    "                alpha=1,\n",
    "                seed=rng\n",
    "            )\n",
    "\n",
    "            X_posterior = esmda.assimilate(X=X_prior, Y=Y, rho=rho)\n",
    "            \n",
    "            end_time = time.perf_counter()\n",
    "            \n",
    "            total_time = end_time - start_time\n",
    "            trial_times.append(total_time)\n",
    "            \n",
    "            print(f\"    Time: {total_time:.2f}s\")\n",
    "            \n",
    "            # Visualize the localization effect\n",
    "            print(\"    Creating visualization...\")\n",
    "            visualize_localization_effect_interactive(\n",
    "                X_prior, X_posterior, well_locations, rho, grid_size, \n",
    "                case_name=f\"(Case {i+1}: {n_wells} wells, {obs_per_well} obs/well, radii and azimuth={localization_radii_azimuth})\"\n",
    "            )\n",
    "            \n",
    "            # Clean up large arrays to free memory\n",
    "            del X_prior, X_posterior, Y, rho\n",
    "        \n",
    "        avg_time = np.mean(trial_times)\n",
    "        std_time = np.std(trial_times)\n",
    "        \n",
    "        result = {\n",
    "            'n_params': n_params,\n",
    "            'n_wells': n_wells,\n",
    "            'obs_per_well': obs_per_well,\n",
    "            'n_obs': n_obs,\n",
    "            'avg_time': avg_time,\n",
    "            'std_time': std_time,\n",
    "            'trial_times': trial_times\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"  Average time: {avg_time:.2f} ± {std_time:.2f}s\")\n",
    "        print(f\"  Time per parameter: {avg_time/n_params*1e6:.2f} μs/param\")\n",
    "        print(f\"  Time per well: {avg_time/n_wells*1e3:.2f} ms/well\")\n",
    "        print(f\"  Time per observation: {avg_time/n_obs*1e3:.2f} ms/obs\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_large_scale_results(results):\n",
    "    \"\"\"Analyze and visualize large-scale results\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"LARGE-SCALE PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"\\nCase {i+1}: {result['n_params']:,} params, {result['n_wells']} wells, {result['obs_per_well']} obs/well\")\n",
    "        print(f\"  Total obs: {result['n_obs']}\")\n",
    "        print(f\"  Total time: {result['avg_time']:.2f} ± {result['std_time']:.2f} seconds\")\n",
    "        print(f\"  Throughput: {result['n_params']/result['avg_time']:.0f} params/second\")\n",
    "        print(f\"  Time per param: {result['avg_time']/result['n_params']*1e6:.2f} μs\")\n",
    "        print(f\"  Time per well: {result['avg_time']/result['n_wells']*1e3:.2f} ms\")\n",
    "        print(f\"  Time per obs: {result['avg_time']/result['n_obs']*1e3:.2f} ms\")\n",
    "\n",
    "# Performance monitoring decorator (simplified)\n",
    "def monitor_performance(func):\n",
    "    \"\"\"Decorator to monitor time only\"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        result = func(*args, **kwargs)\n",
    "        \n",
    "        end_time = time.perf_counter()\n",
    "        \n",
    "        print(f\"Function {func.__name__}:\")\n",
    "        print(f\"  Total time: {end_time - start_time:.2f}s\")\n",
    "        \n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "# Modified main execution\n",
    "if __name__ == \"__main__\":\n",
    "    time_large_scale_performance = monitor_performance(time_large_scale_performance)\n",
    "    \n",
    "    results = time_large_scale_performance()\n",
    "    analyze_large_scale_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc580612-af63-46cb-a25e-e0376347ba9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
